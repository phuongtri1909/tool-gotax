import redis
import os
import logging

# Redis connection
REDIS_HOST = os.getenv('REDIS_HOST', '127.0.0.1')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
REDIS_DB = int(os.getenv('REDIS_DB', 0))
REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', None)

def get_redis_client():
    """Get Redis client instance"""
    return redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        db=REDIS_DB,
        password=REDIS_PASSWORD,
        decode_responses=False
    )

def publish_progress(job_id, percent, message, data=None, **kwargs):
    import json
    import logging
    import time
    
    logger = logging.getLogger(__name__)
    
    # ‚úÖ Retry mechanism v·ªõi timeout handling
    max_retries = 3
    retry_delay = 0.5  # 0.5 gi√¢y
    
    for attempt in range(max_retries):
        try:
            redis_client = get_redis_client()
            
            # ‚úÖ Test connection tr∆∞·ªõc khi s·ª≠ d·ª•ng
            try:
                redis_client.ping()
            except Exception as ping_e:
                logger.warning(f"‚ö†Ô∏è [REDIS] Connection test failed (attempt {attempt + 1}/{max_retries}): {ping_e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    logger.error(f"‚ùå [REDIS] Cannot connect to Redis after {max_retries} attempts")
                    return
            
            progress_data = {
                'percent': percent,
                'message': message,
            }
            
            # T·∫°o data object ƒë·ªÉ frontend c√≥ th·ªÉ truy c·∫≠p
            data_obj = {}
            
            if kwargs:
                for key, value in kwargs.items():
                    if value is not None:
                        progress_data[key] = value
                        # Th√™m v√†o data object n·∫øu l√† field frontend c·∫ßn
                        if key in ['accumulated_total', 'accumulated_downloaded', 'accumulated_percent', 
                                  'thuyet_minh_downloaded', 'thuyet_minh_total',
                                  'total_cccd', 'processed_cccd', 'total_images', 'processed_images', 
                                  'total_rows', 'estimated_cccd', 'processed']:
                            data_obj[key] = value
            
            if data:
                if isinstance(data, dict):
                    # Copy t·∫•t c·∫£ fields t·ª´ data v√†o data_obj (bao g·ªìm c·∫£ gi√° tr·ªã 0)
                    for key, value in data.items():
                        # Copy t·∫•t c·∫£ field, k·ªÉ c·∫£ khi value = 0 (v√¨ 0 l√† gi√° tr·ªã h·ª£p l·ªá)
                        if value is not None or (isinstance(value, (int, float)) and value == 0):
                            data_obj[key] = value
                            # C≈©ng copy l√™n top level cho backward compatibility
                            if key in ['total_cccd', 'processed_cccd', 'total_images', 'processed_images', 
                                      'total_rows', 'estimated_cccd', 'processed']:
                                progress_data[key] = value
                    # ƒê·∫£m b·∫£o c√°c field accumulated_* v√† thuyet_minh_* ƒë∆∞·ª£c copy v√†o data_obj (k·ªÉ c·∫£ khi = 0)
                    for key in ['accumulated_total', 'accumulated_downloaded', 'accumulated_percent',
                               'thuyet_minh_downloaded', 'thuyet_minh_total']:
                        if key in data:
                            # Copy ngay c·∫£ khi gi√° tr·ªã l√† 0
                            data_obj[key] = data[key]
                    progress_data['data'] = data_obj
                else:
                    progress_data['data'] = data
            else:
                # N·∫øu kh√¥ng c√≥ data, v·∫´n t·∫°o data object v·ªõi c√°c field t·ª´ kwargs
                if data_obj:
                    progress_data['data'] = data_obj
            
            progress_json = json.dumps(progress_data, ensure_ascii=False)
            progress_bytes = progress_json.encode('utf-8')
            
            processed_cccd = kwargs.get('processed_cccd') or (data.get('processed_cccd') if isinstance(data, dict) else None)
            total_cccd = kwargs.get('total_cccd') or (data.get('total_cccd') if isinstance(data, dict) else None)
            logger.info(f"üì§ Publishing progress for job {job_id}: {percent}% - {message[:50]}...")
            
            # Publish to pub/sub (for real-time) v·ªõi timeout
            try:
                redis_client.publish(f"job:{job_id}:progress", progress_bytes)
                logger.debug(f"‚úÖ Published to pub/sub: job:{job_id}:progress")
            except Exception as e:
                logger.error(f"‚ùå Error publishing progress to pub/sub: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    logger.error(f"‚ùå Failed to publish to pub/sub after {max_retries} attempts")
            
            # Also push to list (for polling fallback) v·ªõi timeout
            progress_list_key = f"job:{job_id}:progress:list"
            try:
                result = redis_client.rpush(progress_list_key, progress_bytes)
                logger.debug(f"‚úÖ Pushed to Redis list {progress_list_key}, new length: {result}")
            except Exception as e:
                logger.error(f"‚ùå Error pushing progress to list: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    logger.error(f"‚ùå Failed to push to list after {max_retries} attempts")
            
            # Also set job status if needed
            try:
                # Limit list size to prevent memory issues (keep last 100 messages)
                redis_client.ltrim(progress_list_key, -100, -1)
            except Exception as e:
                logger.error(f"Error trimming progress list: {e}")
            
            # ‚úÖ N·∫øu ƒë·∫øn ƒë√¢y th√¨ th√†nh c√¥ng, break kh·ªèi retry loop
            break
            
        except Exception as e:
            logger.error(f"‚ùå [REDIS] Error in publish_progress (attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
                continue
            else:
                logger.error(f"‚ùå [REDIS] Failed to publish progress after {max_retries} attempts: {e}")
                return

def is_job_cancelled(job_id):
    """Check if a job has been cancelled"""
    logger = logging.getLogger(__name__)
    
    try:
        redis_client = get_redis_client()
        cancelled = redis_client.get(f"job:{job_id}:cancelled")
        if cancelled:
            # Handle both bytes and string
            if isinstance(cancelled, bytes):
                cancelled = cancelled.decode('utf-8')
            return cancelled.strip() == "1"
        return False
    except Exception as e:
        logger.error(f"Error checking cancellation for job {job_id}: {e}")
        return False

def cancel_job(job_id):
    logger = logging.getLogger(__name__)
    redis_client = get_redis_client()
    try:
        redis_client.set(f"job:{job_id}:cancelled", "1")
        redis_client.set(f"job:{job_id}:status", "cancelled")
        logger.info(f"Job {job_id} marked as cancelled")
    except Exception as e:
        logger.error(f"Error cancelling job {job_id}: {e}")