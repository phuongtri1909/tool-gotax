"""
Routes cho tool-go-soft (Tax Crawler API)
ƒê√£ migrate sang Quart (async) thay Flask ƒë·ªÉ support async operations

ƒê∆∞·ª£c g·ªçi t·ª´ api_server.py chung
"""
import os
import sys
import json
import logging
import base64
from functools import wraps

# Th√™m parent directory v√†o path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Lazy imports
_session_manager = None
_tax_crawler = None


def get_session_manager():
    global _session_manager
    if _session_manager is None:
        from services.session_manager import session_manager
        _session_manager = session_manager
    return _session_manager


def get_tax_crawler():
    global _tax_crawler
    if _tax_crawler is None:
        from services.tax_crawler import get_tax_crawler as gtc
        _tax_crawler = gtc()
    return _tax_crawler


def check_session_exists(session_id: str) -> tuple[bool, dict]:
    """
    Ki·ªÉm tra session c√≥ t·ªìn t·∫°i kh√¥ng
    
    Returns:
        (exists, error_response): 
        - exists: True n·∫øu session t·ªìn t·∫°i, False n·∫øu kh√¥ng
        - error_response: Dict error response n·∫øu session kh√¥ng t·ªìn t·∫°i, None n·∫øu t·ªìn t·∫°i
    """
    try:
        if not session_id:
            return False, {
                "status": "error",
                "error_code": "MISSING_SESSION_ID",
                "message": "Thi·∫øu session_id. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i."
            }
        
        sm = get_session_manager()
        session = sm.get_session(session_id)
        
        if not session:
            return False, {
                "status": "error",
                "error_code": "SESSION_NOT_FOUND",
                "message": "Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i."
            }
        
        return True, None
    except Exception as e:
        logger.error(f"Error in check_session_exists: {e}", exc_info=True)
        # N·∫øu c√≥ l·ªói, tr·∫£ v·ªÅ session not found ƒë·ªÉ an to√†n
        return False, {
            "status": "error",
            "error_code": "SESSION_NOT_FOUND",
            "message": "Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i."
        }


async def check_session_before_crawl(session_id: str) -> tuple[bool, dict]:
    """
    ‚úÖ H√†m check session chung cho t·∫•t c·∫£ c√°c lo·∫°i crawl (t·ªù khai, gi·∫•y n·ªôp ti·ªÅn, th√¥ng b√°o)
    Ki·ªÉm tra:
    1. Session c√≥ t·ªìn t·∫°i kh√¥ng
    2. Session c√≥ h·ª£p l·ªá kh√¥ng (JSESSIONID)
    
    Returns:
        (is_valid, error_response):
        - is_valid: True n·∫øu session h·ª£p l·ªá, False n·∫øu kh√¥ng
        - error_response: Dict error response n·∫øu session kh√¥ng h·ª£p l·ªá, None n·∫øu h·ª£p l·ªá
    """
    try:
        # B∆∞·ªõc 1: Check session exists
        session_exists, error_response = check_session_exists(session_id)
        if not session_exists:
            logger.warning(f"Session check failed (not exists): {session_id[:8]}... - {error_response.get('error_code')}")
            return False, error_response
        
        # B∆∞·ªõc 2: Check session validity (JSESSIONID)
        sm = get_session_manager()
        session_validity = await sm.check_session_validity(session_id)
        if not session_validity.get("valid", False):
            error_code = session_validity.get("error_code", "SESSION_EXPIRED")
            error_message = session_validity.get("error", "Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.")
            logger.warning(f"Session check failed (invalid): {session_id[:8]}... - {error_code}")
            return False, {
                "status": "error",
                "error_code": error_code,
                "message": error_message
            }
        
        logger.debug(f"Session check passed: {session_id[:8]}...")
        return True, None
    except Exception as e:
        logger.error(f"Error in check_session_before_crawl: {e}")
        # N·∫øu c√≥ l·ªói khi check, tr·∫£ v·ªÅ session expired ƒë·ªÉ an to√†n
        return False, {
            "status": "error",
            "error_code": "SESSION_EXPIRED",
            "message": "Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i."
        }


def register_routes(app, prefix):
    """
    ƒêƒÉng k√Ω routes cho tool n√†y
    
    Args:
        app: Quart app instance
        prefix: URL prefix (v√≠ d·ª•: '/api/go-soft')
    """
    
    # Helper to check if app is Quart (async) or Flask (sync)
    is_async = hasattr(app, 'ensure_async')
    
    if is_async:
        from quart import request, jsonify, Response
        
        async def make_response(data, status=200):
            return jsonify(data), status
        
        async def stream_response(generator):
            async def generate():
                async for event in generator:
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
            
            return Response(
                generate(),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no'
                }
            )
    else:
        from flask import request, jsonify, Response, stream_with_context
        
        def make_response(data, status=200):
            return jsonify(data), status
        
        def stream_response(generator):
            def generate():
                for event in generator:
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
            
            return Response(
                stream_with_context(generate()),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no'
                }
            )
    
    # ==================== HEALTH CHECK ====================
    
    @app.route(f'{prefix}/health', methods=['GET'])
    async def go_soft_health_check():
        """Health check"""
        sm = get_session_manager()
        return jsonify({
            "status": "success",
            "message": "Tax Crawler API is running (Playwright + httpx async)",
            "version": "2.0",
            "active_sessions": sm.get_active_session_count()
        })
    
    # ==================== SESSION MANAGEMENT ====================
    
    @app.route(f'{prefix}/session/create', methods=['POST'])
    async def create_session():
        """
        T·∫°o session m·ªõi v·ªõi Playwright
        Returns: session_id
        """
        try:
            from quart import request
            sm = get_session_manager()
            session_id = await sm.create_session()
            
            return jsonify({
                "status": "success",
                "session_id": session_id
            })
        except Exception as e:
            logger.error(f"Error creating session: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/session/close', methods=['POST'])
    async def close_session():
        """
        ƒê√≥ng session
        Body: { "session_id": "..." }
        """
        try:
            from quart import request
            data = await request.get_json()
            session_id = data.get("session_id")
            
            if not session_id:
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_SESSION_ID",
                    "message": "Missing session_id"
                }), 400
            
            sm = get_session_manager()
            tc = get_tax_crawler()
            
            # Close httpx client too
            await tc.close_http_client(session_id)
            success = await sm.close_session(session_id)
            
            return jsonify({
                "status": "success" if success else "error",
                "message": "Session closed" if success else "Session not found"
            })
        except Exception as e:
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/session/status', methods=['GET'])
    async def session_status():
        """
        Ki·ªÉm tra tr·∫°ng th√°i session
        Query: session_id
        """
        from quart import request
        session_id = request.args.get("session_id")
        
        if not session_id:
            return jsonify({
                "status": "error",
                "message": "Missing session_id"
            }), 400
        
        sm = get_session_manager()
        session = sm.get_session(session_id)
        
        if not session:
            return jsonify({
                "status": "error",
                "error_code": "SESSION_NOT_FOUND",
                "message": "Session not found or expired"
            }), 404
        
        return jsonify({
            "status": "success",
            "session_id": session_id,
            "is_logged_in": session.is_logged_in,
            "username": session.username,
            "created_at": session.created_at.isoformat(),
            "last_active": session.last_active.isoformat()
        })
    
    # ==================== LOGIN FLOW ====================
    
    @app.route(f'{prefix}/login/init', methods=['POST'])
    async def init_login():
        """
        Kh·ªüi t·∫°o trang login v√† l·∫•y captcha
        Body: { "session_id": "..." }
        Returns: { captcha_base64: "..." }
        """
        try:
            from quart import request
            data = await request.get_json()
            session_id = data.get("session_id")
            
            if not session_id:
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_SESSION_ID",
                    "message": "Missing session_id"
                }), 400
            
            sm = get_session_manager()
            result = await sm.init_login_page(session_id)
            
            if result["success"]:
                return jsonify({
                    "status": "success",
                    "captcha_base64": result["captcha_base64"]
                })
            else:
                return jsonify({
                    "status": "error",
                    "message": result.get("error", "Unknown error")
                }), 400
                
        except Exception as e:
            logger.error(f"Error in init_login: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/login/submit', methods=['POST'])
    async def submit_login():
        """
        Submit login v·ªõi username, password v√† captcha
        Body: {
            "session_id": "...",
            "username": "...",
            "password": "...",
            "captcha": "..."
        }
        """
        try:
            from quart import request
            data = await request.get_json()
            session_id = data.get("session_id")
            username = data.get("username")
            password = data.get("password")
            captcha = data.get("captcha")
            
            # Login m·ªõi kh√¥ng c·∫ßn captcha n·ªØa
            if not all([session_id, username, password]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields: session_id, username, password"
                }), 400
            
            sm = get_session_manager()
            # Login m·ªõi kh√¥ng c·∫ßn captcha, g·ª≠i r·ªóng
            captcha = captcha or ""
            result = await sm.submit_login(session_id, username, password, captcha)
            
            if result["success"]:
                return jsonify({
                    "status": "success",
                    "message": "Login successful",
                    "dse_session_id": result.get("dse_session_id")
                })
            else:
                return jsonify({
                    "status": "error",
                    "message": result.get("error", "Login failed")
                }), 401
                
        except Exception as e:
            logger.error(f"Error in submit_login: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    # ==================== CRAWL APIs ====================
    
    @app.route(f'{prefix}/tokhai/types', methods=['GET'])
    async def get_tokhai_types():
        """
        L·∫•y danh s√°ch lo·∫°i t·ªù khai
        Query: session_id
        """
        from quart import request
        session_id = request.args.get("session_id")
        
        if not session_id:
            return jsonify({
                "status": "error",
                "message": "Missing session_id"
            }), 400
        
        tc = get_tax_crawler()
        result = await tc.get_tokhai_types(session_id)
        
        if result["success"]:
            return jsonify({
                "status": "success",
                "tokhai_types": result["tokhai_types"]
            })
        else:
            return jsonify({
                "status": "error",
                "message": result.get("error", "Failed to get tokhai types")
            }), 400
    
    @app.route(f'{prefix}/crawl/tokhai', methods=['POST'])
    async def crawl_tokhai():
        """
        Crawl t·ªù khai (publish events to Redis)
        Body: {
            "job_id": "...",  # Job ID ƒë·ªÉ publish events
            "session_id": "...",
            "tokhai_type": "842" ho·∫∑c "01/GTGT" ho·∫∑c "00" (T·∫•t c·∫£) ho·∫∑c null,
            "start_date": "01/01/2023",
            "end_date": "31/12/2023"
        }
        Returns: { "status": "accepted", "job_id": "..." }
        
        API s·∫Ω publish events v√†o Redis, worker s·∫Ω l·∫Øng nghe t·ª´ Redis
        Note: N·∫øu tokhai_type = "00", null, ho·∫∑c kh√¥ng c√≥ ‚Üí crawl T·∫§T C·∫¢ lo·∫°i t·ªù khai
        """
        try:
            from quart import request
            import asyncio
            from shared.redis_client import publish_progress
            
            data = await request.get_json()
            job_id = data.get("job_id")
            session_id = data.get("session_id")
            tokhai_type = data.get("tokhai_type")  # C√≥ th·ªÉ l√† None, "00", ho·∫∑c gi√° tr·ªã c·ª• th·ªÉ
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            if not all([job_id, session_id, start_date, end_date]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields: job_id, session_id, start_date, end_date"
                }), 400
            
            # N·∫øu kh√¥ng c√≥ tokhai_type ho·∫∑c r·ªóng ‚Üí m·∫∑c ƒë·ªãnh l√† "T·∫•t c·∫£"
            if not tokhai_type or tokhai_type.strip() == "":
                tokhai_type = "00"
            
            # ‚úÖ Check session tr∆∞·ªõc khi crawl (d√πng h√†m chung)
            is_valid, error_response = await check_session_before_crawl(session_id)
            if not is_valid:
                return jsonify(error_response), 401
            
            # Ch·∫°y crawl trong background task v√† publish events v√†o Redis
            async def crawl_and_publish():
                try:
                    tc = get_tax_crawler()
                    results = []
                    total_count = 0
                    zip_filename = None
                    download_id = None
                    accumulated_total = 0
                    accumulated_downloaded = 0
                    
                    async for event in tc.crawl_tokhai(session_id, tokhai_type, start_date, end_date, job_id=job_id):
                        # ‚úÖ Check cancelled tr∆∞·ªõc khi x·ª≠ l√Ω event ti·∫øp theo
                        from shared.redis_client import get_redis_client
                        check_redis = get_redis_client()
                        cancelled = check_redis.get(f"job:{job_id}:cancelled")
                        if cancelled:
                            cancelled = cancelled.decode('utf-8') if isinstance(cancelled, bytes) else str(cancelled).strip()
                            if cancelled == '1':
                                logger.info(f"[API] Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                                check_redis.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                                publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
                                break
                        
                        event_type = event.get('type', 'unknown')
                        
                        # ‚úÖ N·∫øu event l√† error v·ªõi JOB_CANCELLED, d·ª´ng ngay
                        if event_type == 'error' and event.get('error_code') == 'JOB_CANCELLED':
                            logger.info(f"[API] Job {job_id} ƒë√£ b·ªã cancel t·ª´ crawler")
                            check_redis.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                            publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy", event)
                            break
                        
                        if event_type == 'progress':
                            percent = event.get('percent', 0)
                            message = event.get('message', 'ƒêang x·ª≠ l√Ω...')
                            publish_progress(job_id, percent, message, event)
                            
                        elif event_type == 'info':
                            message = event.get('message', '')
                            # ‚úÖ Forward accumulated_percent v√† c√°c field kh√°c t·ª´ event ƒë·ªÉ kh√¥ng reset v·ªÅ 0%
                            percent = event.get('accumulated_percent', event.get('percent', 0))
                            if isinstance(percent, float):
                                percent = int(percent)
                            publish_progress(job_id, percent, message, event)
                            
                        elif event_type == 'special_items':
                            # ‚úÖ Forward accumulated_percent v√† c√°c field kh√°c t·ª´ event ƒë·ªÉ kh√¥ng reset v·ªÅ 0%
                            percent = event.get('accumulated_percent', event.get('percent', 0))
                            if isinstance(percent, float):
                                percent = int(percent)
                            message = event.get('message', '')
                            publish_progress(job_id, percent, message, event)
                            
                        elif event_type == 'download_start':
                            total = event.get('accumulated_total', event.get('total', 0))
                            accumulated_total = total
                            publish_progress(job_id, 0, f"B·∫Øt ƒë·∫ßu t·∫£i {total} file...", event)
                            
                        elif event_type == 'download_progress':
                            current = event.get('accumulated_downloaded', event.get('current', 0))
                            total = event.get('accumulated_total', event.get('total', 0))
                            accumulated_total = total
                            accumulated_downloaded = current
                            # ‚úÖ D√πng % t√≠ch l≈©y t·ª´ event (kh√¥ng t√≠nh l·∫°i ƒë·ªÉ tr√°nh th·ª•t l√πi)
                            percent = event.get('accumulated_percent', event.get('percent', 0))
                            if isinstance(percent, float):
                                percent = int(percent)
                            
                            # ‚úÖ L·∫•y th√¥ng tin t·ªù thuy·∫øt minh t·ª´ event
                            thuyet_minh_downloaded = event.get('thuyet_minh_downloaded', 0)
                            thuyet_minh_total = event.get('thuyet_minh_total', 0)
                            
                            # ‚úÖ T·∫°o message v·ªõi t·ªù thuy·∫øt minh n·∫øu c√≥
                            if thuyet_minh_total > 0:
                                message = f"ƒê√£ t·∫£i {current}/{total} file - {thuyet_minh_downloaded}/{thuyet_minh_total} tm"
                            else:
                                message = f"ƒê√£ t·∫£i {current}/{total} file"
                            
                            # ‚úÖ Th√™m T·∫§T C·∫¢ th√¥ng tin c·∫ßn thi·∫øt v√†o event data ƒë·ªÉ frontend nh·∫≠n ƒë∆∞·ª£c
                            event['accumulated_percent'] = percent
                            event['accumulated_total'] = accumulated_total
                            event['accumulated_downloaded'] = accumulated_downloaded
                            event['thuyet_minh_downloaded'] = thuyet_minh_downloaded
                            event['thuyet_minh_total'] = thuyet_minh_total
                            
                            # LOG: Ki·ªÉm tra event tr∆∞·ªõc khi publish
                            logger.info(f"[API] download_progress event before publish: accumulated_percent={event.get('accumulated_percent')}, accumulated_total={event.get('accumulated_total')}, accumulated_downloaded={event.get('accumulated_downloaded')}, thuyet_minh_downloaded={event.get('thuyet_minh_downloaded')}, thuyet_minh_total={event.get('thuyet_minh_total')}")
                            
                            publish_progress(job_id, percent, message, event)
                            
                        elif event_type == 'item':
                            results.append(event.get('data'))
                            
                        elif event_type == 'complete':
                            # ‚úÖ S·ªë file ƒë√£ t·∫£i = t·ªù khai + t·ªù thuy·∫øt minh (t·ª´ event complete)
                            total_from_event = event.get('total', 0)  # ƒê√¢y l√† total_files_downloaded t·ª´ backend
                            download_id = event.get('download_id')
                            zip_filename = event.get('zip_filename')
                            
                            # ‚úÖ LU√îN d√πng total_from_event (s·ªë file ƒë√£ t·∫£i), KH√îNG d√πng accumulated_total (t·ªïng t√¨m th·∫•y)
                            total_count = total_from_event
                            
                            # Publish complete event
                            from shared.redis_client import get_redis_client
                            redis_client = get_redis_client()
                            
                            result_data = {
                                'total': total_count,  # ‚úÖ S·ªë file ƒë√£ t·∫£i (t·ªù khai + t·ªù thuy·∫øt minh)
                                'zip_filename': zip_filename,
                                'has_zip': False,
                                'download_id': download_id,
                                # ‚úÖ Forward th√™m c√°c field t·ª´ event ƒë·ªÉ frontend c√≥ th·ªÉ hi·ªÉn th·ªã chi ti·∫øt
                                'tokhai_downloaded': event.get('tokhai_downloaded'),
                                'tokhai_total': event.get('tokhai_total'),
                                'thuyet_minh_downloaded': event.get('thuyet_minh_downloaded'),
                                'thuyet_minh_total': event.get('thuyet_minh_total'),
                                'special_items_count': event.get('special_items_count'),
                                'message': event.get('message')
                            }
                            redis_client.set(f"job:{job_id}:result", json.dumps(result_data).encode('utf-8'))
                            redis_client.set(f"job:{job_id}:status", "completed".encode('utf-8'))
                            
                            publish_progress(job_id, 100, "Ho√†n th√†nh crawl", event)
                            logger.info(f"[API] Job {job_id} completed: {total_count} file (tokhai: {event.get('tokhai_downloaded', 0)}, thuyet_minh: {event.get('thuyet_minh_downloaded', 0)}), download_id: {download_id}")
                            
                        elif event_type == 'error':
                            error_msg = event.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')
                            from shared.redis_client import get_redis_client
                            redis_client = get_redis_client()
                            redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
                            redis_client.set(f"job:{job_id}:error", error_msg.encode('utf-8'))
                            publish_progress(job_id, 0, f"L·ªói: {error_msg}")
                            logger.error(f"[API] Job {job_id} error: {error_msg}")
                            
                except Exception as e:
                    error_msg = str(e)
                    logger.error(f"[API] Error in crawl_and_publish for job {job_id}: {error_msg}")
                    from shared.redis_client import get_redis_client
                    redis_client = get_redis_client()
                    redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
                    redis_client.set(f"job:{job_id}:error", error_msg.encode('utf-8'))
                    publish_progress(job_id, 0, f"L·ªói: {error_msg}")
            
            # Ch·∫°y crawl trong background
            asyncio.create_task(crawl_and_publish())
            
            return jsonify({
                "status": "accepted",
                "job_id": job_id,
                "message": "Crawl ƒë√£ ƒë∆∞·ª£c b·∫Øt ƒë·∫ßu, events s·∫Ω ƒë∆∞·ª£c publish v√†o Redis"
            })
            
        except Exception as e:
            logger.error(f"Error in crawl_tokhai: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/crawl/thongbao', methods=['POST'])
    async def crawl_thongbao():
        """
        Crawl th√¥ng b√°o (publish events to Redis)
        Body: {
            "job_id": "...",  # Job ID ƒë·ªÉ publish events
            "session_id": "...",
            "start_date": "01/01/2023",
            "end_date": "31/12/2023"
        }
        Returns: { "status": "accepted", "job_id": "..." }
        
        API s·∫Ω publish events v√†o Redis, worker s·∫Ω l·∫Øng nghe t·ª´ Redis
        """
        try:
            from quart import request
            import asyncio
            from shared.redis_client import publish_progress
            
            data = await request.get_json()
            job_id = data.get("job_id")
            session_id = data.get("session_id")
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            # ‚úÖ job_id l√† required (gi·ªëng t·ªù khai)
            if not all([job_id, session_id, start_date, end_date]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields: job_id, session_id, start_date, end_date"
                }), 400
            
            # ‚úÖ Check session tr∆∞·ªõc khi crawl (d√πng h√†m chung)
            is_valid, error_response = await check_session_before_crawl(session_id)
            if not is_valid:
                return jsonify(error_response), 401
            
            # Ch·∫°y crawl trong background task v√† publish events v√†o Redis
            # Ch·∫°y crawl trong background task v√† publish events v√†o Redis
            async def crawl_and_publish():
                try:
                    tc = get_tax_crawler()
                    results = []
                    total_count = 0
                    zip_filename = None
                    download_id = None
                    accumulated_total = 0
                    accumulated_downloaded = 0
                    
                    async for event in tc.crawl_thongbao(session_id, start_date, end_date, job_id=job_id):
                        # ‚úÖ Check cancelled tr∆∞·ªõc khi x·ª≠ l√Ω event ti·∫øp theo (gi·ªëng t·ªù khai)
                        from shared.redis_client import get_redis_client
                        check_redis = get_redis_client()
                        cancelled = check_redis.get(f"job:{job_id}:cancelled")
                        if cancelled:
                            cancelled = cancelled.decode('utf-8') if isinstance(cancelled, bytes) else str(cancelled).strip()
                            if cancelled == '1':
                                logger.info(f"[API] Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                                check_redis.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                                publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
                                break
                        
                        event_type = event.get('type', 'unknown')
                        
                        # ‚úÖ N·∫øu event l√† error v·ªõi JOB_CANCELLED, d·ª´ng ngay
                        if event_type == 'error' and event.get('error_code') == 'JOB_CANCELLED':
                            logger.info(f"[API] Job {job_id} ƒë√£ b·ªã cancel t·ª´ crawler")
                            check_redis.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                            publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy", event)
                            break
                        
                        if event_type == 'progress':
                            percent = event.get('percent', 0)
                            accumulated_percent = event.get('accumulated_percent', percent)
                            message = event.get('message', 'ƒêang x·ª≠ l√Ω...')
                            logger.debug(f"üì§ [ROUTES] [THONGBAO] Publish progress: {percent}% (accumulated: {accumulated_percent}%)")
                            publish_progress(job_id, accumulated_percent if accumulated_percent is not None else percent, message, event)
                            
                        elif event_type == 'info':
                            message = event.get('message', '')
                            accumulated_percent = event.get('accumulated_percent')
                            logger.debug(f"üì§ [ROUTES] [THONGBAO] Publish info: {message}")
                            publish_progress(job_id, accumulated_percent if accumulated_percent is not None else 0, message, event)
                            
                        elif event_type == 'download_start':
                            total = event.get('accumulated_total', event.get('total', 0))
                            accumulated_total = total
                            accumulated_percent = event.get('accumulated_percent', 0)
                            range_index = event.get('range_index', '?')
                            total_ranges = event.get('total_ranges', '?')
                            date_range = event.get('date_range', '?')
                            logger.debug(f"üì§ [ROUTES] [THONGBAO] Publish download_start: Range {range_index}/{total_ranges} ({date_range}), Total: {total}")
                            publish_progress(job_id, accumulated_percent if accumulated_percent is not None else 0, f"B·∫Øt ƒë·∫ßu t·∫£i {total} file...", event)
                            
                        elif event_type == 'download_progress':
                            current = event.get('accumulated_downloaded', event.get('current', 0))
                            total = event.get('accumulated_total', event.get('total', 0))
                            accumulated_total = total
                            accumulated_downloaded = current
                            accumulated_percent = event.get('accumulated_percent')
                            percent = accumulated_percent if accumulated_percent is not None else (int((current / total) * 100) if total > 0 else 0)
                            logger.debug(f"üì§ [ROUTES] [THONGBAO] Publish download_progress: {current}/{total} files, Accumulated %: {accumulated_percent}%")
                            publish_progress(job_id, percent, f"ƒê√£ t·∫£i {current}/{total} file", event)
                            
                        elif event_type == 'item':
                            results.append(event.get('data'))
                            
                        elif event_type == 'complete':
                            total_from_event = event.get('total', 0)
                            download_id = event.get('download_id')
                            zip_filename = event.get('zip_filename')
                            
                            if accumulated_total > 0:
                                total_count = accumulated_total
                            else:
                                total_count = total_from_event
                            
                            from shared.redis_client import get_redis_client
                            redis_client = get_redis_client()
                            
                            result_data = {
                                'total': total_count,
                                'zip_filename': zip_filename,
                                'has_zip': False,
                                'download_id': download_id
                            }
                            redis_client.set(f"job:{job_id}:result", json.dumps(result_data).encode('utf-8'))
                            redis_client.set(f"job:{job_id}:status", "completed".encode('utf-8'))
                            
                            publish_progress(job_id, 100, "Ho√†n th√†nh crawl", event)
                            
                        elif event_type == 'error':
                            error_msg = event.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')
                            from shared.redis_client import get_redis_client
                            redis_client = get_redis_client()
                            redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
                            redis_client.set(f"job:{job_id}:error", error_msg.encode('utf-8'))
                            publish_progress(job_id, 0, f"L·ªói: {error_msg}")
                            
                except Exception as e:
                    error_msg = str(e)
                    logger.error(f"[API] L·ªói trong qu√° tr√¨nh crawl th√¥ng b√°o cho job {job_id}: {error_msg}")
                    from shared.redis_client import get_redis_client
                    redis_client = get_redis_client()
                    redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
                    redis_client.set(f"job:{job_id}:error", error_msg.encode('utf-8'))
                    publish_progress(job_id, 0, f"L·ªói: {error_msg}")
            
            asyncio.create_task(crawl_and_publish())
            
            return jsonify({
                "status": "accepted",
                "job_id": job_id,
                "message": "Crawl ƒë√£ ƒë∆∞·ª£c b·∫Øt ƒë·∫ßu, events s·∫Ω ƒë∆∞·ª£c publish v√†o Redis"
            })
            
        except Exception as e:
            logger.error(f"Error in crawl_thongbao: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/crawl/giaynoptien', methods=['POST'])
    async def crawl_giaynoptien():
        """
        Crawl gi·∫•y n·ªôp ti·ªÅn
        - N·∫øu c√≥ job_id: publish events to Redis (queue mode)
        - N·∫øu kh√¥ng c√≥ job_id: streaming response (SSE mode - backward compatible)
        Body: {
            "job_id": "...",  # Optional - n·∫øu c√≥ th√¨ d√πng queue mode
            "session_id": "...",
            "start_date": "01/01/2023",
            "end_date": "31/12/2023"
        }
        """
        try:
            from quart import request, Response
            import asyncio
            from shared.redis_client import publish_progress
            
            data = await request.get_json()
            job_id = data.get("job_id")
            session_id = data.get("session_id")
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            # Validate required fields (job_id is optional)
            if not all([session_id, start_date, end_date]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields: session_id, start_date, end_date"
                }), 400
            
            # ‚úÖ Check session exists (n·∫øu backend restart, session s·∫Ω kh√¥ng t·ªìn t·∫°i)
            session_exists, error_response = check_session_exists(session_id)
            if not session_exists:
                # Tr·∫£ v·ªÅ 401 (Unauthorized) thay v√¨ 404 ƒë·ªÉ frontend bi·∫øt c·∫ßn login l·∫°i
                return jsonify(error_response), 401
            
            # ‚úÖ Check session validity tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu crawl (gi·ªëng nh∆∞ check trong login)
            # Check JSESSIONID hi·ªán t·∫°i so v·ªõi JSESSIONID ƒë√£ l∆∞u
            sm = get_session_manager()
            session_validity = await sm.check_session_validity(session_id)
            if not session_validity.get("valid", False):
                error_code = session_validity.get("error_code", "SESSION_EXPIRED")
                error_message = session_validity.get("error", "Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.")
                return jsonify({
                    "status": "error",
                    "error_code": error_code,
                    "message": error_message
                }), 401
            
            # N·∫øu kh√¥ng c√≥ job_id ‚Üí d√πng streaming mode (backward compatible)
            if not job_id:
                tc = get_tax_crawler()
                
                async def generate():
                    async for event in tc.crawl_giay_nop_tien(session_id, start_date, end_date, job_id=None):
                        yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
                
                return Response(
                    generate(),
                    mimetype='text/event-stream',
                    headers={
                        'Cache-Control': 'no-cache',
                        'X-Accel-Buffering': 'no'
                    }
                )
            
            # N·∫øu c√≥ job_id ‚Üí d√πng queue mode (publish to Redis)
            # Ch·∫°y crawl trong background task v√† publish events v√†o Redis
            async def crawl_and_publish():
                try:
                    tc = get_tax_crawler()
                    results = []
                    total_count = 0
                    zip_filename = None
                    download_id = None
                    accumulated_total = 0
                    accumulated_downloaded = 0
                    
                    async for event in tc.crawl_giay_nop_tien(session_id, start_date, end_date):
                        # ‚úÖ Check cancelled tr∆∞·ªõc khi x·ª≠ l√Ω event ti·∫øp theo
                        from shared.redis_client import get_redis_client
                        check_redis = get_redis_client()
                        cancelled = check_redis.get(f"job:{job_id}:cancelled")
                        if cancelled:
                            cancelled = cancelled.decode('utf-8') if isinstance(cancelled, bytes) else str(cancelled).strip()
                            if cancelled == '1':
                                logger.info(f"[API] Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                                check_redis.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                                publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
                                break
                        
                        event_type = event.get('type', 'unknown')
                        
                        # ‚úÖ N·∫øu event l√† error v·ªõi JOB_CANCELLED, d·ª´ng ngay
                        if event_type == 'error' and event.get('error_code') == 'JOB_CANCELLED':
                            logger.info(f"[API] Job {job_id} ƒë√£ b·ªã cancel t·ª´ crawler")
                            check_redis.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                            publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy", event)
                            break
                        
                        if event_type == 'progress':
                            # ‚úÖ Forward accumulated_percent v√† c√°c field kh√°c t·ª´ event ƒë·ªÉ kh√¥ng reset v·ªÅ 0%
                            percent = event.get('accumulated_percent', event.get('percent', 0))
                            if isinstance(percent, float):
                                percent = int(percent)
                            message = event.get('message', 'ƒêang x·ª≠ l√Ω...')
                            publish_progress(job_id, percent, message, event)
                            
                        elif event_type == 'info':
                            message = event.get('message', '')
                            # ‚úÖ Forward accumulated_percent v√† c√°c field kh√°c t·ª´ event ƒë·ªÉ kh√¥ng reset v·ªÅ 0%
                            percent = event.get('accumulated_percent', event.get('percent', 0))
                            if isinstance(percent, float):
                                percent = int(percent)
                            publish_progress(job_id, percent, message, event)
                            
                        elif event_type == 'download_start':
                            total = event.get('accumulated_total', event.get('total', 0))
                            accumulated_total = total
                            publish_progress(job_id, 0, f"B·∫Øt ƒë·∫ßu t·∫£i {total} file...", event)
                            
                        elif event_type == 'download_progress':
                            current = event.get('accumulated_downloaded', event.get('current', 0))
                            total = event.get('accumulated_total', event.get('total', 0))
                            accumulated_total = total
                            accumulated_downloaded = current
                            percent = int((current / total) * 100) if total > 0 else 0
                            publish_progress(job_id, percent, f"ƒê√£ t·∫£i {current}/{total} file", event)
                            
                        elif event_type == 'item':
                            results.append(event.get('data'))
                            
                        elif event_type == 'complete':
                            total_from_event = event.get('total', 0)
                            download_id = event.get('download_id')
                            zip_filename = event.get('zip_filename')
                            
                            if accumulated_total > 0:
                                total_count = accumulated_total
                            else:
                                total_count = total_from_event
                            
                            from shared.redis_client import get_redis_client
                            redis_client = get_redis_client()
                            
                            result_data = {
                                'total': total_count,
                                'zip_filename': zip_filename,
                                'has_zip': False,
                                'download_id': download_id
                            }
                            redis_client.set(f"job:{job_id}:result", json.dumps(result_data).encode('utf-8'))
                            redis_client.set(f"job:{job_id}:status", "completed".encode('utf-8'))
                            
                            publish_progress(job_id, 100, "Ho√†n th√†nh crawl", event)
                            
                        elif event_type == 'error':
                            error_msg = event.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')
                            from shared.redis_client import get_redis_client
                            redis_client = get_redis_client()
                            redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
                            redis_client.set(f"job:{job_id}:error", error_msg.encode('utf-8'))
                            publish_progress(job_id, 0, f"L·ªói: {error_msg}")
                
                except Exception as e:
                    error_msg = str(e)
                    logger.error(f"[API] L·ªói trong qu√° tr√¨nh crawl gi·∫•y n·ªôp ti·ªÅn cho job {job_id}: {error_msg}")
                    from shared.redis_client import get_redis_client
                    redis_client = get_redis_client()
                    redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
                    redis_client.set(f"job:{job_id}:error", error_msg.encode('utf-8'))
                    publish_progress(job_id, 0, f"L·ªói: {error_msg}")
            
            asyncio.create_task(crawl_and_publish())
            
            return jsonify({
                "status": "accepted",
                "job_id": job_id,
                "message": "Crawl ƒë√£ ƒë∆∞·ª£c b·∫Øt ƒë·∫ßu, events s·∫Ω ƒë∆∞·ª£c publish v√†o Redis"
            })
            
        except Exception as e:
            logger.error(f"Error in crawl_giaynoptien: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/download/<download_id>', methods=['GET'])
    async def download_zip(download_id: str):
        """
        Download zip file t·ª´ disk storage
        Worker s·∫Ω g·ªçi endpoint n√†y ƒë·ªÉ download zip file
        """
        try:
            from quart import request, Response
            tc = get_tax_crawler()
            
            # L·∫•y filename t·ª´ query param (optional)
            filename = request.args.get('filename', f'{download_id}.zip')
            
            # ƒê∆∞·ªùng d·∫´n file
            zip_file_path = os.path.join(tc.ZIP_STORAGE_DIR, f"{download_id}.zip")
            
            logger.info(f"Download request for {download_id}, checking file: {zip_file_path}")
            
            if not os.path.exists(zip_file_path):
                return jsonify({
                    "status": "error",
                    "message": f"File not found for download_id: {download_id}"
                }), 404
            
            # ‚úÖ Streaming file ƒë·ªÉ tr√°nh load to√†n b·ªô v√†o memory (quan tr·ªçng cho file l·ªõn)
            file_size = os.path.getsize(zip_file_path)
            logger.info(f"Sending file: {zip_file_path} as {filename} (size: {file_size} bytes)")
            
            async def generate():
                """Generator ƒë·ªÉ stream file theo chunk"""
                chunk_size = 8192  # 8KB chunks
                with open(zip_file_path, 'rb') as f:
                    while True:
                        chunk = f.read(chunk_size)
                        if not chunk:
                            break
                        yield chunk
            
            # Tr·∫£ v·ªÅ streaming response
            response = Response(
                generate(),
                mimetype='application/zip',
                headers={
                    'Content-Disposition': f'attachment; filename="{filename}"',
                    'Content-Length': str(file_size)
                }
            )
            return response
            
        except Exception as e:
            logger.error(f"Error downloading zip {download_id}: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/crawl/tokhai/sync', methods=['POST'])
    async def crawl_tokhai_sync():
        """
        Crawl t·ªù khai (synchronous response)
        D√πng khi client kh√¥ng h·ªó tr·ª£ SSE
        
        Body: {
            "session_id": "...",
            "tokhai_type": "842" ho·∫∑c "01/GTGT" ho·∫∑c "00" (T·∫•t c·∫£) ho·∫∑c null,
            "start_date": "01/01/2023",
            "end_date": "31/12/2023"
        }
        
        Note: N·∫øu tokhai_type = "00", null, ho·∫∑c kh√¥ng c√≥ ‚Üí crawl T·∫§T C·∫¢ lo·∫°i t·ªù khai
        """
        try:
            from quart import request
            data = await request.get_json()
            session_id = data.get("session_id")
            tokhai_type = data.get("tokhai_type")  # C√≥ th·ªÉ l√† None, "00", ho·∫∑c gi√° tr·ªã c·ª• th·ªÉ
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            if not all([session_id, start_date, end_date]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields: session_id, start_date, end_date"
                }), 400
            
            # N·∫øu kh√¥ng c√≥ tokhai_type ho·∫∑c r·ªóng ‚Üí m·∫∑c ƒë·ªãnh l√† "T·∫•t c·∫£"
            if not tokhai_type or tokhai_type.strip() == "":
                tokhai_type = "00"
            
            # Check session exists
            session_exists, error_response = check_session_exists(session_id)
            if not session_exists:
                return jsonify(error_response), 404
            
            tc = get_tax_crawler()
            
            results = []
            final_result = None
            
            async for event in tc.crawl_tokhai(session_id, tokhai_type, start_date, end_date):
                if event["type"] == "item":
                    results.append(event["data"])
                elif event["type"] == "complete":
                    final_result = event
                elif event["type"] == "error":
                    return jsonify({
                        "status": "error",
                        "message": event.get("error", "Unknown error")
                    }), 500
            
            if final_result:
                return jsonify({
                    "status": "success",
                    "total": final_result.get("total", len(results)),
                    "results": results,
                    "zip_base64": final_result.get("zip_base64")
                })
            else:
                return jsonify({
                    "status": "success",
                    "total": len(results),
                    "results": results
                })
            
        except Exception as e:
            logger.error(f"Error in crawl_tokhai_sync: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    # ==================== TOKHAI INFO & DOWNLOAD APIs ====================
    
    @app.route(f'{prefix}/crawl/tokhai/info', methods=['POST'])
    async def crawl_tokhai_info():
        """
        Ch·ªâ l·∫•y th√¥ng tin t·ªù khai (KH√îNG download file)
        D√πng ƒë·ªÉ hi·ªÉn th·ªã danh s√°ch tr∆∞·ªõc, user ch·ªçn t·∫£i sau
        
        Body: {
            "session_id": "...",
            "tokhai_type": "842" ho·∫∑c "01/GTGT" ho·∫∑c "00" (T·∫•t c·∫£) ho·∫∑c null,
            "start_date": "01/01/2023",
            "end_date": "31/12/2023"
        }
        
        Returns: {
            "status": "success",
            "total": 10,
            "results": [
                {
                    "id": "11320250305601017",
                    "name": "01/GTGT (TT80/2021)",
                    "ky_tinh_thue": "Q1/2024",
                    "loai": "Ch√≠nh th·ª©c",
                    "lan_nop": "1",
                    "lan_bo_sung": "",
                    "ngay_nop": "25/03/2025 15:22:00",
                    "noi_nop": "...",
                    "trang_thai": "accepted",
                    "trang_thai_text": "[Chap nhan]",
                    "file_name": "01_GTGT (TT80_2021) -Q1_2024 -L1 -Chinh thuc -(11320250305601017) -[25-03-2025 15-22-00] [Chap nhan].xml",
                    "has_download_link": true
                },
                ...
            ]
        }
        """
        try:
            from quart import request
            data = await request.get_json()
            session_id = data.get("session_id")
            tokhai_type = data.get("tokhai_type")
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            if not all([session_id, start_date, end_date]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields: session_id, start_date, end_date"
                }), 400
            
            if not tokhai_type or tokhai_type.strip() == "":
                tokhai_type = "00"
            
            # Check session exists
            session_exists, error_response = check_session_exists(session_id)
            if not session_exists:
                return jsonify(error_response), 404
            
            tc = get_tax_crawler()
            
            results = []
            final_result = None
            
            async for event in tc.crawl_tokhai_info(session_id, tokhai_type, start_date, end_date):
                if event["type"] == "item":
                    results.append(event["data"])
                elif event["type"] == "complete":
                    final_result = event
                elif event["type"] == "error":
                    return jsonify({
                        "status": "error",
                        "message": event.get("error", "Unknown error")
                    }), 500
            
            if final_result:
                return jsonify({
                    "status": "success",
                    "total": final_result.get("total", len(results)),
                    "results": results
                })
            else:
                return jsonify({
                    "status": "success",
                    "total": len(results),
                    "results": results
                })
            
        except Exception as e:
            logger.error(f"Error in crawl_tokhai_info: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/crawl/thongbao/sync', methods=['POST'])
    async def crawl_thongbao_sync():
        """Crawl th√¥ng b√°o (synchronous response)"""
        try:
            from quart import request
            data = await request.get_json()
            session_id = data.get("session_id")
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            if not all([session_id, start_date, end_date]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields"
                }), 400
            
            # Check session exists
            session_exists, error_response = check_session_exists(session_id)
            if not session_exists:
                return jsonify(error_response), 404
            
            tc = get_tax_crawler()
            
            async for event in tc.crawl_thongbao(session_id, start_date, end_date):
                if event["type"] == "complete":
                    return jsonify({
                        "status": "success",
                        "total": event.get("total", 0),
                        "results_count": event.get("results_count", 0),
                        "files_count": event.get("files_count", 0),
                        "total_size": event.get("total_size", 0),
                        "results": event.get("results", []),
                        "files": event.get("files", []),
                        "zip_base64": event.get("zip_base64"),
                        "zip_filename": event.get("zip_filename")
                    })
                elif event["type"] == "error":
                    return jsonify({
                        "status": "error",
                        "message": event.get("error", "Unknown error")
                    }), 500
            
            return jsonify({
                "status": "success",
                "total": 0,
                "results": []
            })
            
        except Exception as e:
            logger.error(f"Error in crawl_thongbao_sync: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/crawl/giaynoptien', methods=['POST'])
    async def crawl_giay_nop_tien():
        """
        Crawl gi·∫•y n·ªôp ti·ªÅn thu·∫ø (streaming response)
        Body: {
            "session_id": "...",
            "start_date": "01/01/2023",
            "end_date": "31/12/2023"
        }
        """
        from quart import request, Response
        try:
            data = await request.get_json()
            if not data:
                async def generate_error():
                    yield f"data: {json.dumps({'type': 'error', 'error_code': 'INVALID_REQUEST', 'error': 'Invalid request body'}, ensure_ascii=False)}\n\n"
                return Response(
                    generate_error(),
                    mimetype='text/event-stream',
                    headers={
                        'Cache-Control': 'no-cache',
                        'X-Accel-Buffering': 'no'
                    }
                )
            
            session_id = data.get("session_id")
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            if not all([session_id, start_date, end_date]):
                async def generate_error():
                    yield f"data: {json.dumps({'type': 'error', 'error_code': 'MISSING_REQUIRED_FIELDS', 'error': 'Missing required fields'}, ensure_ascii=False)}\n\n"
                return Response(
                    generate_error(),
                    mimetype='text/event-stream',
                    headers={
                        'Cache-Control': 'no-cache',
                        'X-Accel-Buffering': 'no'
                    }
                )
            
            # ‚úÖ Check session tr∆∞·ªõc khi crawl (d√πng h√†m chung)
            logger.info(f"[crawl_giaynoptien] Checking session: {session_id[:8]}...")
            try:
                is_valid, error_response = await check_session_before_crawl(session_id)
                if not is_valid:
                    # Tr·∫£ v·ªÅ error event trong SSE stream (status 200, kh√¥ng ph·∫£i 401)
                    error_code = error_response.get("error_code", "SESSION_EXPIRED")
                    error_message = error_response.get("message", "Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.")
                    logger.warning(f"[crawl_giaynoptien] Session invalid: {error_code} - {error_message}")
                    async def generate_error():
                        yield f"data: {json.dumps({'type': 'error', 'error_code': error_code, 'error': error_message}, ensure_ascii=False)}\n\n"
                    return Response(
                        generate_error(),
                        mimetype='text/event-stream',
                        headers={
                            'Cache-Control': 'no-cache',
                            'X-Accel-Buffering': 'no'
                        }
                    )
            except Exception as check_error:
                logger.error(f"[crawl_giaynoptien] Error checking session: {check_error}", exc_info=True)
                # N·∫øu c√≥ l·ªói khi check, tr·∫£ v·ªÅ error event trong SSE stream
                async def generate_error():
                    yield f"data: {json.dumps({'type': 'error', 'error_code': 'SESSION_EXPIRED', 'error': 'Phi√™n ƒëƒÉng nh·∫≠p ƒë√£ h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.'}, ensure_ascii=False)}\n\n"
                return Response(
                    generate_error(),
                    mimetype='text/event-stream',
                    headers={
                        'Cache-Control': 'no-cache',
                        'X-Accel-Buffering': 'no'
                    }
                )
            
            tc = get_tax_crawler()
            
            async def generate():
                async for event in tc.crawl_giay_nop_tien(session_id, start_date, end_date):
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
            
            return Response(
                generate(),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no'
                }
            )
            
        except Exception as e:
            logger.error(f"Error in crawl_giay_nop_thue: {e}", exc_info=True)
            # ‚úÖ Tr·∫£ v·ªÅ error event trong SSE stream thay v√¨ HTTP error
            error_message = str(e)
            # Check xem c√≥ ph·∫£i l√† session error kh√¥ng
            if "session" in error_message.lower() or "Session" in error_message:
                error_code = "SESSION_EXPIRED"
            else:
                error_code = "CRAWL_ERROR"
            
            async def generate_error():
                yield f"data: {json.dumps({'type': 'error', 'error_code': error_code, 'error': error_message}, ensure_ascii=False)}\n\n"
            return Response(
                generate_error(),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no'
                }
            )
    
    @app.route(f'{prefix}/crawl/giaynoptien/sync', methods=['POST'])
    async def crawl_giay_nop_tien_sync():
        """Crawl gi·∫•y n·ªôp ti·ªÅn thu·∫ø (synchronous response)"""
        try:
            from quart import request
            data = await request.get_json()
            session_id = data.get("session_id")
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            
            if not all([session_id, start_date, end_date]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields"
                }), 400
            
            # ‚úÖ Check session tr∆∞·ªõc khi crawl (d√πng h√†m chung)
            is_valid, error_response = await check_session_before_crawl(session_id)
            if not is_valid:
                return jsonify(error_response), 401
            
            tc = get_tax_crawler()
            
            async for event in tc.crawl_giay_nop_tien(session_id, start_date, end_date):
                if event["type"] == "complete":
                    return jsonify({
                        "status": "success",
                        "total": event.get("total", 0),
                        "results_count": event.get("results_count", 0),
                        "files_count": event.get("files_count", 0),
                        "total_size": event.get("total_size", 0),
                        "results": event.get("results", []),
                        "files": event.get("files", []),
                        "zip_base64": event.get("zip_base64"),
                        "zip_filename": event.get("zip_filename")
                    })
                elif event["type"] == "error":
                    return jsonify({
                        "status": "error",
                        "message": event.get("error", "Unknown error")
                    }), 500
            
            return jsonify({
                "status": "success",
                "total": 0,
                "results": []
            })
            
        except Exception as e:
            logger.error(f"Error in crawl_giay_nop_thue_sync: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    # ==================== CONVERT APIs ====================
    
    @app.route(f'{prefix}/convert/xml2xlsx', methods=['POST'])
    async def convert_xml_to_xlsx():
        """
        Chuy·ªÉn ƒë·ªïi XML sang Excel
        Body: { "zip_base64": "..." } ho·∫∑c upload file
        """
        try:
            from quart import request
            zip_base64 = None
            
            # C√°ch 1: Upload file
            files = await request.files
            if 'file' in files:
                file = files['file']
                if file.filename == '':
                    return jsonify({
                        "status": "error",
                        "message": "No file selected"
                    }), 400
                
                file_content = await file.read()
                zip_base64 = base64.b64encode(file_content).decode('utf-8')
            
            # C√°ch 2: JSON v·ªõi base64
            elif request.is_json:
                data = await request.get_json()
                zip_base64 = data.get("zip_base64")
            
            if not zip_base64:
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing zip_base64 or file"
                }), 400
            
            tc = get_tax_crawler()
            result = await tc.convert_xml_to_xlsx(zip_base64)
            
            if result["success"]:
                return jsonify({
                    "status": "success",
                    "xlsx_base64": result["xlsx_base64"],
                    "row_count": result["row_count"]
                })
            else:
                return jsonify({
                    "status": "error",
                    "message": result.get("error", "Conversion failed")
                }), 500
                
        except Exception as e:
            logger.error(f"Error in convert_xml_to_xlsx: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    # ==================== DEBUG APIs ====================
    
    @app.route(f'{prefix}/debug/screenshot', methods=['POST'])
    async def debug_screenshot():
        """
        L·∫•y screenshot c·ªßa page hi·ªán t·∫°i ƒë·ªÉ debug
        Body: { "session_id": "..." }
        """
        try:
            from quart import request
            import base64
            
            data = await request.get_json()
            session_id = data.get("session_id")
            
            if not session_id:
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_SESSION_ID",
                    "message": "Missing session_id"
                }), 400
            
            sm = get_session_manager()
            session = sm.get_session(session_id)
            
            if not session:
                return jsonify({
                    "status": "error",
                    "error_code": "SESSION_NOT_FOUND",
                    "message": "Session not found"
                }), 404
            
            page = session.page
            screenshot = await page.screenshot(full_page=True)
            screenshot_base64 = base64.b64encode(screenshot).decode('utf-8')
            
            return jsonify({
                "status": "success",
                "current_url": page.url,
                "screenshot_base64": screenshot_base64
            })
            
        except Exception as e:
            logger.error(f"Error in debug_screenshot: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    @app.route(f'{prefix}/debug/navigate', methods=['POST'])
    async def debug_navigate():
        """
        Navigate ƒë·∫øn m·ªôt URL c·ª• th·ªÉ ƒë·ªÉ debug
        Body: { "session_id": "...", "url": "..." }
        """
        try:
            from quart import request
            
            data = await request.get_json()
            session_id = data.get("session_id")
            url = data.get("url")
            
            if not session_id or not url:
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing session_id or url"
                }), 400
            
            sm = get_session_manager()
            session = sm.get_session(session_id)
            
            if not session:
                return jsonify({
                    "status": "error",
                    "error_code": "SESSION_NOT_FOUND",
                    "message": "Session not found"
                }), 404
            
            page = session.page
            await page.goto(url, wait_until='networkidle')
            
            return jsonify({
                "status": "success",
                "current_url": page.url
            })
            
        except Exception as e:
            logger.error(f"Error in debug_navigate: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    # ==================== BATCH CRAWL (Parallel) ====================
    
    @app.route(f'{prefix}/crawl/batch', methods=['POST'])
    async def batch_crawl():
        """
        Crawl nhi·ªÅu lo·∫°i d·ªØ li·ªáu ƒë·ªìng th·ªùi (streaming response)
        
        Body: {
            "session_id": "...",
            "start_date": "01/01/2023",
            "end_date": "31/12/2023",
            "crawl_types": ["tokhai", "thongbao", "giaynoptien"],
            "tokhai_type": "01/GTGT" ho·∫∑c "00" (T·∫•t c·∫£) ho·∫∑c null
        }
        Returns: Server-Sent Events stream
        """
        try:
            from quart import request, Response
            data = await request.get_json()
            session_id = data.get("session_id")
            tokhai_type = data.get("tokhai_type", "00")  # M·∫∑c ƒë·ªãnh "T·∫•t c·∫£"
            start_date = data.get("start_date")
            end_date = data.get("end_date")
            crawl_types = data.get("crawl_types", [])
            
            if not all([session_id, start_date, end_date, crawl_types]):
                return jsonify({
                    "status": "error",
                    "error_code": "MISSING_REQUIRED_FIELDS",
                    "message": "Missing required fields: session_id, start_date, end_date, crawl_types"
                }), 400
            
            # Validate crawl_types
            valid_types = ["tokhai", "thongbao", "giaynoptien"]
            crawl_types = [t for t in crawl_types if t in valid_types]
            
            if not crawl_types:
                return jsonify({
                    "status": "error",
                    "error_code": "INVALID_CRAWL_TYPES",
                    "message": "Kh√¥ng c√≥ lo·∫°i crawl h·ª£p l·ªá. Ch·ªçn t·ª´: tokhai, thongbao, giaynoptien"
                }), 400
            
            # Check session exists
            session_exists, error_response = check_session_exists(session_id)
            if not session_exists:
                return jsonify(error_response), 404
            
            tc = get_tax_crawler()
            
            async def generate():
                async for event in tc.crawl_batch(session_id, start_date, end_date, crawl_types, tokhai_type):
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
            
            return Response(
                generate(),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no'
                }
            )
            
        except Exception as e:
            logger.error(f"Error in batch_crawl: {e}")
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    # ==================== DOWNLOAD ZIP FILE ====================
    
    @app.route(f'{prefix}/download/<download_id>', methods=['GET'], endpoint='go_soft_download_zip')
    async def download_zip(download_id):
        """
        Download ZIP file b·∫±ng download_id
        
        Query params:
            - filename: T√™n file ZIP (optional)
        """
        try:
            from services.tax_crawler import TaxCrawlerService
            from quart import Response
            import os
            
            # L·∫•y ZIP_STORAGE_DIR t·ª´ TaxCrawlerService
            zip_storage_dir = TaxCrawlerService.ZIP_STORAGE_DIR
            zip_file_path = os.path.join(zip_storage_dir, f"{download_id}.zip")
            
            # Ki·ªÉm tra file t·ªìn t·∫°i
            if not os.path.exists(zip_file_path):
                logger.warning(f"Download request for {download_id}, file not found: {zip_file_path}")
                return jsonify({
                    "status": "error",
                    "error_code": "FILE_NOT_FOUND",
                    "message": "ZIP file not found"
                }), 404
            
            # L·∫•y filename t·ª´ query params ho·∫∑c d√πng default
            filename = request.args.get('filename', f"{download_id}.zip")
            
            # ƒê·ªçc file v√† tr·∫£ v·ªÅ
            with open(zip_file_path, 'rb') as f:
                zip_content = f.read()
            
            logger.info(f"Download request for {download_id}, sending file: {zip_file_path} as {filename}")
            
            # ‚úÖ Th√™m CORS headers ƒë·ªÉ frontend c√≥ th·ªÉ download
            headers = {
                'Content-Type': 'application/zip',
                'Content-Disposition': f'attachment; filename="{filename}"',
                'Content-Length': str(len(zip_content)),
                'Access-Control-Allow-Origin': '*',  # ‚úÖ Cho ph√©p t·∫•t c·∫£ origins (ho·∫∑c set c·ª• th·ªÉ: 'https://gotax.vn')
                'Access-Control-Allow-Methods': 'GET, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type',
            }
            
            return Response(
                zip_content,
                mimetype='application/zip',
                headers=headers
            )
            
        except Exception as e:
            logger.error(f"Error in download_zip: {e}")
            import traceback
            traceback.print_exc()
            return jsonify({
                "status": "error",
                "message": str(e)
            }), 500
    
    # ‚úÖ Handle OPTIONS request cho CORS preflight
    @app.route(f'{prefix}/download/<download_id>', methods=['OPTIONS'])
    async def download_zip_options(download_id):
        """Handle CORS preflight request"""
        from quart import Response
        return Response(
            '',
            status=200,
            headers={
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type',
            }
        )

