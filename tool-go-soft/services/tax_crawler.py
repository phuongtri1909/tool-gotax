import os
import asyncio
import base64
import logging
import tempfile
import shutil
import re
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, AsyncGenerator
from io import BytesIO
import zipfile
import uuid

import httpx
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import warnings
from openpyxl import Workbook

# Suppress XMLParsedAsHTMLWarning khi parse XML v·ªõi html.parser
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)
from openpyxl.styles import Font, Border, Side
from openpyxl.utils import get_column_letter
from openpyxl.styles.numbers import FORMAT_NUMBER_COMMA_SEPARATED1

from .session_manager import SessionManager, SessionData

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Mapping lo·∫°i t·ªù khai
TOKHAI_TYPES = {
    "01/GTGT": "01/GTGT",
    "01/GTGT (TT80/2021)": "01/GTGT (TT80/2021)",
    "05/QTT-TNCN": "05/QTT-TNCN",
    "03/TNDN": "03/TNDN",
    "01A/TNDN": "01A/TNDN",
    "01B/TNDN": "01B/TNDN",
    "02/TNDN": "02/TNDN",
    "05/KK-TNCN": "05/KK-TNCN",
    "06/KK-TNCN": "06/KK-TNCN",
    "01/MBAI": "01/MBAI",
    "01/LPMB": "01/LPMB",
}

# Base URL
BASE_URL = "https://thuedientu.gdt.gov.vn"


class TaxCrawlerService:
    ZIP_STORAGE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'temp')
    
    def __init__(self, session_manager: SessionManager):
        self.session_manager = session_manager
        os.makedirs(self.ZIP_STORAGE_DIR, exist_ok=True)
        self._http_clients: Dict[str, httpx.AsyncClient] = {}
    
    async def _get_http_client(self, session_id: str) -> Optional[httpx.AsyncClient]:
        """
        L·∫•y ho·∫∑c t·∫°o httpx client v·ªõi cookies t·ª´ session
        D√πng ƒë·ªÉ crawl nhanh sau khi login
        """
        session = self.session_manager.get_session(session_id)
        if not session or not session.is_logged_in:
            return None
        
        if session_id not in self._http_clients:
            cookies = await self.session_manager.get_cookies_for_httpx(session_id)
            if not cookies:
                return None
            
            self._http_clients[session_id] = httpx.AsyncClient(
                cookies=cookies,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
                },
                timeout=30.0,
                verify=False,
                follow_redirects=True
            )
        
        return self._http_clients[session_id]
    
    async def close_http_client(self, session_id: str):
        """ƒê√≥ng httpx client khi session k·∫øt th√∫c"""
        if session_id in self._http_clients:
            await self._http_clients[session_id].aclose()
            del self._http_clients[session_id]
    
    async def _check_cancelled(self, job_id: str) -> bool:
        """
        Ki·ªÉm tra xem job c√≥ b·ªã cancel kh√¥ng
        
        Returns:
            True n·∫øu job b·ªã cancel, False n·∫øu kh√¥ng
        """
        try:
            from shared.redis_client import get_redis_client
            redis_client = get_redis_client()
            cancelled = redis_client.get(f"job:{job_id}:cancelled")
            if cancelled:
                cancelled = cancelled.decode('utf-8') if isinstance(cancelled, bytes) else str(cancelled).strip()
                if cancelled == '1':
                    return True
            
            # Check status
            status = redis_client.get(f"job:{job_id}:status")
            if status:
                status = status.decode('utf-8') if isinstance(status, bytes) else str(status).strip()
                if status == 'cancelled':
                    return True
            return False
        except Exception as e:
            logger.warning(f"Error checking cancelled flag: {e}")
            return False
    
    async def _check_session_timeout(self, page) -> bool:
        """
        Ki·ªÉm tra xem session c√≥ b·ªã timeout kh√¥ng
        
        Returns:
            True n·∫øu session timeout, False n·∫øu kh√¥ng
        """
        try:
            current_url = page.url
            
            # Check URL timeout
            if 'timeout.jsp' in current_url:
                logger.warning("Session timeout detected from URL")
                return True
            
            # Check content timeout
            try:
                # T√¨m text "Phi√™n giao d·ªãch h·∫øt h·∫°n"
                timeout_text = page.locator('text=Phi√™n giao d·ªãch h·∫øt h·∫°n')
                if await timeout_text.count() > 0:
                    logger.warning("Session timeout detected from content")
                    return True
                
                # T√¨m n√∫t "Tr·ªü l·∫°i" v·ªõi onclick ch·ª©a corpIndexProc
                back_button = page.locator('input[type="button"][onclick*="corpIndexProc"]')
                if await back_button.count() > 0:
                    logger.warning("Session timeout detected from back button")
                    return True
            except Exception as e:
                logger.debug(f"Error checking timeout content: {e}")
            
            return False
        except Exception as e:
            logger.error(f"Error in _check_session_timeout: {e}")
            return False
    
    def _get_date_ranges(self, start_date: str, end_date: str, days_interval: int = 350) -> List[List[str]]:
        date_format = "%d/%m/%Y"
        date1 = datetime.strptime(start_date, date_format)
        date2 = datetime.strptime(end_date, date_format)
        interval = timedelta(days=days_interval)
        
        date_ranges = []
        while date1 <= date2:
            sub_array = [date1.strftime(date_format)]
            date1 += interval
            if date1 > date2:
                date1 = date2
            sub_array.append(date1.strftime(date_format))
            date_ranges.append(sub_array)
            date1 += timedelta(days=1)
        
        return date_ranges
    
    def _calculate_days_between(self, start_date: str, end_date: str) -> int:
        """T√≠nh s·ªë ng√†y gi·ªØa 2 ng√†y (format: DD/MM/YYYY)"""
        date_format = "%d/%m/%Y"
        try:
            date1 = datetime.strptime(start_date, date_format)
            date2 = datetime.strptime(end_date, date_format)
            return (date2 - date1).days + 1  # +1 ƒë·ªÉ t√≠nh c·∫£ ng√†y cu·ªëi
        except Exception as e:
            logger.warning(f"Error calculating days between {start_date} and {end_date}: {e}")
            return 0
    
    def _normalize_tokhai_name(self, name_tk: str) -> str:
        """Chu·∫©n h√≥a t√™n t·ªù khai"""
        if "T·ªú KHAI QUY·∫æT TO√ÅN THU·∫æ THU NH·∫¨P C√Å NH√ÇN" in name_tk:
            if "(TT92/2015)" in name_tk:
                return "05/QTT-TNCN (TT92/2015)"
            elif "TT80/2021" in name_tk:
                return "05/QTT-TNCN (TT80/2021)"
        elif "03/TNDN" in name_tk and "(TT80/2021)" in name_tk:
            return "03/TNDN (TT80/2021)"
        elif "01A/TNDN" in name_tk:
            return "01A/TNDN"
        elif "01B/TNDN" in name_tk:
            return "01B/TNDN"
        elif "02/TNDN" in name_tk:
            return "02/TNDN"
        elif "06/KK-TNCN" in name_tk and "(TT156/2013)" in name_tk:
            return "06/KK-TNCN (TT156/2013)"
        elif "05/KK-TNCN" in name_tk and "(TT92/2015)" in name_tk:
            return "05/KK-TNCN (TT92/2015)"
        elif "05/KK-TNCN" in name_tk and "(TT80)" in name_tk:
            return "05/KK-TNCN (TT80)"
        elif "01/GTGT" in name_tk and "(GTGT)" in name_tk:
            return "01/GTGT (GTGT)"
        elif "01/GTGT" in name_tk and "(TT80/2021)" in name_tk:
            return "01/GTGT (TT80/2021)"
        elif "01/MBAI" in name_tk and "(TT156/2013)" in name_tk:
            return "01/MBAI (TT156/2013)"
        elif "01/LPMB" in name_tk and "(TT80/2021)" in name_tk:
            return "01/LPMB (TT80/2021)"
        
        return name_tk
    
    async def _navigate_to_tokhai_page(self, page, dse_session_id: str) -> bool:
        success = False
        frame = None
        
        try:
            # B∆∞·ªõc 1: Navigate ƒë·∫øn trang dich-vu-khac
            # QUAN TR·ªåNG: Page m·ªõi c·∫ßn navigate ƒë·∫øn ƒë√∫ng URL, kh√¥ng d√πng current_url
            # V√¨ page m·ªõi c√≥ th·ªÉ ch∆∞a c√≥ URL ho·∫∑c URL kh√¥ng ƒë√∫ng
            logger.info("Navigating to /tthc/dich-vu-khac...")
            try:
                # ƒê·∫£m b·∫£o navigate ƒë·∫øn ƒë√∫ng URL (kh√¥ng ph·∫£i homelogin)
                target_url = 'https://dichvucong.gdt.gov.vn/tthc/dich-vu-khac'
                current_url = page.url
                
                # N·∫øu ƒëang ·ªü homelogin ho·∫∑c URL kh√°c, navigate l·∫°i
                if '/tthc/dich-vu-khac' not in current_url:
                    await page.goto(target_url, wait_until='domcontentloaded', timeout=30000)
                    await asyncio.sleep(2)
                    logger.info(f"Successfully navigated to dich-vu-khac, current URL: {page.url}")
                else:
                    logger.info(f"Already on dich-vu-khac page: {current_url}")
            except Exception as nav_err:
                logger.error(f"Error navigating to dich-vu-khac: {nav_err}")
                return False
            
            # B∆∞·ªõc 2: G·ªçi tr·ª±c ti·∫øp h√†m JavaScript connectSSO('360103', '', '', '')
            logger.info("Calling connectSSO('360103', '', '', '') via JavaScript...")
            
            try:
                # G·ªçi h√†m connectSSO tr·ª±c ti·∫øp b·∫±ng JavaScript
                await page.evaluate("""
                    async () => {
                        // Ki·ªÉm tra xem h√†m connectSSO c√≥ t·ªìn t·∫°i kh√¥ng
                        if (typeof connectSSO === 'function') {
                            await connectSSO('360103', '', '', '');
                            return { success: true, message: 'connectSSO called' };
                        } else {
                            return { success: false, message: 'connectSSO function not found' };
                        }
                    }
                """)
                logger.info("connectSSO('360103', '', '', '') called successfully")
                # ƒê·ª£i AJAX ho√†n t·∫•t v√† iframe ƒë∆∞·ª£c set src
                await asyncio.sleep(3)
            except Exception as e:
                logger.error(f"Error calling connectSSO: {e}")
                return False
            
            # B∆∞·ªõc 3: ƒê·ª£i iframe load v·ªõi src t·ª´ thuedientu.gdt.gov.vn
            logger.info("Waiting for iframe to load with thuedientu.gdt.gov.vn...")
            
            # T√¨m iframe trong #iframeRenderSSO
            max_wait = 20  # ƒê·ª£i t·ªëi ƒëa 10 gi√¢y (20 * 0.5)
            for i in range(max_wait):
                try:
                    # T√¨m iframe trong modal #iframeRenderSSO
                    iframe_elem = page.locator('#iframeRenderSSO iframe').first
                    if await iframe_elem.count() > 0:
                        # L·∫•y src c·ªßa iframe
                        iframe_src = await iframe_elem.get_attribute('src')
                        if iframe_src and 'thuedientu.gdt.gov.vn' in iframe_src:
                            logger.info(f"Found iframe with src: {iframe_src[:100]}...")
                            
                            # T√¨m frame t·ª´ page.frames
                            frames = page.frames
                            for f in frames:
                                if 'thuedientu.gdt.gov.vn' in f.url:
                                    frame = f
                                    logger.info(f"Found frame: {frame.url[:100]}...")
                                    break
                            
                            if frame:
                                break
                except Exception as e:
                    logger.debug(f"Waiting for iframe (attempt {i + 1}/{max_wait}): {e}")
                
                await asyncio.sleep(0.5)
            
            # B∆∞·ªõc 4: Switch v√†o iframe v√† ƒë·ª£i #maTKhai xu·∫•t hi·ªán
            if frame:
                try:
                    logger.info("Waiting for #maTKhai in iframe...")
                    await frame.wait_for_load_state('domcontentloaded', timeout=15000)
                    await asyncio.sleep(1)
                    await frame.wait_for_selector('#maTKhai', timeout=15000)
                    success = True
                    logger.info("Tra cuu tokhai page loaded successfully via SSO iframe")
                except Exception as e:
                    logger.warning(f"Frame found but #maTKhai not found: {e}")
                    # Th·ª≠ ƒë·ª£i th√™m m·ªôt ch√∫t
                    try:
                        await asyncio.sleep(2)
                        await frame.wait_for_selector('#maTKhai', timeout=10000)
                        success = True
                        logger.info("Tra cuu tokhai page loaded after additional wait")
                    except:
                        logger.error("Still cannot find #maTKhai after additional wait")
            else:
                logger.error("Iframe not found after clicking connectSSO link")
            
            return success
            
        except Exception as e:
            logger.error(f"Error navigating to tokhai page: {e}")
            return False
    
    async def _navigate_to_tokhai_search(self, session: SessionData) -> bool:
        return await self._navigate_to_tokhai_page(session.page, session.dse_session_id)
    
    async def _navigate_to_thongbao_page(self, page, dse_session_id: str) -> bool:
        success = False
        frame = None
        
        try:
            # B∆∞·ªõc 1: Navigate ƒë·∫øn trang dich-vu-khac
            current_url = page.url
            if '/tthc/dich-vu-khac' not in current_url:
                logger.info("Navigating to /tthc/dich-vu-khac for thongbao...")
                await page.goto('https://dichvucong.gdt.gov.vn/tthc/dich-vu-khac', wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(2)
            else:
                logger.info("Already on /tthc/dich-vu-khac page")
            
            # B∆∞·ªõc 2: G·ªçi tr·ª±c ti·∫øp h√†m JavaScript connectSSO('360102', '', '', '')
            logger.info("Calling connectSSO('360102', '', '', '') via JavaScript...")
            
            try:
                # G·ªçi h√†m connectSSO tr·ª±c ti·∫øp b·∫±ng JavaScript
                await page.evaluate("""
                    async () => {
                        // Ki·ªÉm tra xem h√†m connectSSO c√≥ t·ªìn t·∫°i kh√¥ng
                        if (typeof connectSSO === 'function') {
                            await connectSSO('360102', '', '', '');
                            return { success: true, message: 'connectSSO called' };
                        } else {
                            return { success: false, message: 'connectSSO function not found' };
                        }
                    }
                """)
                logger.info("connectSSO('360102', '', '', '') called successfully")
                # ƒê·ª£i AJAX ho√†n t·∫•t v√† iframe ƒë∆∞·ª£c set src
                await asyncio.sleep(3)
            except Exception as e:
                logger.error(f"Error calling connectSSO for thongbao: {e}")
                return False
            
            # B∆∞·ªõc 3: ƒê·ª£i iframe load v·ªõi src t·ª´ thuedientu.gdt.gov.vn
            logger.info("Waiting for iframe to load with thuedientu.gdt.gov.vn for thongbao...")
            
            # T√¨m iframe trong #iframeRenderSSO
            max_wait = 20  # ƒê·ª£i t·ªëi ƒëa 10 gi√¢y (20 * 0.5)
            for i in range(max_wait):
                try:
                    # T√¨m iframe trong modal #iframeRenderSSO
                    iframe_elem = page.locator('#iframeRenderSSO iframe').first
                    if await iframe_elem.count() > 0:
                        # L·∫•y src c·ªßa iframe
                        iframe_src = await iframe_elem.get_attribute('src')
                        if iframe_src and 'thuedientu.gdt.gov.vn' in iframe_src:
                            logger.info(f"Found iframe with src: {iframe_src[:100]}...")
                            
                            # T√¨m frame t·ª´ page.frames
                            frames = page.frames
                            for f in frames:
                                if 'thuedientu.gdt.gov.vn' in f.url:
                                    frame = f
                                    logger.info(f"Found frame: {frame.url[:100]}...")
                                    break
                            
                            if frame:
                                break
                except Exception as e:
                    logger.debug(f"Waiting for iframe (attempt {i + 1}/{max_wait}): {e}")
                
                await asyncio.sleep(0.5)
            
            # B∆∞·ªõc 4: Switch v√†o iframe v√† ƒë·ª£i form th√¥ng b√°o xu·∫•t hi·ªán
            if frame:
                try:
                    logger.info("Waiting for thong bao form in iframe...")
                    await frame.wait_for_load_state('domcontentloaded', timeout=15000)
                    await asyncio.sleep(1)
                    # ƒê·ª£i form th√¥ng b√°o load - ki·ªÉm tra input qryFromDate
                    await frame.wait_for_selector('#qryFromDate', timeout=15000)
                    success = True
                    logger.info("Tra cuu thong bao page loaded successfully via SSO iframe")
                except Exception as e:
                    logger.warning(f"Frame found but form not found: {e}")
                    # Th·ª≠ ƒë·ª£i th√™m m·ªôt ch√∫t
                    try:
                        await asyncio.sleep(2)
                        await frame.wait_for_selector('#qryFromDate', timeout=10000)
                        success = True
                        logger.info("Tra cuu thong bao page loaded after additional wait")
                    except:
                        logger.error("Still cannot find form after additional wait")
            else:
                logger.error("Iframe not found after calling connectSSO for thongbao")
            
            return success
            
        except Exception as e:
            logger.error(f"Error navigating to thongbao page: {e}")
            return False
    
    async def _navigate_to_giaynoptien_page(self, page, dse_session_id: str) -> bool:
        """
        Navigate ƒë·∫øn trang tra c·ª©u gi·∫•y n·ªôp ti·ªÅn qua dichvucong.gdt.gov.vn
        Gi·ªëng nh∆∞ t·ªù khai nh∆∞ng d√πng connectSSO('330410')
        
        Flow:
        1. Navigate ƒë·∫øn /tthc/dich-vu-khac
        2. G·ªçi connectSSO('330410', '', '', '')
        3. ƒê·ª£i iframe load v·ªõi src t·ª´ thuedientu.gdt.gov.vn
        4. Switch v√†o iframe v√† ƒë·ª£i form gi·∫•y n·ªôp ti·ªÅn xu·∫•t hi·ªán
        
        Returns: True n·∫øu th√†nh c√¥ng
        """
        success = False
        frame = None
        
        try:
            # B∆∞·ªõc 1: Navigate ƒë·∫øn trang dich-vu-khac
            current_url = page.url
            if '/tthc/dich-vu-khac' not in current_url:
                logger.info("Navigating to /tthc/dich-vu-khac for giaynoptien...")
                await page.goto('https://dichvucong.gdt.gov.vn/tthc/dich-vu-khac', wait_until='domcontentloaded', timeout=30000)
                await asyncio.sleep(2)
            else:
                logger.info("Already on /tthc/dich-vu-khac page")
            
            # B∆∞·ªõc 2: G·ªçi tr·ª±c ti·∫øp h√†m JavaScript connectSSO('330410', '', '', '')
            logger.info("Calling connectSSO('330410', '', '', '') via JavaScript...")
            
            try:
                # G·ªçi h√†m connectSSO tr·ª±c ti·∫øp b·∫±ng JavaScript
                await page.evaluate("""
                    async () => {
                        // Ki·ªÉm tra xem h√†m connectSSO c√≥ t·ªìn t·∫°i kh√¥ng
                        if (typeof connectSSO === 'function') {
                            await connectSSO('330410', '', '', '');
                            return { success: true, message: 'connectSSO called' };
                        } else {
                            return { success: false, message: 'connectSSO function not found' };
                        }
                    }
                """)
                logger.info("connectSSO('330410', '', '', '') called successfully")
                # ƒê·ª£i AJAX ho√†n t·∫•t v√† iframe ƒë∆∞·ª£c set src
                await asyncio.sleep(3)
            except Exception as e:
                logger.error(f"Error calling connectSSO for giaynoptien: {e}")
                return False
            
            # B∆∞·ªõc 3: ƒê·ª£i iframe load v·ªõi src t·ª´ thuedientu.gdt.gov.vn
            logger.info("Waiting for iframe to load with thuedientu.gdt.gov.vn for giaynoptien...")
            
            # T√¨m frame tr·ª±c ti·∫øp t·ª´ page.frames (ƒë√°ng tin c·∫≠y h∆°n)
            max_wait = 30  # ƒê·ª£i t·ªëi ƒëa 15 gi√¢y (30 * 0.5)
            frame = None
            for i in range(max_wait):
                try:
                    # T√¨m frame t·ª´ page.frames tr·ª±c ti·∫øp
                    frames = page.frames
                    for f in frames:
                        if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                            frame = f
                            logger.info(f"Found frame: {frame.url[:100]}...")
                            break
                    
                    if frame:
                        # Ki·ªÉm tra xem frame ƒë√£ load ch∆∞a
                        try:
                            await frame.wait_for_load_state('domcontentloaded', timeout=2000)
                            break
                        except:
                            # Frame ch∆∞a load xong, ti·∫øp t·ª•c ƒë·ª£i
                            frame = None
                            pass
                except Exception as e:
                    logger.debug(f"Waiting for frame (attempt {i + 1}/{max_wait}): {e}")
                
                await asyncio.sleep(0.5)
            
            # B∆∞·ªõc 4: Switch v√†o iframe v√† ƒë·ª£i form gi·∫•y n·ªôp ti·ªÅn xu·∫•t hi·ªán
            if frame:
                try:
                    logger.info("Waiting for giay nop tien form in iframe...")
                    await frame.wait_for_load_state('domcontentloaded', timeout=15000)
                    await asyncio.sleep(1)
                    # ƒê·ª£i form gi·∫•y n·ªôp ti·ªÅn load
                    await frame.wait_for_selector('input[name="ngay_lap_tu_ngay"], #ngay_lap_tu_ngay', timeout=15000)
                    success = True
                    logger.info("Tra cuu giay nop tien page loaded successfully via SSO iframe")
                except Exception as e:
                    logger.warning(f"Frame found but form not found: {e}")
                    # Th·ª≠ ƒë·ª£i th√™m m·ªôt ch√∫t
                    try:
                        await asyncio.sleep(2)
                        await frame.wait_for_selector('input[name="ngay_lap_tu_ngay"], #ngay_lap_tu_ngay', timeout=10000)
                        success = True
                        logger.info("Tra cuu giay nop tien page loaded after additional wait")
                    except:
                        logger.error("Still cannot find form after additional wait")
            else:
                logger.error("Iframe not found after calling connectSSO for giaynoptien")
            
            return success
            
        except Exception as e:
            logger.error(f"Error navigating to giaynoptien page: {e}")
            return False
    
    async def crawl_tokhai_info(
        self,
        session_id: str,
        tokhai_type: str,
        start_date: str,
        end_date: str,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Ch·ªâ l·∫•y th√¥ng tin t·ªù khai (KH√îNG download file)
        D√πng ƒë·ªÉ hi·ªÉn th·ªã danh s√°ch tr∆∞·ªõc, user ch·ªçn t·∫£i sau
        
        Yields:
            Dict v·ªõi c√°c key: type, data, progress, error
        """
        session = self.session_manager.get_session(session_id)
        if not session:
            yield {"type": "error", "error": "Session kh√¥ng t·ªìn t·∫°i ho·∫∑c ƒë√£ h·∫øt h·∫°n", "error_code": "SESSION_NOT_FOUND"}
            return
        
        if not session.is_logged_in:
            yield {"type": "error", "error": "Ch∆∞a ƒëƒÉng nh·∫≠p. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "NOT_LOGGED_IN"}
            return
        
        page = session.page
        
        try:
            yield {"type": "info", "message": "ƒêang x·ª≠ l√Ω t·ªù khai..."}
            
            # Navigate ƒë·∫øn trang tra c·ª©u t·ªù khai b·∫±ng JavaScript (nhanh h∆°n click menu)
            success = await self._navigate_to_tokhai_page(page, session.dse_session_id)
            
            if not success:
                yield {"type": "error", "error": "Kh√¥ng th·ªÉ navigate ƒë·∫øn trang tra c·ª©u. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            # Switch to mainframe
            frame = page.frame('mainframe')
            if not frame:
                yield {"type": "error", "error": "Kh√¥ng t√¨m th·∫•y mainframe", "error_code": "NAVIGATION_ERROR"}
                return
            
            yield {"type": "info", "message": "ƒêang ch·ªçn lo·∫°i t·ªù khai..."}
            
            # Ch·ªçn lo·∫°i t·ªù khai
            try:
                select_element = frame.locator('#maTKhai')
                await select_element.wait_for(timeout=10000)
                
                if tokhai_type in ["00", "T·∫•t c·∫£", "tat_ca", None, ""]:
                    await select_element.select_option(value="00")
                    is_all_types = True
                else:
                    try:
                        await select_element.select_option(value=tokhai_type)
                        is_all_types = False
                    except:
                        option = frame.locator(f'#maTKhai option:has-text("{tokhai_type}")')
                        if await option.count() > 0:
                            option_value = await option.first.get_attribute('value')
                            await select_element.select_option(value=option_value)
                            is_all_types = (option_value == "00")
                        else:
                            raise Exception(f"Option not found: {tokhai_type}")
            except Exception as e:
                yield {"type": "error", "error": f"Kh√¥ng t√¨m th·∫•y lo·∫°i t·ªù khai: {tokhai_type}", "error_code": "INVALID_TOKHAI_TYPE"}
                return
            
            await asyncio.sleep(0.5)
            
            # Chia kho·∫£ng th·ªùi gian
            date_ranges = self._get_date_ranges(start_date, end_date)
            
            total_count = 0
            results = []
            
            yield {"type": "info", "message": f"B·∫Øt ƒë·∫ßu crawl {len(date_ranges)} kho·∫£ng th·ªùi gian..."}
            
            for range_idx, date_range in enumerate(date_ranges):
                yield {
                    "type": "progress", 
                    "current": range_idx + 1, 
                    "total": len(date_ranges),
                    "message": f"ƒêang x·ª≠ l√Ω kho·∫£ng {date_range[0]} - {date_range[1]}..."
                }
                
                try:
                    # Nh·∫≠p ng√†y
                    start_input = frame.locator('#qryFromDate')
                    await start_input.fill('')
                    await start_input.fill(date_range[0])
                    
                    end_input = frame.locator('#qryToDate')
                    await end_input.click()
                    await end_input.fill('')
                    await end_input.fill(date_range[1])
                    
                    # Click Tra c·ª©u
                    search_btn = frame.locator('input[value="Tra c·ª©u"]')
                    await search_btn.click()
                    
                    await asyncio.sleep(2)
                    
                    # X·ª≠ l√Ω pagination
                    check_pages = True
                    while check_pages:
                        try:
                            table_body = frame.locator('#allResultTableBody, table.md_list2 tbody, table#data_content_onday tbody').first
                            await table_body.wait_for(timeout=5000)
                        except:
                            yield {"type": "info", "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}"}
                            break
                        
                        rows = table_body.locator('tr')
                        row_count = await rows.count()
                        
                        yield {"type": "progress", "current": total_count, "message": f"ƒêang parse {row_count} t·ªù khai (trang hi·ªán t·∫°i)..."}
                        
                        for i in range(row_count):
                            try:
                                row = rows.nth(i)
                                cols = row.locator('td')
                                col_count = await cols.count()
                                
                                if col_count < 3:
                                    continue
                                
                                # C·ªôt 1: M√£ giao d·ªãch (id_tk)
                                id_tk = await cols.nth(1).text_content()
                                id_tk = id_tk.strip() if id_tk else ""
                                
                                if len(id_tk) < 4:
                                    continue
                                
                                # Extract th√¥ng tin
                                name_tk = await cols.nth(2).text_content() if col_count > 2 else ""
                                ky_tinh_thue = await cols.nth(3).text_content() if col_count > 3 else ""
                                loai_tk = await cols.nth(4).text_content() if col_count > 4 else ""
                                lan_nop = await cols.nth(5).text_content() if col_count > 5 else ""
                                lan_bs = await cols.nth(6).text_content() if col_count > 6 else ""
                                ngay_nop = await cols.nth(7).text_content() if col_count > 7 else ""
                                noi_nop = await cols.nth(9).text_content() if col_count > 9 else ""
                                trang_thai = await cols.nth(10).text_content() if col_count > 10 else ""
                                
                                # Chu·∫©n h√≥a t√™n t·ªù khai
                                name_tk_normalized = self._normalize_tokhai_name(name_tk.strip() if name_tk else "")
                                
                                # X√°c ƒë·ªãnh tr·∫°ng th√°i
                                status = "unknown"
                                status_text = ""
                                trang_thai_lower = trang_thai.lower() if trang_thai else ""
                                if "kh√¥ng ch·∫•p nh·∫≠n" in trang_thai_lower:
                                    status = "rejected"
                                    status_text = "[Khong chap nhan]"
                                elif "ch·∫•p nh·∫≠n" in trang_thai_lower:
                                    status = "accepted"
                                    status_text = "[Chap nhan]"
                                
                                # T·∫°o t√™n file (ƒë·ªÉ user bi·∫øt t√™n file s·∫Ω ƒë∆∞·ª£c t·∫£i)
                                ngay_nop_clean = ngay_nop.strip().replace("/", "-").replace(":", "-") if ngay_nop else ""
                                file_name = f"{name_tk_normalized} -{ky_tinh_thue.strip()} -L{lan_nop.strip()} -{loai_tk.strip()} -({id_tk}) -[{ngay_nop_clean}] {status_text}"
                                file_name = self._remove_accents(file_name)
                                file_name = file_name.replace("/", "_").replace(":", "_").replace("\\", "_")
                                
                                # Check xem c√≥ link download kh√¥ng
                                has_link = False
                                try:
                                    col2 = cols.nth(2)
                                    download_link = col2.locator('a')
                                    link_count = await download_link.count()
                                    
                                    if link_count > 0:
                                        first_link = download_link.first
                                        onclick = await first_link.get_attribute('onclick')
                                        title = await first_link.get_attribute('title')
                                        
                                        if onclick and 'downloadTkhai' in onclick:
                                            has_link = True
                                        elif title and 'T·∫£i t·ªáp' in title:
                                            has_link = True
                                except:
                                    has_link = False
                                
                                result = {
                                    "id": id_tk,
                                    "name": name_tk_normalized,
                                    "ky_tinh_thue": ky_tinh_thue.strip() if ky_tinh_thue else "",
                                    "loai": loai_tk.strip() if loai_tk else "",
                                    "lan_nop": lan_nop.strip() if lan_nop else "",
                                    "lan_bo_sung": lan_bs.strip() if lan_bs else "",
                                    "ngay_nop": ngay_nop.strip() if ngay_nop else "",
                                    "noi_nop": noi_nop.strip() if noi_nop else "",
                                    "trang_thai": status,
                                    "trang_thai_text": status_text,
                                    "file_name": file_name + ".xml",
                                    "has_download_link": has_link  # C√≥ link download s·∫µn hay kh√¥ng
                                }
                                
                                results.append(result)
                                total_count += 1
                                
                                yield {"type": "item", "data": result}
                                
                            except Exception as e:
                                logger.error(f"Error processing row: {e}")
                                continue
                        
                        # Check pagination
                        try:
                            next_btn = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                            if await next_btn.count() > 0:
                                await next_btn.click()
                                await asyncio.sleep(1)
                            else:
                                check_pages = False
                        except:
                            check_pages = False
                
                except Exception as e:
                    logger.error(f"Error processing date range {date_range}: {e}")
                    yield {"type": "warning", "message": f"L·ªói x·ª≠ l√Ω kho·∫£ng {date_range}: {str(e)}"}
                    continue
            
            yield {
                "type": "complete",
                "total": total_count,
                "results": results
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong crawl_tokhai_info: {e}")
            yield {"type": "error", "error": f"L·ªói khi tra c·ª©u th√¥ng tin t·ªù khai: {str(e)}", "error_code": "CRAWL_ERROR"}
    
    async def crawl_tokhai(
        self,
        session_id: str,
        tokhai_type: str,
        start_date: str,
        end_date: str,
        job_id: Optional[str] = None,  # ‚úÖ Th√™m job_id ƒë·ªÉ check cancelled
    ) -> AsyncGenerator[Dict[str, Any], None]:
        session = self.session_manager.get_session(session_id)
        if not session:
            yield {"type": "error", "error": "Session kh√¥ng t·ªìn t·∫°i ho·∫∑c ƒë√£ h·∫øt h·∫°n", "error_code": "SESSION_NOT_FOUND"}
            return
        
        if not session.is_logged_in:
            yield {"type": "error", "error": "Ch∆∞a ƒëƒÉng nh·∫≠p. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "NOT_LOGGED_IN"}
            return
        
        page = session.page
        
        # ‚úÖ FIX: T·∫°o temp directory trong source code thay v√¨ system temp
        # L·∫•y ƒë∆∞·ªùng d·∫´n project (tool-go-soft)
        current_dir = os.path.dirname(os.path.abspath(__file__))  # .../services/
        services_dir = os.path.dirname(current_dir)  # .../tool-go-soft/
        temp_base_dir = os.path.join(services_dir, "temp")  # .../tool-go-soft/temp/
        os.makedirs(temp_base_dir, exist_ok=True)
        
        # T·∫°o temp directory v·ªõi timestamp ƒë·ªÉ tr√°nh conflict
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        temp_dir = os.path.join(temp_base_dir, f"tokhai_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)
        
        logger.info(f"üìÅ Temp directory for debug files: {temp_dir}")  # ‚úÖ Log temp_dir path ƒë·ªÉ d·ªÖ t√¨m file debug
        ssid = session.dse_session_id
        
        try:
            yield {"type": "info", "message": "ƒêang x·ª≠ l√Ω t·ªù khai..."}
            
            success = await self._navigate_to_tokhai_page(page, ssid)
            
            if not success:
                yield {"type": "error", "error": "Kh√¥ng th·ªÉ navigate ƒë·∫øn trang tra c·ª©u. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            frame = None
            try:
                frames = page.frames
                for f in frames:
                    if 'thuedientu.gdt.gov.vn' in f.url:
                        frame = f
                        break
            except Exception as e:
                logger.warning(f"L·ªói khi t√¨m frame: {e}")
            
            if not frame:
                yield {"type": "error", "error": "Kh√¥ng t√¨m th·∫•y iframe sau khi navigate. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            try:
                await frame.wait_for_load_state('domcontentloaded', timeout=15000)
                await asyncio.sleep(1)
                await frame.wait_for_selector('#maTKhai', timeout=15000)
            except Exception as e:
                yield {"type": "error", "error": "Kh√¥ng t√¨m th·∫•y form tra c·ª©u. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            if await self._check_session_timeout(page):
                yield {
                    "type": "error",
                    "error": "Phi√™n giao d·ªãch h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.",
                    "error_code": "SESSION_EXPIRED"
                }
                return
            
            yield {"type": "info", "message": "ƒêang ch·ªçn lo·∫°i t·ªù khai..."}
            
            try:
                select_element = frame.locator('#maTKhai')
                await select_element.wait_for(timeout=10000)
                
                if tokhai_type in ["00", "T·∫•t c·∫£", "tat_ca", None, ""]:
                    await select_element.select_option(value="00")
                    logger.info("Selected tokhai: T·∫•t c·∫£")
                    is_all_types = True
                else:
                    try:
                        await select_element.select_option(value=tokhai_type)
                        logger.info(f"Selected tokhai by value: {tokhai_type}")
                        is_all_types = False
                    except:
                        option = frame.locator(f'#maTKhai option:has-text("{tokhai_type}")')
                        if await option.count() > 0:
                            option_value = await option.first.get_attribute('value')
                            await select_element.select_option(value=option_value)
                            is_all_types = (option_value == "00")
                        else:
                            raise Exception(f"Option not found: {tokhai_type}")
                        
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi ch·ªçn lo·∫°i t·ªù khai: {e}")
                yield {"type": "error", "error": f"Kh√¥ng t√¨m th·∫•y lo·∫°i t·ªù khai: {tokhai_type}. H√£y d√πng value nh∆∞ '842', '00' (T·∫•t c·∫£), ho·∫∑c text nh∆∞ '01/GTGT'", "error_code": "INVALID_TOKHAI_TYPE"}
                return
            
            await asyncio.sleep(0.5)
            
            date_ranges = self._get_date_ranges(start_date, end_date)
            
            total_days = 0
            range_days = []
            for date_range in date_ranges:
                days = self._calculate_days_between(date_range[0], date_range[1])
                range_days.append(days)
                total_days += days
            
            range_percentages = []
            for days in range_days:
                if total_days > 0:
                    percent = (days / total_days) * 100
                else:
                    percent = 100.0 if len(date_ranges) == 1 else 0.0
                range_percentages.append(percent)
            
            total_count = 0
            results = []
            accumulated_total_so_far = 0
            accumulated_percent_so_far = 0.0 
            all_special_items = []
            thuyet_minh_total = 0
            thuyet_minh_downloaded = 0
            
            yield {"type": "info", "message": f"B·∫Øt ƒë·∫ßu crawl {len(date_ranges)} kho·∫£ng th·ªùi gian..."}
            
            http_client = await self._get_http_client(session_id)
            
            for range_idx, date_range in enumerate(date_ranges):
                if job_id and await self._check_cancelled(job_id):
                    logger.info(f"Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                    yield {"type": "error", "error": "Job ƒë√£ b·ªã h·ªßy", "error_code": "JOB_CANCELLED"}
                    return
                
                yield {
                    "type": "progress", 
                    "current": range_idx + 1, 
                    "total": len(date_ranges),
                    "message": f"ƒêang x·ª≠ l√Ω kho·∫£ng {date_range[0]} - {date_range[1]}...",
                    "percent": int(round(accumulated_percent_so_far)),
                    "accumulated_percent": int(round(accumulated_percent_so_far)),
                    "accumulated_total": accumulated_total_so_far,
                    "accumulated_downloaded": total_count,
                    "thuyet_minh_downloaded": thuyet_minh_downloaded,
                    "thuyet_minh_total": thuyet_minh_total
                }
                
                try:
                    # Nh·∫≠p ng√†y b·∫Øt ƒë·∫ßu (id="qryFromDate")
                    start_input = frame.locator('#qryFromDate')
                    await start_input.fill('')
                    await start_input.fill(date_range[0])
                    
                    # Nh·∫≠p ng√†y k·∫øt th√∫c (id="qryToDate")
                    end_input = frame.locator('#qryToDate')
                    await end_input.click()
                    await end_input.fill('')
                    await end_input.fill(date_range[1])
                    
                    # Click button Tra c·ª©u
                    search_btn = frame.locator('input[value="Tra c·ª©u"]')
                    await search_btn.click()
                    
                    await asyncio.sleep(1)
                    
                    try:
                        frames = page.frames
                        for f in frames:
                            if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                frame = f
                                break
                    except Exception as refind_frame_e:
                        pass
                    
                    try:
                        await frame.wait_for_load_state('networkidle', timeout=5000)
                    except Exception as frame_load_e:
                        pass
                    
                    try:
                        table_body = frame.locator('#allResultTableBody, table.md_list2 tbody, table#data_content_onday tbody').first
                        await table_body.wait_for(timeout=10000, state='visible')
                        await asyncio.sleep(1.5)
                    except Exception as e:
                        pass
                        accumulated_percent_so_far = min(100.0, accumulated_percent_so_far)
                        yield {
                            "type": "info", 
                            "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}",
                            "percent": int(round(accumulated_percent_so_far)),
                            "accumulated_percent": int(round(accumulated_percent_so_far)),
                            "accumulated_total": accumulated_total_so_far,
                            "accumulated_downloaded": total_count,
                            "thuyet_minh_downloaded": thuyet_minh_downloaded,
                            "thuyet_minh_total": thuyet_minh_total
                        }
                        continue
                    
                    await asyncio.sleep(1)
                    
                    pagination_info = await self._extract_pagination_info(frame)
                    if not pagination_info:
                        rows = table_body.locator('tr')
                        row_count = await rows.count()
                        if row_count == 0:
                            yield {"type": "info", "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}"}
                            continue
                        else:
                            pagination_info = {
                                "current_page": 1,
                                "total_pages": 1,
                                "total_records": row_count
                            }
                    
                    total_pages = pagination_info["total_pages"]
                    total_records_estimated = pagination_info["total_records"]
                    
                    range_percent = range_percentages[range_idx]
                    
                    yield {
                        "type": "info",
                        "message": f"T√¨m th·∫•y {total_records_estimated} b·∫£n ghi trong {total_pages} trang. B·∫Øt ƒë·∫ßu t·∫£i..."
                    }
                    
                    yield {
                        "type": "download_start",
                        "total_to_download": total_records_estimated,
                        "date_range": f"{date_range[0]} - {date_range[1]}",
                        "range_index": range_idx + 1,
                        "total_ranges": len(date_ranges),
                        "accumulated_total": accumulated_total_so_far + total_records_estimated,
                        "accumulated_downloaded": total_count,
                        "range_percent": range_percent,
                        "accumulated_percent": accumulated_percent_so_far
                    }
                    
                    downloaded_count = 0
                    actual_downloaded = 0
                    actual_thuyet_minh_downloaded = 0
                    special_items = []
                    range_thuyet_minh_total = 0
                    
                    tokhai_count = total_records_estimated
                    if tokhai_count > 0:
                        percent_per_tokhai = range_percent / tokhai_count
                    else:
                        percent_per_tokhai = 0.0
                    
                    previous_first_row_id = None
                    
                    for page_num in range(1, total_pages + 1):
                        if job_id and await self._check_cancelled(job_id):
                            logger.info(f"Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                            yield {"type": "error", "error": "Job ƒë√£ b·ªã h·ªßy", "error_code": "JOB_CANCELLED"}
                            return
                        
                        if page_num > 1:
                            try:
                                next_btn = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                                next_btn_count = await next_btn.count()
                                if next_btn_count > 0:
                                    await asyncio.wait_for(next_btn.click(), timeout=10.0)
                                else:
                                    break
                            except asyncio.TimeoutError:
                                break
                            except Exception as click_e:
                                break
                                break
                            
                            logger.info(f"‚è≥ [TOKHAI] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num - 1}: ƒê·ª£i 2 gi√¢y sau khi click...")
                            await asyncio.sleep(2)
                            logger.info(f"‚úÖ [TOKHAI] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num - 1}: ƒê√£ ƒë·ª£i xong 2 gi√¢y, b·∫Øt ƒë·∫ßu ƒë·ª£i table load...")
                            
                            try:
                                frames = page.frames
                                for f in frames:
                                    if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                        frame = f
                                        logger.info(f"üîÑ [TOKHAI] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ t√¨m l·∫°i frame m·ªõi sau khi click next: {frame.url[:100]}...")
                                        break
                            except Exception as refind_frame_e:
                                logger.warning(f"‚ö†Ô∏è [TOKHAI] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ t√¨m l·∫°i frame m·ªõi sau khi click next: {refind_frame_e}")
                        
                        try:
                            table_body = frame.locator('#allResultTableBody, table.md_list2 tbody, table#data_content_onday tbody').first
                            await asyncio.wait_for(
                                table_body.wait_for(timeout=15000, state='visible'),
                                timeout=20.0
                            )
                            
                            try:
                                await frame.wait_for_load_state('networkidle', timeout=5000)
                            except Exception as frame_load_e:
                                pass
                            
                            await asyncio.sleep(1.5)
                            
                            if page_num > 1:
                                try:
                                    rows_check = table_body.locator('tr')
                                    row_count_check = await rows_check.count()
                                    
                                    first_row_id = None
                                    if row_count_check > 0:
                                        try:
                                            first_row = rows_check.first
                                            first_cols = first_row.locator('td')
                                            col_count = await first_cols.count()
                                            if col_count > 1:
                                                first_row_id = await first_cols.nth(1).text_content()
                                                first_row_id = first_row_id.strip() if first_row_id else None
                                        except Exception as get_id_e:
                                            pass
                                    
                                    if previous_first_row_id and first_row_id:
                                        if previous_first_row_id == first_row_id:
                                            await asyncio.sleep(2)
                                            first_row_id_after_wait = None
                                            if row_count_check > 0:
                                                try:
                                                    first_row_after = rows_check.first
                                                    first_cols_after = first_row_after.locator('td')
                                                    col_count_after = await first_cols_after.count()
                                                    if col_count_after > 1:
                                                        first_row_id_after_wait = await first_cols_after.nth(1).text_content()
                                                        first_row_id_after_wait = first_row_id_after_wait.strip() if first_row_id_after_wait else None
                                                except Exception as get_id_e2:
                                                    logger.debug(f"‚ö†Ô∏è [TOKHAI] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ l·∫•y m√£ giao d·ªãch sau khi ƒë·ª£i: {get_id_e2}")
                                            
                                            if first_row_id_after_wait and previous_first_row_id == first_row_id_after_wait:
                                                logger.error(f"‚ùå [TOKHAI] [{range_idx + 1}/{len(date_ranges)}] Table v·∫´n ch∆∞a chuy·ªÉn trang sau khi ƒë·ª£i th√™m!")
                                                break
                                            elif first_row_id_after_wait:
                                                first_row_id = first_row_id_after_wait
                                        else:
                                            pass
                                except Exception as verify_e:
                                    pass
                        except asyncio.TimeoutError:
                            break
                        except Exception as wait_table_e:
                            break
                        
                        page_params = await self._extract_download_params(frame)
                        if not page_params:
                            continue
                        
                        rows = table_body.locator('tr')
                        row_count = await rows.count()
                        
                        first_row_id_current = None
                        if row_count > 0:
                            try:
                                first_row = rows.first
                                first_cols = first_row.locator('td')
                                col_count_first = await first_cols.count()
                                if col_count_first > 1:
                                    first_row_id_current = await first_cols.nth(1).text_content()
                                    first_row_id_current = first_row_id_current.strip() if first_row_id_current else None
                            except Exception as get_first_id_e:
                                pass
                        
                        page_items_to_download = []
                        
                        for i in range(row_count):
                            try:
                                row = rows.nth(i)
                                cols = row.locator('td')
                                col_count = await cols.count()
                                
                                if col_count < 3:
                                    continue
                                
                                # C·ªôt 1: M√£ giao d·ªãch (id_tk)
                                id_tk = await cols.nth(1).text_content()
                                id_tk = id_tk.strip() if id_tk else ""
                                
                                # C·ªôt 2: T·ªù khai/Ph·ª• l·ª•c
                                name_tk = await cols.nth(2).text_content() if col_count > 2 else ""
                                
                                # Check xem c√≥ link download kh√¥ng
                                download_type = None
                                has_link = False
                                extracted_id = None
                                
                                try:
                                    col2 = cols.nth(2)
                                    download_link = col2.locator('a')
                                    link_count = await download_link.count()
                                    
                                    if link_count > 0:
                                        first_link = download_link.first
                                        onclick = await first_link.get_attribute('onclick')
                                        title = await first_link.get_attribute('title')
                                        
                                        if onclick and 'downloadBke' in onclick:
                                            download_type = "downloadBke"
                                            has_link = True
                                            range_thuyet_minh_total += 1
                                            match = re.search(r"downloadBke\(['\"]?(\d+)['\"]?\)", onclick)
                                            if match:
                                                extracted_id = match.group(1)
                                                if not id_tk or len(id_tk) < 4:
                                                    id_tk = extracted_id
                                        elif onclick and 'downloadTkhai' in onclick:
                                            download_type = "downloadTkhai"
                                            has_link = True
                                            match = re.search(r"downloadTkhai\(['\"]?(\d+)['\"]?\)", onclick)
                                            if match:
                                                extracted_id = match.group(1)
                                        elif title and 'T·∫£i t·ªáp' in title:
                                            has_link = True
                                            download_type = "downloadTkhai"
                                except:
                                    has_link = False
                                
                                if not id_tk or len(id_tk) < 4:
                                    if extracted_id:
                                        id_tk = extracted_id
                                    else:
                                        continue
                                
                                name_tk_normalized = self._normalize_tokhai_name(name_tk.strip() if name_tk else "")
                                
                                if has_link:
                                    ky_tinh_thue = await cols.nth(3).text_content() if col_count > 3 else ""
                                    loai_tk = await cols.nth(4).text_content() if col_count > 4 else ""
                                    lan_nop = await cols.nth(5).text_content() if col_count > 5 else ""
                                    lan_bs = await cols.nth(6).text_content() if col_count > 6 else ""
                                    ngay_nop = await cols.nth(7).text_content() if col_count > 7 else ""
                                    noi_nop = await cols.nth(9).text_content() if col_count > 9 else ""
                                    trang_thai = await cols.nth(10).text_content() if col_count > 10 else ""
                                    
                                    status = "unknown"
                                    status_text = ""
                                    trang_thai_lower = trang_thai.lower() if trang_thai else ""
                                    
                                    if "ti·∫øp nh·∫≠n" in trang_thai_lower or "tiep nhan" in trang_thai_lower:
                                        status = "received"
                                        status_text = "Ti·∫øp nh·∫≠n"
                                    elif "x√°c nh·∫≠n" in trang_thai_lower or "xac nhan" in trang_thai_lower:
                                        status = "confirmed"
                                        status_text = "X√°c nh·∫≠n"
                                    elif "kh√¥ng ch·∫•p nh·∫≠n" in trang_thai_lower or "khong chap nhan" in trang_thai_lower:
                                        status = "rejected"
                                        status_text = "Kh√¥ng ch·∫•p nh·∫≠n"
                                    elif "ch·∫•p nh·∫≠n" in trang_thai_lower or "chap nhan" in trang_thai_lower:
                                        status = "accepted"
                                        status_text = "Ch·∫•p nh·∫≠n"
                                    else:
                                        status_text = trang_thai.strip()[:20] if trang_thai else "Unknown"
                                    
                                    status_text_clean = self._remove_accents(status_text)
                                    
                                    ngay_clean = ""
                                    if ngay_nop:
                                        ngay_parts = ngay_nop.strip().split(" ")
                                        if ngay_parts:
                                            ngay_only = ngay_parts[0]
                                            date_parts = ngay_only.split("/")
                                            if len(date_parts) == 3:
                                                ngay_clean = f"{date_parts[2]}-{date_parts[1]}-{date_parts[0]}"
                                            else:
                                                ngay_clean = ngay_only.replace("/", "-")
                                    
                                    if ngay_clean:
                                        file_name = f"{id_tk} - {status_text_clean} - {ngay_clean}"
                                    else:
                                        file_name = f"{id_tk} - {status_text_clean}"
                                    
                                    max_filename_length = 150
                                    if len(file_name) > max_filename_length:
                                        status_short = {
                                            "Tiep nhan": "TN",
                                            "Chap nhan": "CN",
                                            "Khong chap nhan": "KCN",
                                            "Xac nhan": "XN"
                                        }.get(status_text_clean, status_text_clean[:3])
                                        
                                        if ngay_clean:
                                            file_name = f"{id_tk} - {status_short} - {ngay_clean}"
                                        else:
                                            file_name = f"{id_tk} - {status_short}"
                                        
                                        if len(file_name) > max_filename_length:
                                            file_name = file_name[:max_filename_length]
                                    
                                    # L∆∞u item ƒë·ªÉ download batch
                                    item = {
                                    "id": id_tk,
                                    "name": name_tk_normalized,
                                    "ky_tinh_thue": ky_tinh_thue.strip() if ky_tinh_thue else "",
                                    "loai": loai_tk.strip() if loai_tk else "",
                                    "lan_nop": lan_nop.strip() if lan_nop else "",
                                    "lan_bo_sung": lan_bs.strip() if lan_bs else "",
                                    "ngay_nop": ngay_nop.strip() if ngay_nop else "",
                                    "noi_nop": noi_nop.strip() if noi_nop else "",
                                    "trang_thai": status,
                                    "file_name": file_name,
                                        "has_link": True,
                                        "download_type": download_type,
                                        "page_number": page_num
                                    }
                                    page_items_to_download.append(item)
                                else:
                                    special_item = {
                                        "id": id_tk,
                                        "name": name_tk_normalized,
                                        "page_number": page_num,
                                        "has_link": False,
                                        "date_range": f"{date_range[0]} - {date_range[1]}"
                                    }
                                    special_items.append(special_item)
                                    logger.info(f"üìù Collected special item (no link): {id_tk} - {name_tk_normalized} (page {page_num})")
                                
                            except Exception as e:
                                logger.error(f"Error processing row: {e}")
                                continue
                        
                        if page_items_to_download:
                            page_params_map = {page_num: page_params}
                            
                            tokhai_count = total_records_estimated - range_thuyet_minh_total
                            
                            if tokhai_count > 0:
                                percent_per_tokhai = range_percent / tokhai_count
                            else:
                                percent_per_tokhai = 0.0
                            
                            current_percent_for_page = accumulated_percent_so_far + (actual_downloaded * percent_per_tokhai)
                            current_percent_for_page = min(100.0, current_percent_for_page)
                            accumulated_total_current = accumulated_total_so_far + total_records_estimated
                            yield {
                                "type": "progress",
                                "current": actual_downloaded,
                                "message": f"ƒêang x·ª≠ l√Ω trang {page_num}/{total_pages} ({len(page_items_to_download)} t·ªù khai)...",
                                "percent": int(round(current_percent_for_page)),
                                "accumulated_percent": int(round(current_percent_for_page)),
                                "accumulated_total": accumulated_total_current,
                                "accumulated_downloaded": total_count + actual_downloaded,
                                "thuyet_minh_downloaded": thuyet_minh_downloaded + actual_thuyet_minh_downloaded,
                                "thuyet_minh_total": thuyet_minh_total + range_thuyet_minh_total
                            }
                            
                            successful_downloads = []
                            base_params = page_params
                            
                            for idx, item in enumerate(page_items_to_download):
                                file_num = idx + 1
                                
                                try:
                                    fresh_params = await self._extract_download_params(frame)
                                    if fresh_params:
                                        base_params = fresh_params
                                except:
                                    pass
                                
                                try:
                                    result = await self._download_one_via_url(
                                        session_id,
                                        item["id"],
                                        item,
                                        base_params,
                                        temp_dir,
                                        frame=frame
                                    )
                                    
                                    if result and not isinstance(result, Exception):
                                        successful_downloads.append(result)
                                        
                                        if result.get("download_type") == "downloadBke":
                                            actual_thuyet_minh_downloaded += 1
                                        else:
                                            actual_downloaded += 1
                                        downloaded_count += 1
                                        
                                        current_percent = accumulated_percent_so_far + (actual_downloaded * percent_per_tokhai)
                                        current_percent = min(100.0, current_percent)
                                        
                                        accumulated_total_current = accumulated_total_so_far + total_records_estimated
                                        
                                        yield {
                                            "type": "download_progress",
                                            "downloaded": actual_downloaded,
                                            "total": total_records_estimated,
                                            "percent": int(round(current_percent)),
                                            "date_range": f"{date_range[0]} - {date_range[1]}",
                                            "range_index": range_idx + 1,
                                            "total_ranges": len(date_ranges),
                                            "accumulated_downloaded": total_count + actual_downloaded,
                                            "accumulated_total": accumulated_total_current,
                                            "accumulated_percent": int(round(current_percent)),
                                            "thuyet_minh_downloaded": thuyet_minh_downloaded + actual_thuyet_minh_downloaded,
                                            "thuyet_minh_total": thuyet_minh_total + range_thuyet_minh_total
                                        }
                                        
                                        result_data = {
                                            "id": result["id"],
                                            "name": result["name"],
                                            "ky_tinh_thue": result["ky_tinh_thue"],
                                            "loai": result["loai"],
                                            "lan_nop": result["lan_nop"],
                                            "lan_bo_sung": result["lan_bo_sung"],
                                            "ngay_nop": result["ngay_nop"],
                                            "noi_nop": result["noi_nop"],
                                            "trang_thai": result["trang_thai"],
                                            "file_name": result["file_name"] + ".xml"
                                        }
                                        results.append(result_data)
                                        yield {"type": "item", "data": result_data}
                                    
                                    await asyncio.sleep(0.1)
                                except Exception as e:
                                    logger.error(f"‚ùå Error downloading {item.get('id', 'unknown')}: {e}")
                        
                        previous_first_row_id = first_row_id_current
                    
                    accumulated_total_so_far += total_records_estimated
                    total_count += actual_downloaded
                    
                    accumulated_percent_so_far += range_percent
                    accumulated_percent_so_far = min(100.0, accumulated_percent_so_far)
                    
                    thuyet_minh_total += range_thuyet_minh_total
                    thuyet_minh_downloaded += actual_thuyet_minh_downloaded
                    
                    if special_items:
                        all_special_items.extend(special_items)
                        yield {
                            "type": "special_items",
                            "count": len(special_items),
                            "items": special_items,
                            "date_range": f"{date_range[0]} - {date_range[1]}",
                            "message": f"C√≥ {len(special_items)} t·ªù khai ƒë·∫∑c bi·ªát trong kho·∫£ng {date_range[0]} - {date_range[1]} (ch∆∞a c√≥ c√°ch t·∫£i, ƒë√£ l∆∞u metadata ƒë·ªÉ sau n√†y)",
                            "percent": int(round(accumulated_percent_so_far)),
                            "accumulated_percent": int(round(accumulated_percent_so_far)),
                            "accumulated_total": accumulated_total_so_far,
                            "accumulated_downloaded": total_count,
                            "thuyet_minh_downloaded": thuyet_minh_downloaded,
                            "thuyet_minh_total": thuyet_minh_total
                        }
                        logger.info(f"üìã Found {len(special_items)} special items (no download link) in date range {date_range[0]} - {date_range[1]}")
                    
                    accumulated_percent_so_far = min(100.0, accumulated_percent_so_far)
                    yield {
                        "type": "info",
                        "message": f"ƒê√£ t·∫£i {actual_downloaded} file t·ª´ {total_pages} trang (∆∞·ªõc t√≠nh {total_records_estimated} b·∫£n ghi). C√≥ {len(special_items)} t·ªù khai ƒë·∫∑c bi·ªát ch∆∞a t·∫£i.",
                        "percent": int(round(accumulated_percent_so_far)),
                        "accumulated_percent": int(round(accumulated_percent_so_far)),
                        "accumulated_total": accumulated_total_so_far,
                        "accumulated_downloaded": total_count,
                        "thuyet_minh_downloaded": thuyet_minh_downloaded,
                        "thuyet_minh_total": thuyet_minh_total
                    }
                
                except Exception as e:
                    logger.error(f"Error processing date range {date_range}: {e}")
                    yield {"type": "warning", "message": f"L·ªói x·ª≠ l√Ω kho·∫£ng {date_range}: {str(e)}"}
                    continue
            
            # T·∫°o ZIP file t·ª´ c√°c file ƒë√£ download
            zip_base64 = None
            download_id = None
            files_info = []
            total_size = 0
            
            if os.listdir(temp_dir):
                # T·∫°o t√™n file ZIP
                if is_all_types:
                    zip_filename = f"tokhai_TAT_CA_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                    tokhai_type_label = "T·∫•t c·∫£"
                else:
                    zip_filename = f"tokhai_{tokhai_type}_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                    tokhai_type_label = tokhai_type
                
                # T·∫°o download_id (UUID) ƒë·ªÉ worker c√≥ th·ªÉ download sau
                download_id = str(uuid.uuid4())
                zip_file_path = os.path.join(self.ZIP_STORAGE_DIR, f"{download_id}.zip")
                
                # L∆∞u zip v√†o disk thay v√¨ ch·ªâ t·∫°o base64
                with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                    for file_name in os.listdir(temp_dir):
                        file_path = os.path.join(temp_dir, file_name)
                        if os.path.isfile(file_path):
                            file_size = os.path.getsize(file_path)
                            total_size += file_size
                            zf.write(file_path, file_name)
                            files_info.append({
                                "name": file_name,
                                "size": file_size
                            })
                
                # ƒê·ªçc file ƒë·ªÉ t·∫°o base64 (v·∫´n c·∫ßn cho Redis)
                with open(zip_file_path, 'rb') as f:
                    zip_base64 = base64.b64encode(f.read()).decode('utf-8')
                
                logger.info(f"‚úÖ ƒê√£ t·∫°o file ZIP: {zip_filename} (download_id: {download_id})")
                
                # L∆∞u download_id v√†o Redis
                try:
                    from shared.redis_client import get_redis_client
                    redis_client = get_redis_client()
                    redis_key = f"session:{session_id}:download_id"
                    redis_client.setex(redis_key, 3600, download_id.encode('utf-8'))
                except Exception as redis_err:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u download_id v√†o Redis: {redis_err}")
            else:
                # Kh√¥ng c√≥ files
                pass
            
            # T·∫°o zip_filename n·∫øu ch∆∞a c√≥ (cho tr∆∞·ªùng h·ª£p kh√¥ng c√≥ files)
            if not download_id:
                if is_all_types:
                    zip_filename = f"tokhai_TAT_CA_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                    tokhai_type_label = "T·∫•t c·∫£"
            else:
                zip_filename = f"tokhai_{tokhai_type}_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                tokhai_type_label = tokhai_type
            
            # ƒê·∫øm l·∫°i s·ªë file th·ª±c t·∫ø ƒë√£ download (t·ªù khai + t·ªù thuy·∫øt minh)
            actual_files_count = len(files_info)
            # ƒê·∫øm s·ªë results th·ª±c t·∫ø
            actual_results_count = len(results)
            
            # ‚úÖ T√≠nh s·ªë t·ªù khai ƒë√£ t·∫£i (kh√¥ng bao g·ªìm t·ªù thuy·∫øt minh)
            tokhai_downloaded = total_count  # S·ªë t·ªù khai ƒë√£ t·∫£i (kh√¥ng t√≠nh t·ªù thuy·∫øt minh)
            # ‚úÖ T·ªïng s·ªë file ƒë√£ t·∫£i = t·ªù khai + t·ªù thuy·∫øt minh
            total_files_downloaded = tokhai_downloaded + thuyet_minh_downloaded
            
            # ‚úÖ LOG ƒë·ªÉ debug
            logger.info(f"üìä Complete event - tokhai_downloaded: {tokhai_downloaded}, thuyet_minh_downloaded: {thuyet_minh_downloaded}, total_files_downloaded: {total_files_downloaded}, accumulated_total_so_far: {accumulated_total_so_far}")
            
            # ‚úÖ Message hi·ªÉn th·ªã khi ho√†n th√†nh
            completion_message = f"Ho√†n th√†nh! ƒê√£ t·∫£i {tokhai_downloaded}/{accumulated_total_so_far} t·ªù khai"
            if thuyet_minh_total > 0:
                completion_message += f" - {thuyet_minh_downloaded}/{thuyet_minh_total} t·ªù thuy·∫øt minh"
            if len(all_special_items) > 0:
                completion_message += f". C√≥ {len(all_special_items)} t·ªù khai ƒë·∫∑c bi·ªát kh√¥ng t·∫£i ƒë∆∞·ª£c"
            
            # Total = s·ªë file th·ª±c t·∫ø ƒë√£ download (t·ªù khai + t·ªù thuy·∫øt minh) - ƒë√¢y l√† s·ªë hi·ªÉn th·ªã tr√™n button
            # N·∫øu mu·ªën bi·∫øt s·ªë items ƒë√£ t√¨m th·∫•y, d√πng actual_results_count
            # Complete event KH√îNG g·ª≠i zip_base64 v√† results (qu√° l·ªõn), ch·ªâ g·ª≠i metadata
            yield {
                "type": "complete",
                "total": total_files_downloaded,  # ‚úÖ S·ªë file ƒë√£ t·∫£i (t·ªù khai + t·ªù thuy·∫øt minh) - ƒë·ªÉ hi·ªÉn th·ªã tr√™n button
                "tokhai_downloaded": tokhai_downloaded,  # S·ªë t·ªù khai ƒë√£ t·∫£i
                "tokhai_total": accumulated_total_so_far,  # T·ªïng s·ªë t·ªù khai t√¨m th·∫•y
                "thuyet_minh_downloaded": thuyet_minh_downloaded,  # S·ªë t·ªù thuy·∫øt minh ƒë√£ t·∫£i
                "thuyet_minh_total": thuyet_minh_total,  # T·ªïng s·ªë t·ªù thuy·∫øt minh
                "special_items_count": len(all_special_items),  # S·ªë t·ªù khai ƒë·∫∑c bi·ªát kh√¥ng t·∫£i ƒë∆∞·ª£c
                "results_count": actual_results_count,  # S·ªë items ƒë√£ t√¨m th·∫•y (c√≥ th·ªÉ > files n·∫øu download th·∫•t b·∫°i)
                "total_rows_processed": total_count,  # S·ªë rows ƒë√£ x·ª≠ l√Ω (ƒë·ªÉ debug)
                "files_count": actual_files_count,  # S·ªë file trong ZIP (ƒë·ªÉ ki·ªÉm tra)
                "total_size": total_size,
                "download_id": download_id,
                "zip_filename": zip_filename,
                "tokhai_type": tokhai_type_label,
                "is_all_types": is_all_types,
                "has_zip": download_id is not None,
                "message": completion_message,
                "special_items": all_special_items if len(all_special_items) > 0 else None,
            }
            
            if download_id and zip_base64:
                chunk_size = 5 * 1024 * 1024
                if len(zip_base64) > chunk_size:
                    logger.info(f"Zip base64 is large ({len(zip_base64)/1024/1024:.2f} MB), sending in chunks")
                    for i in range(0, len(zip_base64), chunk_size):
                        chunk = zip_base64[i:i+chunk_size]
                        yield {
                            "type": "zip_chunk",
                            "download_id": download_id,
                            "chunk_index": i // chunk_size,
                            "chunk_data": chunk,
                            "is_last": (i + chunk_size) >= len(zip_base64)
                        }
                else:
                    yield {
                        "type": "zip_data",
                        "download_id": download_id,
                        "zip_base64": zip_base64,
                        "zip_filename": zip_filename
            }
            
        except Exception as e:
            logger.error(f"Error in crawl_tokhai: {e}")
            error_msg = str(e)
            if "timeout" in error_msg.lower() or "phi√™n giao d·ªãch" in error_msg.lower():
                yield {"type": "error", "error": "Phi√™n giao d·ªãch h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "SESSION_EXPIRED"}
            else:
                yield {"type": "error", "error": f"L·ªói khi tra c·ª©u t·ªù khai: {error_msg}", "error_code": "CRAWL_ERROR"}
        
        finally:
            debug_files = []
            try:
                if os.path.exists(temp_dir):
                    for file in os.listdir(temp_dir):
                        if file.startswith('DEBUG_'):
                            debug_files.append(os.path.join(temp_dir, file))
                
                if debug_files:
                    logger.warning(f"‚ö†Ô∏è Found {len(debug_files)} debug files in {temp_dir}:")
                    for debug_file in debug_files:
                        file_size = os.path.getsize(debug_file) if os.path.exists(debug_file) else 0
                        logger.warning(f"  - {os.path.basename(debug_file)} ({file_size} bytes)")
                    logger.warning(f"‚ö†Ô∏è Debug files will be kept for inspection. Temp dir: {temp_dir}")
                else:
                    shutil.rmtree(temp_dir, ignore_errors=True)
            except Exception as e:
                logger.warning(f"Error checking debug files: {e}")
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    def _remove_accents(self, text: str) -> str:
        """Remove Vietnamese accents"""
        try:
            import unidecode
            return unidecode.unidecode(text)
        except:
            return text
    
    async def _extract_pagination_info(self, frame) -> Optional[Dict[str, int]]:
        """
        Extract pagination info t·ª´ div #currAcc
        Returns: {"current_page": 1, "total_pages": 2, "total_records": 13} ho·∫∑c None
        """
        try:
            pagination_div = frame.locator('#currAcc').first
            if await pagination_div.count() == 0:
                logger.warning("‚ö†Ô∏è Cannot find #currAcc div")
                return None
            
            html_content = await pagination_div.inner_html()
            if html_content:
                import re
                page_match = re.search(r'Trang\s+(\d+)\s*/\s*<b>(\d+)</b>', html_content)
                records_match = re.search(r'C√≥\s+<b>(\d+)</b>\s+b·∫£n\s+ghi', html_content)
                
                if page_match:
                    current_page = int(page_match.group(1))
                    total_pages = int(page_match.group(2))
                    total_records = int(records_match.group(1)) if records_match else 0
                    return {
                        "current_page": current_page,
                        "total_pages": total_pages,
                        "total_records": total_records
                    }
            
            text = await pagination_div.text_content()
            if not text:
                logger.warning("‚ö†Ô∏è Pagination div has no content")
                return None
            
            import re
            page_match = re.search(r'Trang\s+(\d+)\s*/\s*(\d+)', text)
            records_match = re.search(r'C√≥\s+(\d+)\s+b·∫£n\s+ghi', text)
            
            if page_match:
                current_page = int(page_match.group(1))
                total_pages = int(page_match.group(2))
                total_records = int(records_match.group(1)) if records_match else 0
                return {
                    "current_page": current_page,
                    "total_pages": total_pages,
                    "total_records": total_records
                }
            
            logger.warning(f"‚ö†Ô∏è Cannot parse pagination info from text: {text[:100]}")
            return None
            
        except Exception as e:
            logger.warning(f"Error extracting pagination info: {e}")
            return None
    
    async def _navigate_to_page(self, frame, target_page: int) -> bool:
        """
        Navigate ƒë·∫øn trang c·ª• th·ªÉ b·∫±ng link c√≥ s·∫µn trong HTML
        Returns: True n·∫øu navigate th√†nh c√¥ng, False n·∫øu kh√¥ng t√¨m th·∫•y link
        """
        try:
            try:
                pagination_div = frame.locator('#currAcc').first
                await pagination_div.wait_for(timeout=3000)
            except:
                logger.warning(f"‚ö†Ô∏è Cannot find #currAcc div, trying to navigate anyway")
                pass
            
            link = None
            
            pagination_div = frame.locator('#currAcc').first
            if await pagination_div.count() > 0:
                link = pagination_div.locator(f'a[href*="&pn={target_page}"]').first
                if await link.count() == 0:
                    link = pagination_div.locator(f'a:has-text("{target_page}")').first
            
            if not link or await link.count() == 0:
                link = frame.locator(f'a[href*="&pn={target_page}"]').first
                if await link.count() == 0:
                    link = frame.locator(f'a:has-text("{target_page}")').first
            
            if not link or await link.count() == 0:
                logger.warning(f"‚ö†Ô∏è Cannot find link to page {target_page}")
                return False
            
            # Click link
            await link.click()
            
            try:
                table_body = frame.locator('#allResultTableBody, table.md_list2 tbody, table#data_content_onday tbody').first
                await table_body.wait_for(timeout=5000)
                await asyncio.sleep(1)
            except:
                logger.warning(f"‚ö†Ô∏è Table not found after navigating to page {target_page}")
            
            pagination_info = await self._extract_pagination_info(frame)
            if pagination_info and pagination_info["current_page"] == target_page:
                logger.info(f"‚úÖ Navigated to page {target_page}")
            else:
                logger.info(f"‚úÖ Navigated to page {target_page} (verification: current_page={pagination_info.get('current_page') if pagination_info else 'unknown'})")
            
            return True
                
        except Exception as e:
            logger.error(f"Error navigating to page {target_page}: {e}")
            return False
    
    async def _take_screenshot_on_download_error(
        self,
        session_id: str,
        ma_tkhai: str,
        error_reason: str,
        frame=None
    ) -> Optional[str]:
        """
        Ch·ª•p m√†n h√¨nh khi download fail ƒë·ªÉ debug
        
        Args:
            session_id: Session ID ƒë·ªÉ l·∫•y page
            ma_tkhai: M√£ giao d·ªãch c·ªßa t·ªù khai
            error_reason: L√Ω do l·ªói (ƒë·ªÉ ƒë·∫∑t t√™n file)
            frame: Frame hi·ªán t·∫°i (n·∫øu c√≥)
        
        Returns:
            Path ƒë·∫øn file screenshot ho·∫∑c None n·∫øu fail
        """
        try:
            # L·∫•y session ƒë·ªÉ c√≥ page
            session = self.session_manager.get_session(session_id)
            if not session or not session.page:
                logger.warning(f"[{ma_tkhai}] Cannot take screenshot: No session or page")
                return None
            
            page = session.page
            
            # T·∫°o th∆∞ m·ª•c screenshots trong project
            # L·∫•y ƒë∆∞·ªùng d·∫´n project (tool-go-soft)
            # File n√†y ·ªü: tool-gotax/tool-go-soft/services/tax_crawler.py
            # Mu·ªën t·ªõi: tool-gotax/tool-go-soft/screenshots/
            current_dir = os.path.dirname(os.path.abspath(__file__))  # .../services/
            services_dir = os.path.dirname(current_dir)  # .../tool-go-soft/
            screenshots_dir = os.path.join(services_dir, "screenshots")  # .../tool-go-soft/screenshots/
            os.makedirs(screenshots_dir, exist_ok=True)
            
            # T·∫°o t√™n file v·ªõi timestamp v√† ma_tkhai
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_error_reason = error_reason.replace(" ", "_").replace("/", "_").replace("\\", "_")[:50]
            screenshot_filename = f"ERROR_{ma_tkhai}_{safe_error_reason}_{timestamp}.png"
            screenshot_path = os.path.join(screenshots_dir, screenshot_filename)
            
            # Ch·ª•p m√†n h√¨nh
            try:
                # ‚úÖ FIX: Frame kh√¥ng c√≥ method screenshot() tr·ª±c ti·∫øp
                # D√πng page.screenshot() ƒë·ªÉ ch·ª•p to√†n b·ªô page (bao g·ªìm frame)
                # Ho·∫∑c n·∫øu mu·ªën ch·ª•p ch·ªâ frame, d√πng frame.locator('body').screenshot()
                if frame:
                    # Th·ª≠ ch·ª•p frame content tr∆∞·ªõc (n·∫øu ƒë∆∞·ª£c)
                    try:
                        frame_body = frame.locator('body')
                        await frame_body.screenshot(path=screenshot_path, timeout=5000)
                        if os.path.exists(screenshot_path):
                            file_size = os.path.getsize(screenshot_path)
                            logger.warning(f"üì∏ [{ma_tkhai}] Screenshot saved (frame): {screenshot_path} ({file_size} bytes) - Error: {error_reason}")
                            return screenshot_path
                    except Exception as frame_e:
                        logger.debug(f"üì∏ [{ma_tkhai}] Cannot screenshot frame directly: {frame_e}, trying page screenshot")
                
                # Fallback: Ch·ª•p to√†n b·ªô page (s·∫Ω bao g·ªìm frame)
                await page.screenshot(path=screenshot_path, full_page=True, timeout=5000)
                
                if os.path.exists(screenshot_path):
                    file_size = os.path.getsize(screenshot_path)
                    logger.warning(f"üì∏ [{ma_tkhai}] Screenshot saved (page): {screenshot_path} ({file_size} bytes) - Error: {error_reason}")
                    return screenshot_path
                else:
                    logger.warning(f"üì∏ [{ma_tkhai}] Screenshot file not created: {screenshot_path}")
                    return None
            except Exception as e:
                logger.warning(f"üì∏ [{ma_tkhai}] Error taking screenshot: {e}")
                return None
                
        except Exception as e:
            logger.warning(f"üì∏ [{ma_tkhai}] Error in _take_screenshot_on_download_error: {e}")
            return None
    
    async def _navigate_to_page(self, frame, target_page: int) -> bool:
        """
        Navigate ƒë·∫øn trang c·ª• th·ªÉ b·∫±ng c√°ch click v√†o link pagination
        
        Args:
            frame: Frame hi·ªán t·∫°i
            target_page: S·ªë trang c·∫ßn navigate (1, 2, 3, ...)
        
        Returns:
            True n·∫øu navigate th√†nh c√¥ng, False n·∫øu fail
        """
        try:
            # ‚úÖ FIX: Check trang hi·ªán t·∫°i tr∆∞·ªõc, n·∫øu ƒëang ·ªü target page th√¨ kh√¥ng c·∫ßn navigate
            pagination_info = await self._extract_pagination_info(frame)
            if pagination_info:
                current_page = pagination_info.get("current_page", 0)
                if current_page == target_page:
                    logger.info(f"‚úÖ Already on page {target_page}")
                    return True
            
            # N·∫øu l√† trang 1 v√† kh√¥ng c√≥ pagination info, gi·∫£ ƒë·ªãnh ƒëang ·ªü trang 1
            if target_page == 1 and not pagination_info:
                logger.info(f"‚úÖ Assuming already on page 1 (no pagination info)")
                return True
            
            # ƒê·ª£i pagination div xu·∫•t hi·ªán
            try:
                pagination_div = frame.locator('#currAcc').first
                await pagination_div.wait_for(timeout=5000)
            except:
                logger.warning(f"‚ö†Ô∏è Cannot find #currAcc div")
                # N·∫øu kh√¥ng c√≥ pagination div v√† target l√† trang 1, gi·∫£ ƒë·ªãnh OK
                if target_page == 1:
                    return True
                return False
            
            # ‚úÖ FIX: T√¨m link b·∫±ng nhi·ªÅu c√°ch
            # C√°ch 1: T√¨m t·∫•t c·∫£ link trong pagination v√† check href c√≥ ch·ª©a pn={target_page}
            # (Handle c·∫£ &&pn= v√† &pn=)
            link = None
            all_links = pagination_div.locator('a')
            link_count = await all_links.count()
            
            for i in range(link_count):
                link_elem = all_links.nth(i)
                href = await link_elem.get_attribute('href') or ''
                # Check nhi·ªÅu pattern: pn=2, &pn=2, &&pn=2
                if f'pn={target_page}' in href:
                    link = link_elem
                    break
            
            # C√°ch 2: N·∫øu kh√¥ng t√¨m th·∫•y qua href, t√¨m link c√≥ text ch√≠nh x√°c = target_page
            if not link:
                for i in range(link_count):
                    link_elem = all_links.nth(i)
                    text = await link_elem.text_content()
                    if text and text.strip() == str(target_page):
                        link = link_elem
                        break
            
            # Click v√†o link n·∫øu t√¨m th·∫•y
            if link:
                await link.click()
                await asyncio.sleep(1)  # ƒê·ª£i page load
                
                # Verify navigation th√†nh c√¥ng b·∫±ng c√°ch check pagination info
                try:
                    await asyncio.sleep(0.5)  # ƒê·ª£i th√™m m·ªôt ch√∫t
                    pagination_info = await self._extract_pagination_info(frame)
                    if pagination_info:
                        current_page = pagination_info.get("current_page", 0)
                        if current_page == target_page:
                            logger.info(f"‚úÖ Navigated to page {target_page} (verified)")
                            return True
                        else:
                            logger.warning(f"‚ö†Ô∏è Navigation verification failed: expected page {target_page}, got {current_page}")
                    else:
                        logger.warning(f"‚ö†Ô∏è Cannot verify navigation (no pagination info)")
                        # V·∫´n return True v√¨ ƒë√£ click ƒë∆∞·ª£c
                        return True
                except Exception as verify_e:
                    logger.warning(f"‚ö†Ô∏è Error verifying navigation: {verify_e}")
                    # V·∫´n return True v√¨ ƒë√£ click ƒë∆∞·ª£c
                    return True
            else:
                logger.warning(f"‚ö†Ô∏è Cannot find link to page {target_page}")
                return False
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error navigating to page {target_page}: {e}")
            return False
    
    async def _extract_pagination_info(self, frame) -> Optional[Dict[str, int]]:
        """
        Extract pagination info t·ª´ #currAcc div
        Returns: {"current_page": 1, "total_pages": 2, "total_records": 13} ho·∫∑c None
        """
        try:
            pagination_div = frame.locator('#currAcc').first
            if await pagination_div.count() == 0:
                return None
            
            # L·∫•y innerHTML ƒë·ªÉ c√≥ th·ªÉ parse <b> tags
            html_content = await pagination_div.inner_html()
            
            # Pattern 1: "Trang 1/<b>2</b>. C√≥ <b>13</b> b·∫£n ghi."
            import re
            pattern_html = r"Trang\s+(\d+)/<b>(\d+)</b>\.\s+C√≥\s+<b>(\d+)</b>\s+b·∫£n\s+ghi"
            match = re.search(pattern_html, html_content)
            
            if match:
                current_page = int(match.group(1))
                total_pages = int(match.group(2))
                total_records = int(match.group(3))
                return {
                    "current_page": current_page,
                    "total_pages": total_pages,
                    "total_records": total_records
                }
            
            # Pattern 2: "Trang 1/2. C√≥ 13 b·∫£n ghi." (plain text)
            text_content = await pagination_div.text_content()
            pattern_text = r"Trang\s+(\d+)/(\d+)\.\s+C√≥\s+(\d+)\s+b·∫£n\s+ghi"
            match = re.search(pattern_text, text_content)
            
            if match:
                current_page = int(match.group(1))
                total_pages = int(match.group(2))
                total_records = int(match.group(3))
                return {
                    "current_page": current_page,
                    "total_pages": total_pages,
                    "total_records": total_records
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error extracting pagination info: {e}")
            return None
    
    async def _extract_download_params(self, frame) -> Optional[Dict[str, str]]:
        """
        L·∫•y c√°c tham s·ªë dse_* t·ª´ form (c·∫ßn thi·∫øt ƒë·ªÉ build URL download)
        ‚úÖ FIX: Lo·∫°i b·ªè dse_pageId v√† pn (pagination params), d√πng dse_pageId c·ªë ƒë·ªãnh khi download
        """
        try:
            params = {}
            
            # L·∫•y t·ª´ hidden inputs trong form
            form = frame.locator('form[name="traCuuKhaiForm"], form#traCuuKhaiForm').first
            if await form.count() > 0:
                inputs = form.locator('input[type="hidden"]')
                input_count = await inputs.count()
                
                for i in range(input_count):
                    input_elem = inputs.nth(i)
                    name = await input_elem.get_attribute('name')
                    value = await input_elem.get_attribute('value')
                    if name and value:
                        # ‚úÖ FIX: B·ªè qua dse_pageId v√† pn (pagination params)
                        # dse_pageId trong form l√† s·ªë trang pagination (6, 7, ...)
                        # Khi download c·∫ßn d√πng dse_pageId c·ªë ƒë·ªãnh (14 ho·∫∑c 8)
                        if name in ['dse_pageId', 'pn']:
                            continue
                        params[name] = value
            
            # Th√™m c√°c params c·ªë ƒë·ªãnh
            params.update({
                'dse_operationName': 'traCuuToKhaiProc',
                'dse_processorState': 'viewTraCuuTkhai',
                'dse_nextEventName': 'downTkhai',
                'dse_applicationId': '-1'
            })
            
            logger.info(f"Extracted download params: {list(params.keys())}")
            return params if params.get('dse_sessionId') else None
            
        except Exception as e:
            logger.error(f"Error extracting download params: {e}")
            return None
    
    async def _download_one_via_url(
        self, 
        session_id: str,
        ma_tkhai: str, 
        item: Dict,
        base_params: Dict[str, str],
        temp_dir: str,
        frame=None
    ) -> Optional[Dict]:
        """
        Download 1 file b·∫±ng c√°ch g·ªçi URL tr·ª±c ti·∫øp v·ªõi httpx
        ‚úÖ FIX: Th√™m frame parameter ƒë·ªÉ c√≥ th·ªÉ navigate v·ªÅ ƒë√∫ng trang n·∫øu c·∫ßn
        """
        try:
            # Build URL v·ªõi params
            params = base_params.copy()
            params['messageId'] = ma_tkhai
            
            # ‚úÖ FIX: Set dse_pageId c·ªë ƒë·ªãnh (kh√¥ng d√πng t·ª´ form)
            # N·∫øu c√≥ dse_processorId, d√πng pageId=8, ng∆∞·ª£c l·∫°i d√πng pageId=14
            if params.get('dse_processorId'):
                params['dse_pageId'] = '8'
            else:
                params['dse_pageId'] = '14'
            
            # ‚úÖ FIX: Th√™m pn (page number) d·ª±a tr√™n page_number c·ªßa item
            # pn c·∫ßn match v·ªõi trang c·ªßa item ƒë·ªÉ download ƒë√∫ng
            page_number = item.get("page_number", 1)
            params['pn'] = str(page_number)
            
            # X√°c ƒë·ªãnh download_type ƒë·ªÉ set dse_nextEventName
            download_type = item.get("download_type", "downloadTkhai")
            if download_type == "downloadBke":
                params['dse_nextEventName'] = 'downBke'
            else:
                params['dse_nextEventName'] = 'downTkhai'
            
            download_url = "https://thuedientu.gdt.gov.vn/etaxnnt/Request"
            
            # L·∫•y httpx client (ƒë√£ c√≥ cookies t·ª´ session)
            http_client = await self._get_http_client(session_id)
            if not http_client:
                logger.warning(f"No http client for {ma_tkhai}")
                # ‚úÖ Ch·ª•p m√†n h√¨nh khi kh√¥ng c√≥ http client
                await self._take_screenshot_on_download_error(
                    session_id, ma_tkhai, "No_http_client", frame
                )
                return None
            
            # ‚úÖ FIX: Retry logic cho t·ªù thuy·∫øt minh v√† c√°c file c√≥ th·ªÉ fail
            max_retries = 2 if download_type == "downloadBke" else 1
            retry_delay = 1.0
            
            for retry in range(max_retries):
                if retry > 0:
                    await asyncio.sleep(retry_delay)
                
                # ‚úÖ FIX QUAN TR·ªåNG: Navigate v·ªÅ ƒë√∫ng trang TR∆Ø·ªöC KHI download (c·∫£ l·∫ßn ƒë·∫ßu v√† retry)
                # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o browser ƒëang ·ªü ƒë√∫ng trang c·ªßa item, tr√°nh server tr·∫£ v·ªÅ HTML c·ªßa trang kh√°c
                if frame:
                    try:
                        navigate_success = await self._navigate_to_page(frame, page_number)
                        if navigate_success:
                            # ƒê·ª£i table load sau khi navigate
                            try:
                                table_body = frame.locator('#allResultTableBody, table.md_list2 tbody, table#data_content_onday tbody').first
                                await table_body.wait_for(timeout=5000)
                                await asyncio.sleep(0.5)  # ƒê·ª£i th√™m m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o page ƒë√£ load xong
                            except:
                                pass
                            
                            fresh_params = await self._extract_download_params(frame)
                            if fresh_params:
                                params.update({
                                    'dse_sessionId': fresh_params.get('dse_sessionId', params.get('dse_sessionId')),
                                    'dse_processorId': fresh_params.get('dse_processorId', params.get('dse_processorId')),
                                })
                        else:
                            pass
                    except Exception as nav_e:
                        pass
                
                try:
                    response = await http_client.get(download_url, params=params, timeout=30.0)
                except Exception as e:
                    if retry < max_retries - 1:
                        continue
                    # ‚úÖ Ch·ª•p m√†n h√¨nh khi fail sau khi retry h·∫øt
                    await self._take_screenshot_on_download_error(
                        session_id, ma_tkhai, f"Request_error_{str(e)[:30]}", frame
                    )
                    return None
            
                if response.status_code != 200:
                    if retry < max_retries - 1:
                        continue
                    await self._take_screenshot_on_download_error(
                        session_id, ma_tkhai, f"HTTP_{response.status_code}", frame
                    )
                    return None
                
                if len(response.content) == 0:
                    location = response.headers.get('location', '')
                    if retry < max_retries - 1:
                        continue
                    # ‚úÖ Ch·ª•p m√†n h√¨nh khi fail sau khi retry h·∫øt
                    await self._take_screenshot_on_download_error(
                        session_id, ma_tkhai, "Empty_response_0_bytes", frame
                    )
                    return None
                
                content = response.content
                content_type = response.headers.get('content-type', '').lower()
                
                # ‚úÖ FIX: Check content-type header tr∆∞·ªõc
                is_xml_by_type = 'xml' in content_type or 'text/xml' in content_type
                is_xlsx_by_type = 'spreadsheet' in content_type or 'excel' in content_type or 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' in content_type
                is_xls_by_type = 'application/vnd.ms-excel' in content_type or 'application/msexcel' in content_type
                
                # Validate XML/XLSX/XLS by content
                is_xml = content.startswith(b'<?xml') or b'<HSoTKhai' in content or b'<TKhai' in content or b'<BKe' in content
                
                # ‚úÖ FIX: XLSX files start with PK (ZIP signature)
                # ZIP local file header: PK\x03\x04
                # ZIP end of central directory: PK\x05\x06
                # Check ·ªü ƒë·∫ßu file (c√≥ th·ªÉ c√≥ BOM nh∆∞ng th∆∞·ªùng kh√¥ng)
                is_xlsx = (len(content) >= 2 and content[:2] == b'PK') or \
                          (len(content) >= 4 and content[0:2] == b'PK' and content[2:4] in [b'\x03\x04', b'\x05\x06'])
                
                # ‚úÖ FIX: XLS files (Excel 97-2003) start with OLE2 signature
                # OLE2 signature: D0 CF 11 E0 A1 B1 1A E1 (8 bytes ƒë·∫ßu)
                # ‚úÖ FIX: M·ªü r·ªông check cho c·∫£ downloadTkhai khi content-type l√† application/octet-stream
                # (BCTC c√≥ type downloadTkhai nh∆∞ng server c√≥ th·ªÉ tr·∫£ v·ªÅ XLS file)
                is_xls = False
                if len(content) >= 8:
                    xls_signature = b'\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1'
                    if content[:8] == xls_signature:
                        # Accept XLS cho downloadBke ho·∫∑c khi content-type kh√¥ng r√µ r√†ng
                        if download_type == "downloadBke" or 'application/octet-stream' in content_type or content_type == '':
                            is_xls = True
                            logger.info(f"[{ma_tkhai}] Detected XLS file (Excel 97-2003) - OLE2 signature")
                
                # ‚úÖ FIX: Check xem c√≥ ph·∫£i l√† HTML error page kh√¥ng (∆∞u ti√™n check n√†y tr∆∞·ªõc)
                # Check trong to√†n b·ªô content (kh√¥ng ch·ªâ 500 bytes ƒë·∫ßu) v√¨ HTML c√≥ th·ªÉ n·∫±m ·ªü gi·ªØa
                is_html = (b'<!DOCTYPE html>' in content or 
                          b'<html' in content.lower()[:1000] or 
                          b'<HTML' in content[:1000] or
                          'html' in content_type or
                          (len(content) > 1000 and b'<body' in content.lower()[:2000] and b'<head' in content.lower()[:2000]))
                
                # ‚úÖ FIX: N·∫øu l√† HTML, save file t·∫°m ƒë·ªÉ debug v√† reject
                if is_html:
                    # Save file t·∫°m ƒë·ªÉ inspect
                    debug_file = os.path.join(temp_dir, f"DEBUG_{ma_tkhai}_retry{retry+1}.html")
                    try:
                        with open(debug_file, 'wb') as f:
                            f.write(content)
                        logger.warning(f"[{ma_tkhai}] Server returned HTML page (saved to {debug_file}, content-type: {content_type}, size: {len(content)} bytes, retry: {retry+1}/{max_retries})")
                    except:
                        pass
                    
                    content_preview = content[:500].decode('utf-8', errors='ignore')
                    logger.debug(f"[{ma_tkhai}] Content preview: {content_preview[:200]}")
                    if retry < max_retries - 1:
                        continue
                    # ‚úÖ Ch·ª•p m√†n h√¨nh khi fail sau khi retry h·∫øt
                    await self._take_screenshot_on_download_error(
                        session_id, ma_tkhai, "HTML_response", frame
                    )
                    return None
                
                # ‚úÖ FIX: N·∫øu content h·ª£p l·ªá (XML/XLSX/XLS), break kh·ªèi retry loop
                if (is_xml or is_xlsx or is_xls) or (is_xml_by_type or is_xlsx_by_type or is_xls_by_type):
                    break
                
                # ‚úÖ FIX: N·∫øu content-type l√† application/octet-stream v√† size h·ª£p l√Ω (>1000 bytes)
                # C√≥ th·ªÉ l√† XLSX file (server kh√¥ng set ƒë√∫ng content-type)
                # Check xem c√≥ ph·∫£i l√† binary file kh√¥ng (kh√¥ng ph·∫£i text/HTML)
                # ‚úÖ FIX: Check v·ªõi 'in' thay v√¨ '==' v√¨ c√≥ th·ªÉ c√≥ th√™m charset=utf-8
                if 'application/octet-stream' in content_type or content_type == '':
                    if len(content) > 1000:
                        # ‚úÖ FIX: Check XLS signature tr∆∞·ªõc (OLE2 - 8 bytes ƒë·∫ßu)
                        # ‚úÖ FIX: M·ªü r·ªông check cho c·∫£ downloadTkhai (BCTC c√≥ type downloadTkhai nh∆∞ng c√≥ th·ªÉ l√† XLS)
                        if len(content) >= 8:
                            xls_signature = b'\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1'
                            if content[:8] == xls_signature:
                                logger.info(f"[{ma_tkhai}] Detected XLS file (Excel 97-2003) despite content-type: {content_type}")
                                is_xls = True
                                break
                        
                        # ‚úÖ FIX: Check PK signature ·ªü nhi·ªÅu v·ªã tr√≠ (c√≥ th·ªÉ b·ªã wrap ho·∫∑c offset)
                        # Check ·ªü ƒë·∫ßu file (0-100 bytes)
                        pk_positions = []
                        for offset in range(0, min(100, len(content) - 2)):
                            if content[offset:offset+2] == b'PK':
                                pk_positions.append(offset)
                                # Check xem c√≥ ph·∫£i l√† ZIP/XLSX signature h·ª£p l·ªá kh√¥ng
                                if offset + 4 <= len(content):
                                    next_bytes = content[offset+2:offset+4]
                                    if next_bytes in [b'\x03\x04', b'\x05\x06', b'\x07\x08']:
                                        logger.info(f"[{ma_tkhai}] Detected XLSX/ZIP file (PK signature at offset {offset}) despite content-type: {content_type}")
                                        is_xlsx = True
                                        break
                        
                        if is_xlsx:
                            break
                            
                        # XML signature: <?xml (check ·ªü ƒë·∫ßu file)
                        first_bytes = content[:10] if len(content) >= 10 else content
                        if first_bytes[:5] == b'<?xml':
                            logger.info(f"[{ma_tkhai}] Detected XML file despite content-type: {content_type}")
                            is_xml = True
                            break
                        
                        # ‚úÖ FIX: N·∫øu t√¨m th·∫•y PK nh∆∞ng kh√¥ng ph·∫£i signature h·ª£p l·ªá, log ƒë·ªÉ debug
                        if pk_positions:
                            logger.debug(f"[{ma_tkhai}] Found PK at positions {pk_positions} but not valid ZIP signature")
                        # N·∫øu kh√¥ng ph·∫£i HTML v√† size h·ª£p l√Ω, c√≥ th·ªÉ l√† binary file h·ª£p l·ªá
                        # ƒê·∫∑c bi·ªát cho downloadBke (t·ªù thuy·∫øt minh) - th∆∞·ªùng l√† XLSX
                        elif len(content) > 5000:
                            # ‚úÖ FIX: B·ªè l∆∞u .bin ‚Üí L∆∞u HTML c·ªßa frame browser ƒë·ªÉ debug
                            logger.warning(f"[{ma_tkhai}] Unknown binary content (content-type: {content_type}, size: {len(content)} bytes). First bytes (hex): {first_bytes.hex()[:40]}")
                            try:
                                # L∆∞u HTML c·ªßa frame hi·ªán t·∫°i ƒë·ªÉ debug (thay v√¨ l∆∞u .bin)
                                if frame:
                                    frame_html = await frame.content()
                                    debug_html_file = os.path.join(temp_dir, f"DEBUG_{ma_tkhai}_retry{retry+1}_page.html")
                                    with open(debug_html_file, 'w', encoding='utf-8') as f:
                                        f.write(frame_html)
                                    logger.warning(f"[{ma_tkhai}] Saved browser page HTML to {debug_html_file}")
                                # ‚úÖ Ch·ª•p m√†n h√¨nh ngay khi l∆∞u file debug
                                await self._take_screenshot_on_download_error(
                                    session_id, ma_tkhai, f"Unknown_binary_{len(content)}bytes", frame
                                )
                            except Exception as e:
                                logger.warning(f"[{ma_tkhai}] Error saving debug HTML: {e}")
                            
                            # Log ƒë·ªÉ debug
                            logger.debug(f"[{ma_tkhai}] First 100 bytes (hex): {content[:100].hex()}")
                            # Check xem c√≥ ph·∫£i l√† text/HTML kh√¥ng (check m·ªôt ph·∫ßn content)
                            try:
                                text_sample = content[:1000].decode('utf-8', errors='ignore')
                                if '<html' in text_sample.lower() or '<!doctype' in text_sample.lower():
                                    logger.warning(f"[{ma_tkhai}] Content appears to be HTML despite size")
                                    if retry < max_retries - 1:
                                        continue
                                    return None
                            except:
                                pass
                            
                            # Retry ƒë·ªÉ xem c√≥ ph·∫£i l√† timing issue kh√¥ng
                            if retry < max_retries - 1:
                                continue
                            # N·∫øu l√† downloadBke v√† size l·ªõn, c√≥ th·ªÉ accept (server c√≥ th·ªÉ tr·∫£ v·ªÅ file nh∆∞ng kh√¥ng ƒë√∫ng format)
                            # Nh∆∞ng ƒë·ªÉ an to√†n, reject n·∫øu kh√¥ng detect ƒë∆∞·ª£c signature
                            # ‚úÖ Ch·ª•p m√†n h√¨nh khi fail sau khi retry h·∫øt
                            await self._take_screenshot_on_download_error(
                                session_id, ma_tkhai, f"Unknown_binary_{len(content)}bytes", frame
                            )
                            return None
                
                # ‚úÖ FIX: N·∫øu kh√¥ng detect ƒë∆∞·ª£c, save file debug v√† log
                logger.warning(f"[{ma_tkhai}] Not XML/XLSX/XLS content (content-type: {content_type}, size: {len(content)} bytes, retry: {retry+1}/{max_retries})")
                
                # ‚úÖ FIX: B·ªè l∆∞u .bin ‚Üí L∆∞u HTML c·ªßa frame browser ƒë·ªÉ debug
                if len(content) > 1000 and frame:
                    try:
                        frame_html = await frame.content()
                        debug_html_file = os.path.join(temp_dir, f"DEBUG_{ma_tkhai}_retry{retry+1}_page.html")
                        with open(debug_html_file, 'w', encoding='utf-8') as f:
                            f.write(frame_html)
                        logger.warning(f"[{ma_tkhai}] Saved browser page HTML to {debug_html_file}")
                        # ‚úÖ Ch·ª•p m√†n h√¨nh ngay khi l∆∞u file debug
                        await self._take_screenshot_on_download_error(
                            session_id, ma_tkhai, f"Invalid_content_{len(content)}bytes", frame
                        )
                    except Exception as e:
                        logger.warning(f"[{ma_tkhai}] Failed to save debug HTML: {e}")
                
                # Log th√™m ƒë·ªÉ debug
                if len(content) > 0:
                    first_bytes = content[:100] if len(content) >= 100 else content
                    logger.debug(f"[{ma_tkhai}] First bytes (hex): {first_bytes.hex()[:200]}")
                    # Th·ª≠ decode ƒë·ªÉ xem c√≥ ph·∫£i text kh√¥ng
                    try:
                        text_preview = content[:200].decode('utf-8', errors='ignore')
                        logger.debug(f"[{ma_tkhai}] First 200 chars (text): {text_preview[:100]}")
                    except:
                        pass
                
                # Retry n·∫øu c√≤n l∆∞·ª£t
                if retry < max_retries - 1:
                    continue
                # ‚úÖ Ch·ª•p m√†n h√¨nh khi fail sau khi retry h·∫øt
                await self._take_screenshot_on_download_error(
                    session_id, ma_tkhai, f"Invalid_content_{content_type}_{len(content)}bytes", frame
                )
                return None
            
            # ‚úÖ FIX: N·∫øu ra kh·ªèi loop m√† v·∫´n kh√¥ng c√≥ content h·ª£p l·ªá, return None
            if not (is_xml or is_xlsx or is_xls) and not (is_xml_by_type or is_xlsx_by_type or is_xls_by_type):
                logger.warning(f"[{ma_tkhai}] Failed after {max_retries} retries")
                # ‚úÖ Ch·ª•p m√†n h√¨nh khi fail sau khi retry h·∫øt
                await self._take_screenshot_on_download_error(
                    session_id, ma_tkhai, f"Failed_after_{max_retries}_retries", frame
                )
                return None
            
            # L∆∞u file v·ªõi extension ƒë√∫ng
            file_name = item["file_name"]
            # ‚úÖ FIX: X√°c ƒë·ªãnh extension d·ª±a tr√™n file type th·ª±c t·∫ø
            if is_xls:
                file_ext = ".xls"
            elif is_xlsx or (download_type == "downloadBke" and not is_xml):
                file_ext = ".xlsx"
            else:
                file_ext = ".xml"
            
            # ‚úÖ FIX: Validate m√£ giao d·ªãch trong file XML ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªã l·ªôn file
            if is_xml:
                try:
                    content_str = content.decode('utf-8', errors='ignore')
                    # Check xem m√£ giao d·ªãch c√≥ trong file kh√¥ng
                    if ma_tkhai not in content_str:
                        logger.warning(f"‚ö†Ô∏è [{ma_tkhai}] M√£ giao d·ªãch kh√¥ng t√¨m th·∫•y trong XML content - c√≥ th·ªÉ file b·ªã l·ªôn!")
                        # V·∫´n l∆∞u nh∆∞ng log warning ƒë·ªÉ debug
                except:
                    pass
            
            # N·∫øu file_name ƒë√£ c√≥ extension h·ª£p l·ªá, gi·ªØ nguy√™n, n·∫øu kh√¥ng th√¨ th√™m extension
            if not file_name.endswith((".xml", ".xlsx", ".xls")):
                save_path = os.path.join(temp_dir, file_name + file_ext)
            else:
                save_path = os.path.join(temp_dir, file_name)
            
            with open(save_path, 'wb') as f:
                f.write(content)
            
            # Verify
            if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                logger.info(f"‚úÖ [{ma_tkhai}] Downloaded {len(content)} bytes -> {os.path.basename(save_path)}")
                return item
            else:
                logger.warning(f"‚ùå [{ma_tkhai}] File not saved")
                # ‚úÖ Ch·ª•p m√†n h√¨nh khi file kh√¥ng ƒë∆∞·ª£c save
                await self._take_screenshot_on_download_error(
                    session_id, ma_tkhai, "File_not_saved", frame
                )
                return None
                
        except asyncio.TimeoutError:
            logger.warning(f"‚è±Ô∏è [{ma_tkhai}] Timeout")
            # ‚úÖ Ch·ª•p m√†n h√¨nh khi timeout
            await self._take_screenshot_on_download_error(
                session_id, ma_tkhai, "Timeout", frame
            )
            return None
        except Exception as e:
            logger.warning(f"‚ùå [{ma_tkhai}] Error: {e}")
            # ‚úÖ Ch·ª•p m√†n h√¨nh khi c√≥ exception
            await self._take_screenshot_on_download_error(
                session_id, ma_tkhai, f"Exception_{str(e)[:30]}", frame
            )
            return None
    
    async def _batch_download_one(self, session: SessionData, item: Dict, temp_dir: str, ssid: str, frame, session_id: str = None) -> Optional[Dict]:
        """
        Download m·ªôt item b·∫±ng c√°ch g·ªçi JS function v√† intercept response
        ‚úÖ FIXED: Wait for navigation, then extract XML from final page
        """
        try:
            id_tk = item["id"]
            file_name = item["file_name"]
            has_link = item.get("has_link", False)
            download_type = item.get("download_type", "downloadTkhai")
            
            # B·ªè qua tr∆∞·ªùng h·ª£p kh√¥ng c√≥ link
            if not has_link:
                logger.info(f"Skipping {id_tk} - no download link (special tokhai)")
                return None
            
            logger.info(f"[DEBUG] Downloading {id_tk} via {download_type}")
            
            page = session.page
            
            # ============================================
            # ‚úÖ STRATEGY: G·ªçi JS ‚Üí ƒê·ª£i navigation ‚Üí L·∫•y XML t·ª´ page cu·ªëi c√πng
            # ============================================
            
            try:
                # B∆∞·ªõc 1: G·ªçi JavaScript function (trigger navigation)
                js_function = "downloadBke" if download_type == "downloadBke" else "downloadTkhai"
                logger.info(f"[DEBUG] Calling {js_function}('{id_tk}')")
                
                # Track navigation
                navigation_promise = page.wait_for_load_state('domcontentloaded', timeout=30000)
                
                # G·ªçi function trong frame
                await frame.evaluate(f"{js_function}('{id_tk}')")
                logger.info(f"[DEBUG] Called {js_function}('{id_tk}')")
                
                # B∆∞·ªõc 2: ƒê·ª£i navigation ho√†n t·∫•t
                try:
                    await navigation_promise
                    logger.info(f"[DEBUG] Navigation completed for {id_tk}")
                except:
                    # N·∫øu kh√¥ng c√≥ navigation event, ƒë·ª£i m·ªôt ch√∫t
                    await asyncio.sleep(2)
                    logger.info(f"[DEBUG] No navigation event, waited 2s for {id_tk}")
                
                # B∆∞·ªõc 3: ƒê·ª£i content load xong
                await asyncio.sleep(1)
                
                # B∆∞·ªõc 4: L·∫•y content t·ª´ page hi·ªán t·∫°i
                page_content = await page.content()
                logger.info(f"[DEBUG] Got page content, length: {len(page_content)}")
                
                # B∆∞·ªõc 5: Validate content
                is_xml = '<?xml' in page_content or '<HSoTKhai' in page_content or '<TKhai' in page_content
                is_error_page = 'timeout.jsp' in page.url or 'error' in page.url.lower()
                is_html_page = '<!DOCTYPE html>' in page_content or '<html' in page_content
                
                if is_error_page:
                    logger.warning(f"[DEBUG] Error page detected for {id_tk}")
                    return None
                
                if is_xml and not is_html_page:
                    # Content l√† XML thu·∫ßn t√∫y
                    response_data = page_content.encode('utf-8')
                    logger.info(f"[DEBUG] Valid XML content for {id_tk}: {len(response_data)} bytes")
                elif '<?xml' in page_content:
                    # XML n·∫±m trong HTML (c√≥ th·ªÉ trong <pre> tag ho·∫∑c embedded)
                    # Extract XML t·ª´ page
                    try:
                        # T√¨m XML trong page content
                        xml_start = page_content.find('<?xml')
                        if xml_start >= 0:
                            # T√¨m closing tag cu·ªëi c√πng
                            # Gi·∫£ s·ª≠ XML k·∫øt th√∫c b·∫±ng </HSoTKhai> ho·∫∑c tag t∆∞∆°ng t·ª±
                            closing_tags = ['</HSoTKhai>', '</TKhai>', '</BKe>']
                            xml_end = -1
                            for tag in closing_tags:
                                pos = page_content.rfind(tag)
                                if pos > xml_start:
                                    xml_end = pos + len(tag)
                                    break
                            
                            if xml_end > xml_start:
                                xml_content = page_content[xml_start:xml_end]
                                response_data = xml_content.encode('utf-8')
                                logger.info(f"[DEBUG] Extracted XML from HTML for {id_tk}: {len(response_data)} bytes")
                            else:
                                logger.warning(f"[DEBUG] Could not find XML closing tag for {id_tk}")
                                return None
                        else:
                            logger.warning(f"[DEBUG] No XML found in page content for {id_tk}")
                            return None
                    except Exception as e:
                        logger.warning(f"[DEBUG] Error extracting XML for {id_tk}: {e}")
                        return None
                else:
                    # Kh√¥ng ph·∫£i XML
                    logger.warning(f"[DEBUG] Content is not XML for {id_tk}")
                    logger.debug(f"[DEBUG] First 500 chars: {page_content[:500]}")
                    return None
                
                # B∆∞·ªõc 6: L∆∞u file
                if response_data and len(response_data) > 100:
                    file_ext = ".xlsx" if download_type == "downloadBke" else ".xml"
                    save_path = os.path.join(temp_dir, file_name + file_ext if not file_name.endswith((".xml", ".xlsx")) else file_name)
                    
                    # L∆∞u file
                    with open(save_path, 'wb') as f:
                        f.write(response_data)
                    
                    # Verify file
                    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                        # Final validation: check file content
                        with open(save_path, 'rb') as f:
                            first_bytes = f.read(100)
                            if b'<?xml' in first_bytes or b'<HSoTKhai' in first_bytes:
                                logger.info(f"‚úÖ Downloaded {id_tk} ({len(response_data)} bytes) -> {save_path}")
                                return item
                            else:
                                logger.warning(f"‚ùå Downloaded file is not valid XML for {id_tk}")
                                # Delete invalid file
                                os.remove(save_path)
                                return None
                    else:
                        logger.warning(f"‚ùå File not saved properly for {id_tk}")
                        return None
                else:
                    logger.warning(f"‚ùå No valid response data for {id_tk}")
                    return None
                    
            except Exception as inner_e:
                logger.warning(f"‚ùå Error in download process for {id_tk}: {inner_e}")
                return None
                
        except Exception as e:
            logger.warning(f"‚ùå Error downloading {item.get('id', 'unknown')}: {e}")
            return None
    
    async def _batch_download_optimized(
        self,
        session_id: str,
        download_queue: List[Dict],
        temp_dir: str,
        frame,
        batch_size: int = 3,
        page_params_map: Dict[int, Dict[str, str]] = None,
        progress_callback=None  # ‚úÖ Callback ƒë·ªÉ yield progress sau m·ªói file
    ) -> List[Dict]:
        """
        Download v·ªõi batching t·ªëi ∆∞u:
        - D√πng params t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng trang
        - Download theo batch (3-5 files/batch)
        - D√πng httpx thay v√¨ playwright navigation
        
        Returns: List c√°c item ƒë√£ download th√†nh c√¥ng
        """
        
        # ‚úÖ FIX: Group items theo page_number ƒë·ªÉ d√πng ƒë√∫ng params
        items_by_page = {}
        for item in download_queue:
            page_num = item.get("page_number", 1)
            if page_num not in items_by_page:
                items_by_page[page_num] = []
            items_by_page[page_num].append(item)
        
        logger.info(f"üìã Grouped {len(download_queue)} items into {len(items_by_page)} pages")
        
        if not page_params_map:
            logger.error("‚ùå No page_params_map provided, cannot download")
            return []
        
        # B∆∞·ªõc 2: Download t·ª´ng page v·ªõi params t∆∞∆°ng ·ª©ng
        total = len(download_queue)
        successful_downloads = []
        
        logger.info(f"üì¶ Starting batch download: {total} files, batch_size={batch_size}")
        
        # Download t·ª´ng page
        for page_num, page_items in sorted(items_by_page.items()):
            # L·∫•y params cho page n√†y
            base_params = page_params_map.get(page_num)
            if not base_params:
                logger.warning(f"‚ö†Ô∏è No params for page {page_num}, skipping {len(page_items)} items")
                continue
            
            logger.info(f"üìÑ Downloading page {page_num}: {len(page_items)} items")
            
            # ‚úÖ FIX: Navigate v·ªÅ ƒë√∫ng trang tr∆∞·ªõc khi download (ƒë·∫£m b·∫£o params ƒë√∫ng)
            # Ch·ªâ navigate n·∫øu kh√¥ng ph·∫£i trang 1 (v√¨ ƒë√£ ·ªü trang 1 r·ªìi)
            if page_num > 1:
                navigate_success = await self._navigate_to_page(frame, page_num)
                if not navigate_success:
                    logger.warning(f"‚ö†Ô∏è Cannot navigate to page {page_num}, skipping {len(page_items)} items")
                    continue
                
                # ƒê·ª£i table load v√† extract params l·∫°i (ƒë·∫£m b·∫£o params m·ªõi nh·∫•t)
                try:
                    table_body = frame.locator('#allResultTableBody, table.md_list2 tbody, table#data_content_onday tbody').first
                    await table_body.wait_for(timeout=5000)
                    await asyncio.sleep(1)
                    
                    # Extract params l·∫°i t·ª´ trang n√†y (c√≥ th·ªÉ ƒë√£ thay ƒë·ªïi sau khi navigate)
                    fresh_params = await self._extract_download_params(frame)
                    if fresh_params:
                        base_params = fresh_params
                        logger.info(f"‚úÖ Refreshed params for page {page_num}")
                except:
                    logger.warning(f"‚ö†Ô∏è Cannot refresh params for page {page_num}, using cached params")
            
            # ‚úÖ PH∆Ø∆†NG √ÅN 3: Sequential Download v·ªõi Refresh Params
            # Download tu·∫ßn t·ª± t·ª´ng file, refresh params tr∆∞·ªõc m·ªói file ƒë·ªÉ ƒë·∫£m b·∫£o ·ªïn ƒë·ªãnh
            logger.info(f"üì¶ Page {page_num}: Downloading {len(page_items)} files sequentially...")
            
            for idx, item in enumerate(page_items):
                file_num = idx + 1
                logger.info(f"üìÑ Page {page_num}, File {file_num}/{len(page_items)}: {item['id']}")
                
                # ‚úÖ Refresh params tr∆∞·ªõc m·ªói file ƒë·ªÉ ƒë·∫£m b·∫£o state ƒë√∫ng
                # (Tr√°nh tr∆∞·ªùng h·ª£p params b·ªã outdated sau khi download file tr∆∞·ªõc)
                try:
                    fresh_params = await self._extract_download_params(frame)
                    if fresh_params:
                        base_params = fresh_params
                        # logger.debug(f"‚úÖ Refreshed params for file {file_num}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Cannot refresh params for file {file_num}, using cached params: {e}")
                
                # Download file
                try:
                    result = await self._download_one_via_url(
                        session_id,
                        item["id"],
                        item,
                        base_params,
                        temp_dir,
                        frame=frame  # ‚úÖ Truy·ªÅn frame ƒë·ªÉ c√≥ th·ªÉ navigate khi retry
                    )
                    
                    if result and not isinstance(result, Exception):
                        successful_downloads.append(result)
                        logger.info(f"‚úÖ Page {page_num}, File {file_num}/{len(page_items)}: Success")
                        
                        # ‚úÖ Yield progress sau m·ªói file download xong
                        if progress_callback:
                            await progress_callback(result, successful_downloads, page_items)
                    else:
                        logger.warning(f"‚ùå Page {page_num}, File {file_num}/{len(page_items)}: Failed")
                
                except Exception as e:
                    logger.error(f"‚ùå Page {page_num}, File {file_num}/{len(page_items)}: Error - {e}")
                
                # ‚úÖ Delay nh·ªè gi·ªØa m·ªói file (0.1s) ƒë·ªÉ:
                # - Tr√°nh spam server
                # - ƒê·∫£m b·∫£o browser state ·ªïn ƒë·ªãnh
                # - V·∫´n t·∫£i ƒë·ªß nhanh (50 files ~ 5 gi√¢y ch·ªâ t√≠nh delay)
                await asyncio.sleep(0.1)
            
            # Sequential download ƒë√£ t·ª± ƒë·ªông t·∫°o kho·∫£ng c√°ch gi·ªØa c√°c page
            # Kh√¥ng c·∫ßn delay th√™m
        
        logger.info(f"üéâ Total downloaded: {len(successful_downloads)} / {total}")
        return successful_downloads
    
    async def _batch_download(self, session: SessionData, download_queue: List[Dict], temp_dir: str, ssid: str, frame, session_id: str = None, page_params_map: Dict[int, Dict[str, str]] = None):
        """
        ‚úÖ FIXED VERSION: Download v·ªõi batching t·ªëi ∆∞u d√πng httpx
        
        Returns: List c√°c item download th√†nh c√¥ng
        """
        if not session_id:
            logger.warning("No session_id provided, cannot use optimized download")
            return []
        
        # Filter ch·ªâ l·∫•y items c√≥ link (b·ªè qua special tokhai)
        download_queue_filtered = [item for item in download_queue if item.get("has_link", False)]
        
        if not download_queue_filtered:
            logger.warning("No items with download links")
            return []
        
        if not page_params_map:
            logger.warning("No page_params_map provided, cannot download")
            return []
        
        # G·ªçi download optimized
        return await self._batch_download_optimized(
            session_id=session_id,
            download_queue=download_queue_filtered,
            temp_dir=temp_dir,
            frame=frame,
            batch_size=3,  # T·ªëi ∆∞u: 3 files/batch
            page_params_map=page_params_map  # ‚úÖ FIX: Truy·ªÅn page_params_map
        )
    
    async def _batch_download_old(self, session: SessionData, download_queue: List[Dict], temp_dir: str, ssid: str, frame, session_id: str = None):
        """
        [DEPRECATED] Download nhi·ªÅu file song song (t·ªëi ∆∞u t·ªëc ƒë·ªô)
        Limit concurrent downloads = 5 ƒë·ªÉ kh√¥ng qu√° t·∫£i
        
        Returns: List c√°c item download th√†nh c√¥ng
        """
        semaphore = asyncio.Semaphore(5)  # Max 5 downloads c√πng l√∫c
        page = session.page
        successful_downloads = []  # Track nh·ªØng item download th√†nh c√¥ng
        
        async def download_one(item: Dict):
            async with semaphore:
                try:
                    id_tk = item["id"]
                    file_name = item["file_name"]
                    cols = item["cols"]
                    has_link = item.get("has_link", False)
                    download_type = item.get("download_type")  # "downloadTkhai", "downloadBke", ho·∫∑c None
                    
                    if has_link:
                        # C√≥ link - click ƒë·ªÉ download (b√¨nh th∆∞·ªùng)
                        download_link = cols.nth(2).locator('a')
                        # B·∫Øt download event t·ª´ page (kh√¥ng ph·∫£i frame)
                        async with page.expect_download(timeout=30000) as download_info:
                            await download_link.first.click()
                        
                        download = await download_info.value
                        # File thuy·∫øt minh c√≥ th·ªÉ l√† .xlsx, c√≤n t·ªù khai th∆∞·ªùng l√† .xml
                        file_ext = ".xlsx" if download_type == "downloadBke" else ".xml"
                        save_path = os.path.join(temp_dir, file_name + file_ext if not file_name.endswith((".xml", ".xlsx")) else file_name)
                        await download.save_as(save_path)
                        
                        # Ki·ªÉm tra file ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng
                        if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                            logger.info(f"Downloaded {id_tk} ({download_type}) -> {file_name}")
                            successful_downloads.append(item)
                        else:
                            logger.warning(f"Download failed: File not saved or empty for {id_tk}")
                    else:
                        # T·ªù khai ƒë·∫∑c bi·ªát - kh√¥ng c√≥ link <a> download (kh√¥ng c√≥ onclick="downloadTkhai" ho·∫∑c title="T·∫£i t·ªáp")
                        # H√†m downloadTkhai(msgId) d√πng window.location.href ƒë·ªÉ navigate, kh√¥ng trigger download event tr√™n page
                        # N√™n c·∫ßn build URL v√† d√πng new_page.goto() ƒë·ªÉ trigger download event
                        logger.info(f"Special tokhai (no download link) detected: {id_tk}, building download URL")
                        
                        current_ssid = ssid
                        
                        # L·∫•y session ID t·ª´ form
                        if not current_ssid or current_ssid == "NotFound":
                            try:
                                dse_session_input = frame.locator('form[name="traCuuKhaiForm"] input[name="dse_sessionId"], form#traCuuKhaiForm input[name="dse_sessionId"], input[name="dse_sessionId"]').first
                                if await dse_session_input.count() > 0:
                                    current_ssid = await dse_session_input.get_attribute('value') or ""
                                    if current_ssid:
                                        logger.info(f"Retrieved dse_sessionId from form input: {current_ssid[:30]}...")
                            except Exception as e:
                                logger.warning(f"Error getting dse_sessionId from form input: {e}")
                        
                        # L·∫•y t·ª´ frame URL n·∫øu ch∆∞a c√≥
                        if not current_ssid or current_ssid == "NotFound":
                            try:
                                frame_url = frame.url
                                match = re.search(r"[&?]dse_sessionId=([^&]+)", frame_url)
                                if match:
                                    current_ssid = match.group(1)
                                    logger.info(f"Retrieved dse_sessionId from frame URL: {current_ssid[:30]}...")
                            except Exception as e:
                                logger.warning(f"Error getting dse_sessionId from frame URL: {e}")
                        
                        if current_ssid and current_ssid != "NotFound":
                            # L·∫•y processor ID t·ª´ form
                            dse_processor_id = ""
                            try:
                                processor_id_input = frame.locator('form[name="traCuuKhaiForm"] input[name="dse_processorId"], form#traCuuKhaiForm input[name="dse_processorId"], input[name="dse_processorId"]').first
                                if await processor_id_input.count() > 0:
                                    dse_processor_id = await processor_id_input.first.get_attribute('value') or ""
                                    if dse_processor_id:
                                        logger.info(f"Retrieved dse_processorId from form: {dse_processor_id[:30]}...")
                            except:
                                pass
                            
                            # Build URL gi·ªëng nh∆∞ h√†m downloadTkhai() l√†m
                            # downloadTkhai() l√†m: window.location.href='/etaxnnt/Request?dse_sessionId=...&dse_applicationId=-1&dse_operationName=traCuuToKhaiProc&dse_pageId=8&dse_processorState=viewTraCuuTkhai&dse_processorId=...&dse_nextEventName=downTkhai&messageId='+msgId
                            if dse_processor_id:
                                # C√≥ processor ID: d√πng pageId=10 (ho·∫∑c c√≥ th·ªÉ l√† 8 nh∆∞ trong HTML m·∫´u)
                                download_url = f"{BASE_URL}/etaxnnt/Request?dse_sessionId={current_ssid}&dse_applicationId=-1&dse_operationName=traCuuToKhaiProc&dse_pageId=8&dse_processorState=viewTraCuuTkhai&dse_processorId={dse_processor_id}&dse_nextEventName=downTkhai&messageId={id_tk}"
                            else:
                                # Kh√¥ng c√≥ processor ID: d√πng pageId=14
                                download_url = f"{BASE_URL}/etaxnnt/Request?dse_sessionId={current_ssid}&dse_applicationId=-1&dse_operationName=traCuuToKhaiProc&dse_pageId=14&dse_processorState=viewTraCuuTkhai&dse_nextEventName=downTkhai&messageId={id_tk}"
                            
                            logger.info(f"Downloading special (no link) {id_tk} via new_page.goto(): {download_url[:100]}...")
                            
                            new_page = None
                            try:
                                new_page = await session.context.new_page()
                                new_page.set_default_timeout(30000)
                                
                                # Intercept response ƒë·ªÉ b·∫Øt file download
                                download_occurred = False
                                response_data = None
                                
                                async def handle_response(response):
                                    nonlocal download_occurred, response_data
                                    content_type = response.headers.get('content-type', '').lower()
                                    # Ki·ªÉm tra n·∫øu response l√† XML file
                                    if 'xml' in content_type or response.url.endswith('.xml') or 'application/xml' in content_type or 'text/xml' in content_type:
                                        download_occurred = True
                                        response_data = await response.body()
                                        logger.info(f"Got XML response for {id_tk}, size: {len(response_data)} bytes")
                                
                                new_page.on("response", handle_response)
                                
                                # Navigate ƒë·∫øn URL
                                response = await new_page.goto(download_url, wait_until="domcontentloaded", timeout=30000)
                                
                                # Ch·ªù m·ªôt ch√∫t ƒë·ªÉ response ƒë∆∞·ª£c x·ª≠ l√Ω
                                await asyncio.sleep(1)
                                
                                # N·∫øu c√≥ download event, b·∫Øt n√≥
                                if download_occurred and response_data:
                                    save_path = os.path.join(temp_dir, file_name + ".xml" if not file_name.endswith(".xml") else file_name)
                                    with open(save_path, 'wb') as f:
                                        f.write(response_data)
                                    
                                    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                                        logger.info(f"Downloaded special (no link) {id_tk} -> {file_name}")
                                        successful_downloads.append(item)
                                    else:
                                        logger.warning(f"Download failed: File not saved or empty for special {id_tk}")
                                else:
                                    # Fallback: th·ª≠ b·∫Øt download event
                                    try:
                                        async with new_page.expect_download(timeout=5000) as download_info:
                                            # Trigger download b·∫±ng c√°ch click ho·∫∑c navigate l·∫°i
                                            await new_page.reload(wait_until="domcontentloaded")
                                        
                                        download = await download_info.value
                                        save_path = os.path.join(temp_dir, file_name + ".xml" if not file_name.endswith(".xml") else file_name)
                                        await download.save_as(save_path)
                                        
                                        if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                                            logger.info(f"Downloaded special (no link) {id_tk} via download event -> {file_name}")
                                            successful_downloads.append(item)
                                        else:
                                            logger.warning(f"Download failed: File not saved or empty for special {id_tk}")
                                    except:
                                        # N·∫øu kh√¥ng c√≥ download event, th·ª≠ l·∫•y content t·ª´ response
                                        if response:
                                            content = await response.body()
                                            if content and len(content) > 100:  # C√≥ th·ªÉ l√† XML file
                                                save_path = os.path.join(temp_dir, file_name + ".xml" if not file_name.endswith(".xml") else file_name)
                                                with open(save_path, 'wb') as f:
                                                    f.write(content)
                                                
                                                if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                                                    logger.info(f"Downloaded special (no link) {id_tk} from response body -> {file_name}")
                                                    successful_downloads.append(item)
                                                else:
                                                    logger.warning(f"Download failed: File not saved or empty for special {id_tk}")
                                            else:
                                                logger.warning(f"No valid content in response for {id_tk}")
                                        else:
                                            logger.warning(f"No response received for {id_tk}")
                            except Exception as e2:
                                logger.warning(f"Error downloading special {id_tk} via new_page.goto(): {e2}")
                                
                                # Fallback: th·ª≠ d√πng httpx client v·ªõi cookies
                                if session_id:
                                    try:
                                        logger.info(f"Trying httpx fallback for {id_tk}")
                                        http_client = await self._get_http_client(session_id)
                                        if http_client:
                                            response = await http_client.get(download_url, timeout=30.0)
                                            if response.status_code == 200:
                                                content = response.content
                                                if content and len(content) > 100:
                                                    save_path = os.path.join(temp_dir, file_name + ".xml" if not file_name.endswith(".xml") else file_name)
                                                    with open(save_path, 'wb') as f:
                                                        f.write(content)
                                                    
                                                    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                                                        logger.info(f"Downloaded special (no link) {id_tk} via httpx -> {file_name}")
                                                        successful_downloads.append(item)
                                                    else:
                                                        logger.warning(f"httpx download failed: File not saved or empty for {id_tk}")
                                                else:
                                                    logger.warning(f"httpx response has no valid content for {id_tk}")
                                            else:
                                                logger.warning(f"httpx response status {response.status_code} for {id_tk}")
                                    except Exception as e3:
                                        logger.warning(f"httpx fallback also failed for {id_tk}: {e3}")
                                    finally:
                                        if new_page:
                                            try:
                                                await new_page.close()
                                            except:
                                                pass
                        else:
                            logger.warning(f"No valid session ID for special download: {id_tk}. ssid={ssid}")
                except Exception as e:
                    logger.warning(f"Error downloading {item.get('id', 'unknown')}: {e}")
        
        # Download t·∫•t c·∫£ song song (max 5 c√πng l√∫c)
        await asyncio.gather(*[download_one(item) for item in download_queue], return_exceptions=True)
        
        return successful_downloads
    
    async def _download_xml(self, client: httpx.AsyncClient, url: str, temp_dir: str, file_id: str):
        """Download XML file (async)"""
        try:
            response = await client.get(url)
            if response.status_code == 200:
                file_path = os.path.join(temp_dir, f"{file_id}.xml")
                with open(file_path, 'wb') as f:
                    f.write(response.content)
        except Exception as e:
            logger.error(f"Error downloading {file_id}: {e}")
    
    async def _download_single_thongbao(self, session: SessionData, item: Dict, temp_dir: str, max_retries: int = 2) -> bool:
        """
        Download 1 file th√¥ng b√°o v·ªõi retry logic (d√πng Playwright expect_download nh∆∞ c≈©)
        
        Returns:
            True n·∫øu download th√†nh c√¥ng
        """
        page = session.page
        id_tb = item["id"]
        file_name = item.get("file_name", id_tb)
        
        for retry in range(max_retries + 1):
            try:
                # ∆Øu ti√™n d√πng download_link ƒë√£ t√¨m s·∫µn
                download_link = item.get("download_link")
                
                if not download_link:
                    # Fallback: t√¨m l·∫°i t·ª´ cols
                    cols = item.get("cols")
                    col_idx = item.get("col_index", 10)
                    if cols:
                        download_link = cols.nth(col_idx).locator('a:has-text("T·∫£i v·ªÅ")')
                
                if download_link and await download_link.count() > 0:
                    async with page.expect_download(timeout=30000) as download_info:
                        await download_link.first.click()
                    
                    download = await download_info.value
                    save_path = os.path.join(temp_dir, file_name + ".xml" if not file_name.endswith(".xml") else file_name)
                    await download.save_as(save_path)
                    
                    # Verify file exists and has content
                    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                        logger.info(f"Downloaded thongbao {id_tb} -> {file_name}")
                        return True
                    else:
                        raise Exception("File empty or not saved")
                else:
                    logger.warning(f"No download link for thongbao {id_tb}")
                    return False
                    
            except Exception as e:
                logger.warning(f"Error downloading thongbao {id_tb} (attempt {retry + 1}/{max_retries + 1}): {e}")
                if retry < max_retries:
                    await asyncio.sleep(1)  # Wait before retry
        
        return False
    
    async def _download_xml_with_name(self, client: httpx.AsyncClient, url: str, temp_dir: str, file_id: str, file_name: str):
        """Download XML file v·ªõi t√™n file custom (async)"""
        try:
            response = await client.get(url)
            if response.status_code == 200:
                # ƒê·∫£m b·∫£o t√™n file h·ª£p l·ªá
                safe_name = file_name.replace("/", "_").replace("\\", "_").replace(":", "_")
                if not safe_name.endswith(".xml"):
                    safe_name += ".xml"
                file_path = os.path.join(temp_dir, safe_name)
                with open(file_path, 'wb') as f:
                    f.write(response.content)
                return {"id": file_id, "file_name": safe_name, "size": len(response.content)}
        except Exception as e:
            logger.error(f"Error downloading {file_id}: {e}")
            return None
    
    async def crawl_thongbao(
        self,
        session_id: str,
        start_date: str,
        end_date: str,
        job_id: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        session = self.session_manager.get_session(session_id)
        if not session:
            yield {"type": "error", "error": "Session kh√¥ng t·ªìn t·∫°i ho·∫∑c ƒë√£ h·∫øt h·∫°n", "error_code": "SESSION_NOT_FOUND"}
            return
        
        if not session.is_logged_in:
            yield {"type": "error", "error": "Ch∆∞a ƒëƒÉng nh·∫≠p. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "NOT_LOGGED_IN"}
            return
        
        page = session.page
        
        # ‚úÖ FIX: T·∫°o temp directory trong source code thay v√¨ system temp (gi·ªëng t·ªù khai)
        # L·∫•y ƒë∆∞·ªùng d·∫´n project (tool-go-soft)
        current_dir = os.path.dirname(os.path.abspath(__file__))  # .../services/
        services_dir = os.path.dirname(current_dir)  # .../tool-go-soft/
        temp_base_dir = os.path.join(services_dir, "temp")  # .../tool-go-soft/temp/
        os.makedirs(temp_base_dir, exist_ok=True)
        
        # T·∫°o temp directory v·ªõi timestamp ƒë·ªÉ tr√°nh conflict
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        temp_dir = os.path.join(temp_base_dir, f"thongbao_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)
        
        logger.info(f"üìÅ Temp directory for thongbao files: {temp_dir}")  # ‚úÖ Log temp_dir path ƒë·ªÉ d·ªÖ t√¨m file debug
        ssid = session.dse_session_id
        
        try:
            yield {"type": "info", "message": "ƒêang x·ª≠ l√Ω ..."}
            
            # Navigate ƒë·∫øn trang tra c·ª©u th√¥ng b√°o qua connectSSO (gi·ªëng t·ªù khai)
            success = await self._navigate_to_thongbao_page(page, ssid)
            
            if not success:
                yield {"type": "error", "error": "Kh√¥ng th·ªÉ navigate ƒë·∫øn trang tra c·ª©u th√¥ng b√°o. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            # T√¨m frame t·ª´ iframe SSO (gi·ªëng t·ªù khai)
            frame = None
            try:
                frames = page.frames
                for f in frames:
                    if 'thuedientu.gdt.gov.vn' in f.url:
                        frame = f
                        logger.info(f"Found frame for thongbao: {frame.url[:100]}...")
                        break
            except Exception as e:
                logger.warning(f"Error finding frame: {e}")
            
            if not frame:
                yield {"type": "error", "error": "Kh√¥ng t√¨m th·∫•y iframe sau khi navigate. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            # ƒê·ª£i frame load v√† ki·ªÉm tra form th√¥ng b√°o
            try:
                await frame.wait_for_load_state('domcontentloaded', timeout=15000)
                await asyncio.sleep(1)
                await frame.wait_for_selector('#qryFromDate', timeout=15000)
                logger.info("Tra cuu thong bao form loaded successfully")
            except Exception as e:
                logger.warning(f"Frame found but form not found: {e}")
                
                # ‚úÖ Screenshot khi c√≥ l·ªói kh√¥ng t√¨m th·∫•y form
                try:
                    screenshot_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "screenshots", f"thongbao_{session_id[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
                    os.makedirs(screenshot_dir, exist_ok=True)
                    
                    # Screenshot page
                    page_screenshot = os.path.join(screenshot_dir, "01_error_page.png")
                    await page.screenshot(path=page_screenshot, full_page=True)
                    logger.info(f"üì∏ Screenshot page saved: {page_screenshot}")
                    
                    # Screenshot frame
                    try:
                        frame_screenshot = os.path.join(screenshot_dir, "02_error_frame.png")
                        await frame.screenshot(path=frame_screenshot, full_page=True)
                        logger.info(f"üì∏ Screenshot frame saved: {frame_screenshot}")
                    except Exception as frame_e:
                        logger.warning(f"‚ö†Ô∏è Cannot screenshot frame: {frame_e}")
                    
                    # L·∫•y HTML c·ªßa frame ƒë·ªÉ debug
                    try:
                        frame_html = await frame.content()
                        html_file = os.path.join(screenshot_dir, "03_error_frame.html")
                        with open(html_file, 'w', encoding='utf-8') as f:
                            f.write(frame_html)
                        pass
                    except Exception as html_e:
                        pass
                    
                except Exception as screenshot_e:
                    logger.error(f"‚ùå Error taking screenshot: {screenshot_e}")
                
                yield {"type": "error", "error": "Kh√¥ng t√¨m th·∫•y form tra c·ª©u th√¥ng b√°o. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            # Check session timeout
            if await self._check_session_timeout(page):
                yield {
                    "type": "error",
                    "error": "Phi√™n giao d·ªãch h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.",
                    "error_code": "SESSION_EXPIRED"
                }
                return
            
            # Chia kho·∫£ng th·ªùi gian
            date_ranges = self._get_date_ranges(start_date, end_date)
            
            # ‚úÖ T√çNH % THEO C√îNG TH·ª®C M·ªöI: T√≠nh s·ªë ng√†y c·ªßa t·ª´ng kho·∫£ng
            total_days = 0
            range_days = []  # S·ªë ng√†y c·ªßa t·ª´ng kho·∫£ng
            for date_range in date_ranges:
                days = self._calculate_days_between(date_range[0], date_range[1])
                range_days.append(days)
                total_days += days
            
            # T√≠nh % cho m·ªói kho·∫£ng d·ª±a tr√™n s·ªë ng√†y
            range_percentages = []
            for days in range_days:
                if total_days > 0:
                    percent = (days / total_days) * 100
                else:
                    percent = 100.0 if len(date_ranges) == 1 else 0.0
                range_percentages.append(percent)
            
            total_count = 0
            results = []
            files_info = []
            total_size = 0
            accumulated_total_so_far = 0  # T·ªïng s·ªë file ƒë√£ bi·∫øt t·ª´ c√°c kho·∫£ng tr∆∞·ªõc
            accumulated_percent_so_far = 0.0  # % t√≠ch l≈©y t·ª´ c√°c kho·∫£ng tr∆∞·ªõc
            accumulated_downloaded_so_far = 0  # S·ªë file ƒë√£ download t·ª´ c√°c kho·∫£ng tr∆∞·ªõc
            
            yield {"type": "info", "message": f"B·∫Øt ƒë·∫ßu crawl {len(date_ranges)} kho·∫£ng th·ªùi gian..."}
            
            def check_cancelled():
                if not job_id:
                    return False
                try:
                    from shared.redis_client import get_redis_client
                    redis_client = get_redis_client()
                    cancelled = redis_client.get(f"job:{job_id}:cancelled")
                    if cancelled:
                        cancelled = cancelled.decode('utf-8') if isinstance(cancelled, bytes) else str(cancelled).strip()
                        return cancelled == '1'
                    return False
                except Exception as e:
                    return False
            
            for range_idx, date_range in enumerate(date_ranges):
                if check_cancelled():
                    yield {
                        "type": "error",
                        "error": "Job ƒë√£ b·ªã h·ªßy",
                        "error_code": "JOB_CANCELLED"
                    }
                    return
                accumulated_percent_so_far_at_range_start = accumulated_percent_so_far
                # ‚úÖ Gi·ªØ nguy√™n percent hi·ªán t·∫°i khi chuy·ªÉn kho·∫£ng (kh√¥ng reset v·ªÅ 0)
                yield {
                    "type": "progress", 
                    "current": range_idx + 1, 
                    "total": len(date_ranges),
                    "message": f"ƒêang x·ª≠ l√Ω kho·∫£ng {date_range[0]} - {date_range[1]}...",
                    "percent": int(round(accumulated_percent_so_far)),  # ‚úÖ Gi·ªØ nguy√™n percent, kh√¥ng reset v·ªÅ 0
                    "accumulated_percent": int(round(accumulated_percent_so_far)),
                    "accumulated_total": accumulated_total_so_far,  # ‚úÖ T·ªïng t√≠ch l≈©y t·ª´ c√°c kho·∫£ng tr∆∞·ªõc
                    "accumulated_downloaded": accumulated_downloaded_so_far
                }
                
                try:
                    # Nh·∫≠p ng√†y b·∫Øt ƒë·∫ßu - d√πng id qryFromDate theo HTML form
                    start_input = frame.locator('#qryFromDate')
                    await start_input.fill('')
                    await start_input.fill(date_range[0])
                    
                    # Nh·∫≠p ng√†y k·∫øt th√∫c - d√πng id qryToDate theo HTML form
                    end_input = frame.locator('#qryToDate')
                    await end_input.click()
                    await end_input.fill('')
                    await end_input.fill(date_range[1])
                    
                    # Click t√¨m ki·∫øm - button "Tra c·ª©u"
                    search_btn = frame.locator('input[value="Tra c·ª©u"]')
                    await search_btn.click()
                    
                    await asyncio.sleep(2)
                    
                    logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ click search cho kho·∫£ng: {date_range[0]} - {date_range[1]}")
                    
                    # ‚úÖ ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o request ƒë√£ ƒë∆∞·ª£c g·ª≠i
                    await asyncio.sleep(1)
                    
                    # ‚úÖ T√¨m l·∫°i frame m·ªõi sau khi click search (iframe c√≥ th·ªÉ reload khi chuy·ªÉn kho·∫£ng th·ªùi gian)
                    try:
                        frames = page.frames
                        for f in frames:
                            if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                frame = f  # C·∫≠p nh·∫≠t frame object m·ªõi
                                logger.info(f"üîÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ t√¨m l·∫°i frame m·ªõi sau khi click search: {frame.url[:100]}...")
                                break
                    except Exception as refind_frame_e:
                        logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ t√¨m l·∫°i frame m·ªõi sau khi click search: {refind_frame_e}")
                    
                    # ‚úÖ ƒê·ª£i frame load xong tr∆∞·ªõc khi ƒë·ª£i table
                    try:
                        await frame.wait_for_load_state('networkidle', timeout=5000)
                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Frame ƒë√£ load xong (networkidle)")
                    except Exception as frame_load_e:
                        logger.debug(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ƒë·ª£i frame networkidle: {frame_load_e}")
                    
                    # ‚úÖ ƒê·ª£i table load xong ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√£ chuy·ªÉn sang kho·∫£ng m·ªõi
                    try:
                        logger.info(f"‚è≥ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒêang ƒë·ª£i table load sau khi click search...")
                        table_body_check = frame.locator('#allResultTableBody, table.result_table tbody, table#data_content_onday tbody').first
                        await table_body_check.wait_for(timeout=10000, state='visible')
                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Table ƒë√£ load xong sau khi click search")
                        
                        # ‚úÖ ƒê·ª£i th√™m m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c render xong
                        await asyncio.sleep(1.5)
                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ ƒë·ª£i th√™m ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë√£ render xong")
                    except Exception as wait_table_e:
                        logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ƒë·ª£i table load sau khi click search: {wait_table_e}")
                        # Ti·∫øp t·ª•c x·ª≠ l√Ω, s·∫Ω ki·ªÉm tra "Kh√¥ng c√≥ d·ªØ li·ªáu" ·ªü b∆∞·ªõc ti·∫øp theo
                    
                    # X·ª≠ l√Ω ph√¢n trang
                    check_pages = True
                    page_num = 0
                    range_total_records = None  # T·ªïng s·ªë b·∫£n ghi trong kho·∫£ng n√†y (parse t·ª´ currAcc)
                    range_downloaded_so_far = 0  # T·ªïng s·ªë file ƒë√£ download trong kho·∫£ng n√†y (t·ª´ c√°c trang tr∆∞·ªõc)
                    max_pages = 100  # ‚úÖ Gi·ªõi h·∫°n s·ªë trang t·ªëi ƒëa ƒë·ªÉ tr√°nh v√≤ng l·∫∑p v√¥ h·∫°n
                    previous_row_count = 0  # ‚úÖ L∆∞u s·ªë rows c·ªßa trang tr∆∞·ªõc ƒë·ªÉ verify table ƒë√£ chuy·ªÉn trang
                    previous_first_row_id = None  # ‚úÖ L∆∞u m√£ giao d·ªãch c·ªßa row ƒë·∫ßu ti√™n trang tr∆∞·ªõc ƒë·ªÉ verify table ƒë√£ chuy·ªÉn trang
                    while check_pages and page_num < max_pages:
                        # ‚úÖ Check cancelled tr∆∞·ªõc khi x·ª≠ l√Ω trang ti·∫øp theo
                        if check_cancelled():
                            logger.info(f"[THONGBAO] Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                            yield {
                                "type": "error",
                                "error": "Job ƒë√£ b·ªã h·ªßy",
                                "error_code": "JOB_CANCELLED"
                            }
                            return
                        
                        page_num += 1
                        logger.info(f"üìÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒêang x·ª≠ l√Ω trang {page_num}... (check_pages={check_pages})")
                        # T√¨m b·∫£ng k·∫øt qu·∫£ - theo HTML: #allResultTableBody ho·∫∑c table.result_table tbody
                        try:
                            # ‚úÖ TƒÉng timeout v√† th√™m retry ƒë·ªÉ ƒë·∫£m b·∫£o table ƒë∆∞·ª£c load
                            table_body = frame.locator('#allResultTableBody, table.result_table tbody, table#data_content_onday tbody').first
                            await table_body.wait_for(timeout=10000, state='visible')
                        except Exception as e:
                            # ‚úÖ LOG chi ti·∫øt khi kh√¥ng t√¨m th·∫•y table
                            logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng t√¨m th·∫•y b·∫£ng k·∫øt qu·∫£ cho kho·∫£ng {date_range[0]} - {date_range[1]}: {e}")
                            
                            # ‚úÖ Parse currAcc ƒë·ªÉ ki·ªÉm tra xem c√≥ d·ªØ li·ªáu kh√¥ng (ngay c·∫£ khi kh√¥ng c√≥ table)
                            try:
                                curr_acc = frame.locator('#currAcc').first
                                if await curr_acc.count() > 0:
                                    curr_acc_text = await curr_acc.text_content()
                                    import re
                                    match = re.search(r'C√≥\s*<b>(\d+)</b>\s*b·∫£n\s*ghi|C√≥\s*(\d+)\s*b·∫£n\s*ghi', curr_acc_text)
                                    if match:
                                        range_total_records = int(match.group(1) or match.group(2))
                                        logger.info(f"üìä [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Parse t·ªïng s·ªë b·∫£n ghi t·ª´ currAcc (kh√¥ng c√≥ table): {range_total_records}")
                                        # N·∫øu c√≥ s·ªë b·∫£n ghi nh∆∞ng kh√¥ng c√≥ table, c√≥ th·ªÉ l√† l·ªói load trang - RETRY
                                        logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] C√ì {range_total_records} b·∫£n ghi nh∆∞ng kh√¥ng t√¨m th·∫•y table! ƒêang retry...")
                                        # Retry: ƒë·ª£i th√™m v√† th·ª≠ l·∫°i
                                        await asyncio.sleep(3)
                                        try:
                                            table_body = frame.locator('#allResultTableBody, table.result_table tbody, table#data_content_onday tbody').first
                                            await table_body.wait_for(timeout=10000, state='visible')
                                            logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Retry th√†nh c√¥ng, ƒë√£ t√¨m th·∫•y table!")
                                            # Ti·∫øp t·ª•c x·ª≠ l√Ω b√¨nh th∆∞·ªùng (kh√¥ng break)
                                        except Exception as retry_e:
                                            logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Retry v·∫´n th·∫•t b·∫°i: {retry_e}")
                                            # N·∫øu retry v·∫´n th·∫•t b·∫°i, break v√† b·ªè qua kho·∫£ng n√†y
                                            if total_count == 0:
                                                yield {
                                                    "type": "info", 
                                                    "message": f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]} (c√≥ {range_total_records} b·∫£n ghi nh∆∞ng kh√¥ng load ƒë∆∞·ª£c table)",
                                                    "percent": int(round(accumulated_percent_so_far)),
                                                    "accumulated_percent": int(round(accumulated_percent_so_far)),
                                                    "accumulated_total": accumulated_total_so_far,
                                                    "accumulated_downloaded": accumulated_downloaded_so_far
                                                }
                                            accumulated_percent_so_far += range_percentages[range_idx]
                                            break
                            except Exception as parse_e:
                                logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ parse currAcc: {parse_e}")
                            
                            if total_count == 0:
                                # ‚úÖ Gi·ªØ nguy√™n percent hi·ªán t·∫°i khi kh√¥ng c√≥ d·ªØ li·ªáu
                                yield {
                                    "type": "info", 
                                    "message": f"Kh√¥ng c√≥ th√¥ng b√°o trong kho·∫£ng {date_range[0]} - {date_range[1]}",
                                    "percent": int(round(accumulated_percent_so_far)),
                                    "accumulated_percent": int(round(accumulated_percent_so_far)),
                                    "accumulated_total": accumulated_total_so_far,
                                    "accumulated_downloaded": accumulated_downloaded_so_far
                                }
                            # ‚úÖ V·∫´n c·ªông % c·ªßa kho·∫£ng n√†y khi kh√¥ng c√≥ d·ªØ li·ªáu
                            accumulated_percent_so_far += range_percentages[range_idx]
                            break
                        
                        # ‚úÖ Parse t·ªïng s·ªë b·∫£n ghi t·ª´ ph·∫ßn currAcc (ch·ªâ parse ·ªü trang ƒë·∫ßu ti√™n)
                        if page_num == 1:
                            try:
                                curr_acc = frame.locator('#currAcc').first
                                if await curr_acc.count() > 0:
                                    curr_acc_text = await curr_acc.text_content()
                                    # Parse pattern: "C√≥ <b>34</b> b·∫£n ghi" ho·∫∑c "C√≥ 34 b·∫£n ghi"
                                    import re
                                    match = re.search(r'C√≥\s*<b>(\d+)</b>\s*b·∫£n\s*ghi|C√≥\s*(\d+)\s*b·∫£n\s*ghi', curr_acc_text)
                                    if match:
                                        range_total_records = int(match.group(1) or match.group(2))
                                        logger.info(f"üìä [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Parse t·ªïng s·ªë b·∫£n ghi t·ª´ currAcc: {range_total_records}")
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ parse t·ªïng s·ªë b·∫£n ghi t·ª´ currAcc: {e}")
                        
                        rows = table_body.locator('tr')
                        row_count = await rows.count()
                        
                        # ‚úÖ T√≠nh % cho kho·∫£ng n√†y
                        range_percent = range_percentages[range_idx]  # % c·ªßa kho·∫£ng n√†y
                        
                        logger.info(f"üìä [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: T√¨m th·∫•y {row_count} rows, Range %: {range_percent:.2f}%, Accumulated %: {accumulated_percent_so_far:.2f}%")
                        
                        yield {
                            "type": "progress", 
                            "current": total_count, 
                            "message": f"ƒêang x·ª≠ l√Ω {row_count} th√¥ng b√°o (trang hi·ªán t·∫°i)...",
                            "percent": int(round(min(accumulated_percent_so_far, 100))),  # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                            "accumulated_percent": int(round(min(accumulated_percent_so_far, 100))),  # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                            "accumulated_total": accumulated_total_so_far,
                            "accumulated_downloaded": accumulated_downloaded_so_far
                        }
                        
                        download_queue = []
                        page_valid_count = 0
                        range_total_items = 0  # T·ªïng s·ªë items trong kho·∫£ng n√†y
                        
                        for i in range(row_count):
                            try:
                                row = rows.nth(i)
                                cols = row.locator('td')
                                col_count = await cols.count()
                                
                                if col_count < 6:
                                    continue
                                
                                # Theo HTML contentthongbao.html:
                                # C·ªôt 0: STT
                                # C·ªôt 1: CQ th√¥ng b√°o (Ng√¢n h√†ng/C∆° quan thu·∫ø)
                                # C·ªôt 2: M√£ giao d·ªãch
                                # C·ªôt 3: Lo·∫°i th√¥ng b√°o
                                # C·ªôt 4: S·ªë th√¥ng b√°o
                                # C·ªôt 5: Ng√†y th√¥ng b√°o
                                # C·ªôt 6-9: S·ªë GNT, M√£ hi·ªáu ch·ª©ng t·ª´, S·ªë ch·ª©ng t·ª´, Ng√†y n·ªôp thu·∫ø
                                # C·ªôt 10: Chi ti·∫øt | T·∫£i v·ªÅ
                                
                                # C·ªôt 2: M√£ giao d·ªãch
                                ma_giao_dich = await cols.nth(2).text_content()
                                ma_giao_dich = ma_giao_dich.strip() if ma_giao_dich else ""
                                
                                if not ma_giao_dich or len(ma_giao_dich) < 5:
                                    continue
                                
                                # Ch·ªâ ƒë·∫øm khi item h·ª£p l·ªá
                                page_valid_count += 1
                                
                                # C·ªôt 1: CQ th√¥ng b√°o
                                cq_thong_bao = await cols.nth(1).text_content()
                                cq_thong_bao = cq_thong_bao.strip() if cq_thong_bao else ""
                                
                                # C·ªôt 3: Lo·∫°i th√¥ng b√°o
                                loai_thong_bao = await cols.nth(3).text_content()
                                loai_thong_bao = loai_thong_bao.strip() if loai_thong_bao else ""
                                
                                # C·ªôt 4: S·ªë th√¥ng b√°o
                                so_thong_bao = await cols.nth(4).text_content()
                                so_thong_bao = so_thong_bao.strip() if so_thong_bao else ""
                                
                                # C·ªôt 5: Ng√†y th√¥ng b√°o
                                ngay_thong_bao = await cols.nth(5).text_content()
                                ngay_thong_bao = ngay_thong_bao.strip() if ngay_thong_bao else ""
                                
                                result = {
                                    "id": ma_giao_dich,
                                    "ma_giao_dich": ma_giao_dich,
                                    "cq_thong_bao": cq_thong_bao,
                                    "loai_thong_bao": loai_thong_bao,
                                    "so_thong_bao": so_thong_bao,
                                    "ngay_thong_bao": ngay_thong_bao,
                                    "type": "thongbao"
                                }
                                results.append(result)
                                
                                # ‚úÖ Th√™m accumulated fields v√†o event item ƒë·ªÉ frontend hi·ªÉn th·ªã ƒë√∫ng
                                yield {
                                    "type": "item", 
                                    "data": result,
                                    "accumulated_total": accumulated_total_so_far,
                                    "accumulated_downloaded": accumulated_downloaded_so_far,
                                    "accumulated_percent": int(round(min(accumulated_percent_so_far, 100)))
                                }
                                
                                # T√¨m link "T·∫£i v·ªÅ" trong c√°c c·ªôt
                                # Th·ª≠ t√¨m trong c·ªôt cu·ªëi c√πng tr∆∞·ªõc, sau ƒë√≥ t√¨m trong t·∫•t c·∫£ c√°c c·ªôt
                                download_link_found = None
                                download_col_index = None
                                
                                # C√°ch 1: T√¨m trong c·ªôt cu·ªëi c√πng (th∆∞·ªùng l√† c·ªôt 10 ho·∫∑c cu·ªëi c√πng)
                                last_col_index = col_count - 1
                                if last_col_index >= 0:
                                    last_col = cols.nth(last_col_index)
                                    download_link = last_col.locator('a:has-text("T·∫£i v·ªÅ"), a[title*="T·∫£i"], a[href*="download"]')
                                    if await download_link.count() > 0:
                                        download_link_found = download_link
                                        download_col_index = last_col_index
                                
                                # C√°ch 2: N·∫øu kh√¥ng t√¨m th·∫•y, t√¨m trong t·∫•t c·∫£ c√°c c·ªôt
                                if not download_link_found:
                                    for col_idx in range(col_count - 1, -1, -1):  # T√¨m t·ª´ cu·ªëi l√™n ƒë·∫ßu
                                        col = cols.nth(col_idx)
                                        download_link = col.locator('a:has-text("T·∫£i v·ªÅ"), a:has-text("T·∫£i"), a[title*="T·∫£i"], a[href*="download"]')
                                        if await download_link.count() > 0:
                                            download_link_found = download_link
                                            download_col_index = col_idx
                                            break
                                
                                if download_link_found:
                                    # T·∫°o t√™n file t·ª´ th√¥ng tin th√¥ng b√°o
                                    ngay_clean = ngay_thong_bao.replace("/", "-").replace(":", "-").replace(" ", "_")
                                    file_name = f"{ma_giao_dich} - {loai_thong_bao[:40]} - {ngay_clean}"
                                    file_name = self._remove_accents(file_name)
                                    file_name = file_name.replace("/", "_").replace(":", "_").replace("\\", "_")
                                    
                                    download_queue.append({
                                        "id": ma_giao_dich,
                                        "loai_thong_bao": loai_thong_bao,
                                        "ngay_thong_bao": ngay_thong_bao,
                                        "file_name": file_name,
                                        "download_link": download_link_found,
                                        "cols": cols,
                                        "col_index": download_col_index
                                    })
                                else:
                                    logger.debug(f"Kh√¥ng t√¨m th·∫•y link download cho th√¥ng b√°o {ma_giao_dich}, c√≥ {col_count} c·ªôt")
                            
                            except Exception as e:
                                logger.error(f"Error processing row: {e}")
                                continue
                        
                        # C·ªông s·ªë items h·ª£p l·ªá v√†o range_total_items
                        range_total_items += page_valid_count
                        
                        logger.info(f"üìã [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: C√≥ {page_valid_count} items h·ª£p l·ªá, {len(download_queue)} items c√≥ link download")
                        
                        # Download t·ª´ng file v√† yield progress
                        if download_queue:
                            queue_total = len(download_queue)
                            
                            # ‚úÖ T√≠nh % cho m·ªói file download
                            # N·∫øu c√≥ range_total_records, d√πng n√≥ ƒë·ªÉ t√≠nh % ch√≠nh x√°c (cho t·∫•t c·∫£ c√°c trang)
                            if range_total_records:
                                # T√≠nh % d·ª±a tr√™n t·ªïng s·ªë b·∫£n ghi trong kho·∫£ng (d√πng cho t·∫•t c·∫£ c√°c trang)
                                percent_per_file = range_percent / range_total_records
                                logger.info(f"üìä [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: D√πng range_total_records={range_total_records} ƒë·ªÉ t√≠nh % per file: {percent_per_file:.4f}%")
                            elif queue_total > 0:
                                # N·∫øu kh√¥ng c√≥ range_total_records, t√≠nh % d·ª±a tr√™n s·ªë file tr√™n trang hi·ªán t·∫°i
                                percent_per_file = range_percent / queue_total
                            else:
                                percent_per_file = 0.0
                            
                            # ‚úÖ C·∫≠p nh·∫≠t accumulated_total khi bi·∫øt s·ªë file c·∫ßn download
                            # N·∫øu c√≥ range_total_records v√† ƒëang ·ªü trang ƒë·∫ßu, d√πng n√≥ ƒë·ªÉ c·∫≠p nh·∫≠t accumulated_total
                            if range_total_records and page_num == 1:
                                # Ch·ªâ c·∫≠p nh·∫≠t accumulated_total ·ªü trang ƒë·∫ßu ti√™n v·ªõi t·ªïng s·ªë b·∫£n ghi
                                accumulated_total_so_far += range_total_records
                                logger.info(f"üìä [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] C·∫≠p nh·∫≠t accumulated_total v·ªõi range_total_records={range_total_records}, accumulated_total_so_far={accumulated_total_so_far}")
                            elif not range_total_records:
                                # N·∫øu kh√¥ng c√≥ range_total_records, c·ªông s·ªë file tr√™n trang hi·ªán t·∫°i
                                accumulated_total_so_far += queue_total
                            
                            # Hi·ªÉn th·ªã t·ªïng s·ªë file s·∫Ω t·∫£i (d√πng range_total_records n·∫øu c√≥, n·∫øu kh√¥ng d√πng queue_total)
                            display_total = range_total_records if range_total_records else queue_total
                            
                            logger.info(f"‚¨áÔ∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: B·∫Øt ƒë·∫ßu download {queue_total} files (t·ªïng kho·∫£ng: {display_total}), Range %: {range_percent:.2f}%, Percent per file: {percent_per_file:.4f}%, Accumulated total: {accumulated_total_so_far}, Accumulated %: {accumulated_percent_so_far:.2f}%")
                            
                            # ‚úÖ CH·ªà publish download_start khi b·∫Øt ƒë·∫ßu kho·∫£ng m·ªõi (trang 1), kh√¥ng publish khi chuy·ªÉn trang
                            if page_num == 1:
                                yield {
                                    "type": "download_start",
                                    "total_to_download": display_total,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file s·∫Ω t·∫£i trong kho·∫£ng
                                    "current_page_download": queue_total,  # S·ªë file tr√™n trang hi·ªán t·∫°i
                                    "date_range": f"{date_range[0]} - {date_range[1]}",
                                    "range_index": range_idx + 1,
                                    "total_ranges": len(date_ranges),
                                    "accumulated_total": accumulated_total_so_far,
                                    "accumulated_downloaded": accumulated_downloaded_so_far,
                                    "range_percent": range_percent,  # % c·ªßa kho·∫£ng n√†y
                                    "accumulated_percent": int(round(min(accumulated_percent_so_far, 100))),  # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                                    "message": f"B·∫Øt ƒë·∫ßu t·∫£i {display_total} th√¥ng b√°o trong kho·∫£ng {date_range[0]} - {date_range[1]}..."
                                }
                            
                            downloaded = 0
                            
                            for item_idx, item in enumerate(download_queue, 1):
                                try:
                                    logger.info(f"üì• [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang download file {item_idx}/{queue_total}: {item.get('id', 'N/A')}...")
                                    success = await self._download_single_thongbao(session, item, temp_dir)
                                    if success:
                                        downloaded += 1
                                        accumulated_downloaded_so_far += 1
                                        range_downloaded_so_far += 1  # ‚úÖ C·ªông d·ªìn s·ªë file ƒë√£ download trong kho·∫£ng n√†y
                                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ download th√†nh c√¥ng file {item_idx}/{queue_total}: {item.get('id', 'N/A')}")
                                    else:
                                        logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Download th·∫•t b·∫°i file {item_idx}/{queue_total}: {item.get('id', 'N/A')}")
                                except Exception as download_e:
                                    logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi download file {item_idx}/{queue_total} ({item.get('id', 'N/A')}): {download_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    # Ti·∫øp t·ª•c download file ti·∫øp theo
                                    continue
                                
                                # ‚úÖ T√≠nh % t√≠ch l≈©y: % t·ª´ c√°c kho·∫£ng tr∆∞·ªõc + % c·ªßa c√°c file ƒë√£ download trong kho·∫£ng n√†y
                                # QUAN TR·ªåNG: D√πng accumulated_percent_so_far_at_range_start (kh√¥ng ph·∫£i accumulated_percent_so_far)
                                # ƒë·ªÉ tr√°nh c·ªông d·ªìn sai khi ƒë√£ c·∫≠p nh·∫≠t accumulated_percent_so_far trong v√≤ng l·∫∑p
                                if range_total_records:
                                    # T√≠nh % d·ª±a tr√™n t·ªïng s·ªë b·∫£n ghi trong kho·∫£ng
                                    # % c·ªßa kho·∫£ng n√†y = (s·ªë file ƒë√£ download / t·ªïng s·ªë file trong kho·∫£ng) * % c·ªßa kho·∫£ng
                                    range_accumulated_percent = (range_downloaded_so_far / range_total_records) * range_percent
                                    # C·ªông v·ªõi % t√≠ch l≈©y t·ª´ c√°c kho·∫£ng tr∆∞·ªõc (t·∫°i th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu kho·∫£ng n√†y)
                                    current_accumulated_percent = accumulated_percent_so_far_at_range_start + range_accumulated_percent
                                else:
                                    # T√≠nh % d·ª±a tr√™n s·ªë file tr√™n trang hi·ªán t·∫°i
                                    current_accumulated_percent = accumulated_percent_so_far_at_range_start + (downloaded * percent_per_file)
                                
                                # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                                current_accumulated_percent = min(current_accumulated_percent, 100.0)
                                
                                # ‚úÖ C·∫¨P NH·∫¨T accumulated_percent_so_far li√™n t·ª•c trong qu√° tr√¨nh download
                                accumulated_percent_so_far = current_accumulated_percent
                                
                                # Hi·ªÉn th·ªã t·ªïng s·ªë file ƒë√£ download trong kho·∫£ng (d√πng range_total_records n·∫øu c√≥)
                                display_total = range_total_records if range_total_records else queue_total
                                display_downloaded = range_downloaded_so_far if range_total_records else downloaded
                                
                                if item_idx % 5 == 0 or item_idx == queue_total:  # Log m·ªói 5 file ho·∫∑c file cu·ªëi
                                    logger.info(f"‚¨áÔ∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ download {display_downloaded}/{display_total} files (trang: {downloaded}/{queue_total}), Current accumulated %: {accumulated_percent_so_far:.2f}%")
                                
                                # ‚úÖ Yield progress event v·ªõi exception handling
                                try:
                                    yield {
                                        "type": "download_progress",
                                        "downloaded": display_downloaded,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file ƒë√£ download trong kho·∫£ng
                                        "total": display_total,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file s·∫Ω t·∫£i trong kho·∫£ng
                                        "current_page_downloaded": downloaded,  # S·ªë file ƒë√£ download tr√™n trang hi·ªán t·∫°i
                                        "current_page_total": queue_total,  # S·ªë file tr√™n trang hi·ªán t·∫°i
                                        "percent": round(display_downloaded / display_total * 100, 1) if display_total > 0 else 0,
                                        "current_item": item.get("id", ""),
                                        "accumulated_total": accumulated_total_so_far,
                                        "accumulated_downloaded": accumulated_downloaded_so_far,
                                        "accumulated_percent": int(round(accumulated_percent_so_far)),  # ‚úÖ D√πng accumulated_percent_so_far ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t
                                        "message": f"ƒê√£ t·∫£i {display_downloaded}/{display_total} ({round(display_downloaded / display_total * 100, 1) if display_total > 0 else 0}%)"
                                    }
                                except Exception as yield_e:
                                    logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi yield progress event: {yield_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    # Ti·∫øp t·ª•c download file ti·∫øp theo, kh√¥ng d·ª´ng v√¨ l·ªói yield
                                    pass
                            
                            # ‚úÖ C·∫≠p nh·∫≠t previous_row_count sau khi x·ª≠ l√Ω xong trang n√†y
                            previous_row_count = row_count
                            
                            # ‚úÖ C·∫≠p nh·∫≠t accumulated_percent_so_far sau khi download xong kho·∫£ng n√†y (ch·ªâ ·ªü trang cu·ªëi c√πng)
                            # Ch·ªâ c·∫≠p nh·∫≠t khi kh√¥ng c√≤n trang ti·∫øp theo v√† ƒë√£ download h·∫øt t·∫•t c·∫£ file trong kho·∫£ng
                            if not check_pages:  # N·∫øu kh√¥ng c√≤n trang ti·∫øp theo
                                # ƒê·∫£m b·∫£o accumulated_percent_so_far ƒë·∫°t ƒë√∫ng % c·ªßa kho·∫£ng n√†y
                                # N·∫øu c√≥ range_total_records, ƒë√£ t√≠nh % d·ª±a tr√™n s·ªë file download, kh√¥ng c·∫ßn c·ªông th√™m
                                # N·∫øu kh√¥ng c√≥ range_total_records, c·ªông % c·ªßa kho·∫£ng n√†y
                                if not range_total_records:
                                    accumulated_percent_so_far += range_percent
                                # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                                accumulated_percent_so_far = min(accumulated_percent_so_far, 100.0)
                            
                            # Hi·ªÉn th·ªã t·ªïng s·ªë file ƒë√£ download trong kho·∫£ng (d√πng range_total_records n·∫øu c√≥)
                            display_total = range_total_records if range_total_records else queue_total
                            display_downloaded = range_downloaded_so_far if range_total_records else downloaded
                            
                            logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Ho√†n th√†nh download {display_downloaded}/{display_total} files (trang: {downloaded}/{queue_total}), Accumulated %: {accumulated_percent_so_far:.2f}%")
                            
                            yield {
                                "type": "download_complete",
                                "downloaded": display_downloaded,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file ƒë√£ download trong kho·∫£ng
                                "total": display_total,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file s·∫Ω t·∫£i trong kho·∫£ng
                                "current_page_downloaded": downloaded,  # S·ªë file ƒë√£ download tr√™n trang hi·ªán t·∫°i
                                "current_page_total": queue_total,  # S·ªë file tr√™n trang hi·ªán t·∫°i
                                "accumulated_total": accumulated_total_so_far,
                                "accumulated_downloaded": accumulated_downloaded_so_far,
                                "accumulated_percent": int(round(accumulated_percent_so_far)),  # ‚úÖ ƒê√£ ƒë·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                                # ‚úÖ KH√îNG g·ª≠i message ƒë·ªÉ frontend kh√¥ng hi·ªÉn th·ªã "Ho√†n th√†nh t·∫£i..."
                                # "message": f"Ho√†n th√†nh t·∫£i {display_downloaded}/{display_total} th√¥ng b√°o"
                            }
                        
                        # Ch·ªâ c·ªông s·ªë items h·ª£p l·ªá v√†o total_count
                        total_count += page_valid_count
                        
                        # Check pagination - next page
                        try:
                            logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang ki·ªÉm tra n√∫t next...")
                            next_btn = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                            next_btn_count = await next_btn.count()
                            logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: S·ªë l∆∞·ª£ng n√∫t next: {next_btn_count}")
                            
                            if next_btn_count > 0:
                                logger.info(f"‚û°Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: C√≥ trang ti·∫øp theo, ƒëang chuy·ªÉn trang...")
                                
                                # ‚úÖ Click v·ªõi timeout v√† logging
                                try:
                                    logger.info(f"üñ±Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang click n√∫t next...")
                                    await asyncio.wait_for(next_btn.click(), timeout=10.0)
                                    logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ click n√∫t next th√†nh c√¥ng")
                                except asyncio.TimeoutError:
                                    logger.error(f"‚è±Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Timeout khi click n√∫t next (10s)")
                                    check_pages = False
                                    continue
                                except Exception as click_e:
                                    logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi click n√∫t next: {click_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    check_pages = False
                                    continue
                                
                                # ‚úÖ ƒê·ª£i trang load xong tr∆∞·ªõc khi ti·∫øp t·ª•c
                                logger.info(f"‚è≥ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê·ª£i 2 gi√¢y sau khi click...")
                                await asyncio.sleep(2)
                                logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ ƒë·ª£i xong 2 gi√¢y, b·∫Øt ƒë·∫ßu ƒë·ª£i table load...")
                                
                                # ‚úÖ T√¨m l·∫°i frame m·ªõi sau khi click next (iframe c√≥ th·ªÉ reload khi chuy·ªÉn trang)
                                try:
                                    frames = page.frames
                                    for f in frames:
                                        if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                            frame = f  # C·∫≠p nh·∫≠t frame object m·ªõi
                                            logger.info(f"üîÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ t√¨m l·∫°i frame m·ªõi sau khi click next: {frame.url[:100]}...")
                                            break
                                except Exception as refind_frame_e:
                                    logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ t√¨m l·∫°i frame m·ªõi sau khi click next: {refind_frame_e}")
                                
                                # ‚úÖ Ki·ªÉm tra l·∫°i xem c√≥ trang ti·∫øp theo kh√¥ng (sau khi click)
                                try:
                                    logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang ƒë·ª£i table load cho trang {page_num + 1}...")
                                    # ‚úÖ Ki·ªÉm tra frame c√≤n t·ªìn t·∫°i kh√¥ng
                                    try:
                                        frame_url = frame.url
                                        logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Frame URL: {frame_url[:100]}...")
                                    except Exception as frame_check_e:
                                        logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Frame kh√¥ng c√≤n t·ªìn t·∫°i sau khi click: {frame_check_e}")
                                        import traceback
                                        logger.error(f"Traceback: {traceback.format_exc()}")
                                        check_pages = False
                                        continue
                                    
                                    # ƒê·ª£i table load ƒë·ªÉ ƒë·∫£m b·∫£o trang ƒë√£ chuy·ªÉn (tƒÉng timeout l√™n 15 gi√¢y)
                                    logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang t√¨m table locator...")
                                    try:
                                        table_body_check = frame.locator('#allResultTableBody, table.result_table tbody, table#data_content_onday tbody').first
                                        logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ t√¨m th·∫•y table locator, ƒëang ƒë·ª£i table visible...")
                                    except Exception as locator_e:
                                        logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] L·ªói khi t√¨m table locator: {locator_e}")
                                        import traceback
                                        logger.error(f"Traceback: {traceback.format_exc()}")
                                        check_pages = False
                                        continue
                                    
                                    try:
                                        await asyncio.wait_for(
                                            table_body_check.wait_for(timeout=15000, state='visible'),
                                            timeout=20.0  # T·ªïng timeout 20 gi√¢y
                                        )
                                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Table ƒë√£ visible, ƒëang verify table ƒë√£ chuy·ªÉn trang...")
                                        
                                        # ‚úÖ ƒê·ª£i frame load xong tr∆∞·ªõc khi verify table
                                        try:
                                            await frame.wait_for_load_state('networkidle', timeout=5000)
                                            logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Frame ƒë√£ load xong (networkidle) sau khi click next")
                                        except Exception as frame_load_e:
                                            logger.debug(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ƒë·ª£i frame networkidle: {frame_load_e}")
                                        
                                        # ‚úÖ ƒê·ª£i th√™m m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o table ƒë√£ load xong v√† render ƒë√∫ng
                                        await asyncio.sleep(1.5)
                                        
                                        # ‚úÖ Verify table ƒë√£ th·ª±c s·ª± chuy·ªÉn trang b·∫±ng c√°ch so s√°nh m√£ giao d·ªãch c·ªßa row ƒë·∫ßu ti√™n
                                        try:
                                            rows_check = table_body_check.locator('tr')
                                            row_count_check = await rows_check.count()
                                            logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Verify: Table c√≥ {row_count_check} rows sau khi click next (trang tr∆∞·ªõc: {previous_row_count} rows)")
                                            
                                            # ‚úÖ L·∫•y m√£ giao d·ªãch c·ªßa row ƒë·∫ßu ti√™n ƒë·ªÉ verify
                                            first_row_id = None
                                            if row_count_check > 0:
                                                try:
                                                    first_row = rows_check.first
                                                    first_cols = first_row.locator('td')
                                                    col_count = await first_cols.count()
                                                    if col_count > 2:
                                                        # M√£ giao d·ªãch ·ªü c·ªôt 2 (theo HTML structure)
                                                        first_row_id = await first_cols.nth(2).text_content()
                                                        first_row_id = first_row_id.strip() if first_row_id else None
                                                        logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] M√£ giao d·ªãch row ƒë·∫ßu ti√™n sau click next: {first_row_id}")
                                                except Exception as get_id_e:
                                                    logger.debug(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ l·∫•y m√£ giao d·ªãch row ƒë·∫ßu ti√™n: {get_id_e}")
                                            
                                            # N·∫øu table v·∫´n c√≥ c√πng s·ªë rows nh∆∞ trang tr∆∞·ªõc, ki·ªÉm tra m√£ giao d·ªãch
                                            if row_count_check == previous_row_count and previous_row_count > 0:
                                                logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Table v·∫´n c√≥ {row_count_check} rows gi·ªëng trang tr∆∞·ªõc, ƒë·ª£i th√™m v√† ki·ªÉm tra m√£ giao d·ªãch...")
                                                await asyncio.sleep(2)
                                                
                                                # L·∫•y l·∫°i m√£ giao d·ªãch sau khi ƒë·ª£i
                                                first_row_id_after_wait = None
                                                if row_count_check > 0:
                                                    try:
                                                        first_row_after = rows_check.first
                                                        first_cols_after = first_row_after.locator('td')
                                                        col_count_after = await first_cols_after.count()
                                                        if col_count_after > 2:
                                                            first_row_id_after_wait = await first_cols_after.nth(2).text_content()
                                                            first_row_id_after_wait = first_row_id_after_wait.strip() if first_row_id_after_wait else None
                                                    except Exception as get_id_e2:
                                                        logger.debug(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ l·∫•y m√£ giao d·ªãch sau khi ƒë·ª£i: {get_id_e2}")
                                                
                                                row_count_check = await rows_check.count()
                                                logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Sau khi ƒë·ª£i th√™m: Table c√≥ {row_count_check} rows, m√£ giao d·ªãch: {first_row_id_after_wait}")
                                                
                                                # ‚úÖ N·∫øu s·ªë rows v·∫´n gi·ªëng, ki·ªÉm tra m√£ giao d·ªãch
                                                if row_count_check == previous_row_count:
                                                    # So s√°nh v·ªõi m√£ giao d·ªãch c·ªßa trang tr∆∞·ªõc
                                                    if previous_first_row_id and first_row_id_after_wait:
                                                        if previous_first_row_id == first_row_id_after_wait:
                                                            logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Table v·∫´n ch∆∞a chuy·ªÉn trang sau khi click next! (M√£ giao d·ªãch gi·ªëng nhau: {previous_first_row_id} == {first_row_id_after_wait})")
                                                            check_pages = False
                                                            continue
                                                        else:
                                                            logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Table ƒë√£ chuy·ªÉn trang (m√£ giao d·ªãch kh√°c: {previous_first_row_id} ‚Üí {first_row_id_after_wait})")
                                                    elif not previous_first_row_id or not first_row_id_after_wait:
                                                        # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c m√£ giao d·ªãch, ch·ªâ d·ª±a v√†o s·ªë rows
                                                        logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ verify b·∫±ng m√£ giao d·ªãch, nh∆∞ng s·ªë rows v·∫´n gi·ªëng, ti·∫øp t·ª•c th·ª≠...")
                                                        # Ti·∫øp t·ª•c x·ª≠ l√Ω, c√≥ th·ªÉ table ƒë√£ chuy·ªÉn nh∆∞ng kh√¥ng verify ƒë∆∞·ª£c
                                            else:
                                                # S·ªë rows kh√°c nhau ‚Üí table ƒë√£ chuy·ªÉn trang
                                                logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Table ƒë√£ chuy·ªÉn trang (s·ªë rows kh√°c: {previous_row_count} ‚Üí {row_count_check})")
                                                
                                        except Exception as verify_e:
                                            logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ verify table: {verify_e}")
                                        
                                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num + 1} ƒë√£ load xong, ti·∫øp t·ª•c x·ª≠ l√Ω...")
                                        
                                        # ‚úÖ C·∫≠p nh·∫≠t previous_row_count v√† previous_first_row_id cho l·∫ßn verify ti·∫øp theo
                                        previous_row_count = row_count_check
                                        previous_first_row_id = first_row_id_after_wait if first_row_id_after_wait else first_row_id
                                        
                                        # ‚úÖ Ti·∫øp t·ª•c v√≤ng l·∫∑p (check_pages v·∫´n True)
                                    except Exception as wait_table_e:
                                        logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] L·ªói khi ƒë·ª£i table visible: {wait_table_e}")
                                        import traceback
                                        logger.error(f"Traceback: {traceback.format_exc()}")
                                        # Re-raise ƒë·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi except block b√™n ngo√†i
                                        raise
                                except asyncio.TimeoutError:
                                    logger.error(f"‚è±Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Timeout khi ƒë·ª£i table load cho trang {page_num + 1} (20s)")
                                    # ‚úÖ Retry: ƒê·ª£i th√™m v√† th·ª≠ l·∫°i
                                    logger.info(f"üîÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Retry ƒë·ª£i table load...")
                                    await asyncio.sleep(3)
                                    try:
                                        table_body_check_retry = frame.locator('#allResultTableBody, table.result_table tbody, table#data_content_onday tbody').first
                                        await asyncio.wait_for(
                                            table_body_check_retry.wait_for(timeout=15000, state='visible'),
                                            timeout=20.0
                                        )
                                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Retry th√†nh c√¥ng, trang {page_num + 1} ƒë√£ load xong")
                                        
                                        # ‚úÖ T√¨m l·∫°i frame m·ªõi sau khi retry (iframe c√≥ th·ªÉ reload)
                                        try:
                                            frames = page.frames
                                            for f in frames:
                                                if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                                    frame = f  # C·∫≠p nh·∫≠t frame object m·ªõi
                                                    logger.info(f"üîÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ t√¨m l·∫°i frame m·ªõi sau retry: {frame.url[:100]}...")
                                                    break
                                        except Exception as refind_frame_e:
                                            logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ t√¨m l·∫°i frame m·ªõi sau retry: {refind_frame_e}")
                                    except Exception as retry_e:
                                        logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Retry v·∫´n th·∫•t b·∫°i: {retry_e}")
                                        import traceback
                                        logger.error(f"Traceback: {traceback.format_exc()}")
                                        # Ki·ªÉm tra l·∫°i n√∫t next sau khi ƒë·ª£i
                                        await asyncio.sleep(2)
                                        try:
                                            next_btn_check = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                                            next_btn_check_count = await next_btn_check.count()
                                            logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Sau retry, s·ªë l∆∞·ª£ng n√∫t next: {next_btn_check_count}")
                                            if next_btn_check_count == 0:
                                                logger.info(f"üèÅ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Sau khi click, kh√¥ng c√≤n n√∫t next, k·∫øt th√∫c ph√¢n trang")
                                                check_pages = False
                                            else:
                                                logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] V·∫´n c√≤n n√∫t next nh∆∞ng table kh√¥ng load, k·∫øt th√∫c ph√¢n trang ƒë·ªÉ tr√°nh hang")
                                                check_pages = False
                                        except Exception as check_e:
                                            logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] L·ªói khi ki·ªÉm tra n√∫t next sau retry: {check_e}")
                                            check_pages = False
                                except Exception as wait_e:
                                    logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num + 1} ch∆∞a load xong sau khi click next: {wait_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    # ‚úÖ Retry: ƒê·ª£i th√™m v√† th·ª≠ l·∫°i
                                    logger.info(f"üîÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Retry ƒë·ª£i table load...")
                                    await asyncio.sleep(3)
                                    try:
                                        table_body_check_retry = frame.locator('#allResultTableBody, table.result_table tbody, table#data_content_onday tbody').first
                                        await asyncio.wait_for(
                                            table_body_check_retry.wait_for(timeout=15000, state='visible'),
                                            timeout=20.0
                                        )
                                        logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Retry th√†nh c√¥ng, trang {page_num + 1} ƒë√£ load xong")
                                        
                                        # ‚úÖ T√¨m l·∫°i frame m·ªõi sau khi retry (iframe c√≥ th·ªÉ reload)
                                        try:
                                            frames = page.frames
                                            for f in frames:
                                                if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                                    frame = f  # C·∫≠p nh·∫≠t frame object m·ªõi
                                                    logger.info(f"üîÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ t√¨m l·∫°i frame m·ªõi sau retry: {frame.url[:100]}...")
                                                    break
                                        except Exception as refind_frame_e:
                                            logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ t√¨m l·∫°i frame m·ªõi sau retry: {refind_frame_e}")
                                    except Exception as retry_e:
                                        logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Retry v·∫´n th·∫•t b·∫°i: {retry_e}")
                                        import traceback
                                        logger.error(f"Traceback: {traceback.format_exc()}")
                                        
                                        # ‚úÖ Ki·ªÉm tra "Kh√¥ng c√≥ d·ªØ li·ªáu" khi table kh√¥ng load
                                        try:
                                            no_data_text = frame.locator('div:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), strong:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), div.align-center:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu")').first
                                            if await no_data_text.count() > 0:
                                                no_data_content = await no_data_text.text_content()
                                                if "Kh√¥ng c√≥ d·ªØ li·ªáu" in (no_data_content or ""):
                                                    logger.info(f"üìä [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Ph√°t hi·ªán 'Kh√¥ng c√≥ d·ªØ li·ªáu' sau retry, d·ª´ng pagination")
                                                    check_pages = False
                                                    continue
                                        except Exception as no_data_check_e3:
                                            logger.debug(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ki·ªÉm tra 'Kh√¥ng c√≥ d·ªØ li·ªáu' sau retry: {no_data_check_e3}")
                                        
                                        # Ki·ªÉm tra l·∫°i n√∫t next sau khi ƒë·ª£i
                                        await asyncio.sleep(2)
                                        try:
                                            next_btn_check = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                                            next_btn_check_count = await next_btn_check.count()
                                            logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Sau retry, s·ªë l∆∞·ª£ng n√∫t next: {next_btn_check_count}")
                                            if next_btn_check_count == 0:
                                                logger.info(f"üèÅ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Sau khi click, kh√¥ng c√≤n n√∫t next, k·∫øt th√∫c ph√¢n trang")
                                                check_pages = False
                                                continue
                                            else:
                                                logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] V·∫´n c√≤n n√∫t next nh∆∞ng table kh√¥ng load, k·∫øt th√∫c ph√¢n trang ƒë·ªÉ tr√°nh hang")
                                                check_pages = False
                                                continue
                                        except Exception as check_e:
                                            logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] L·ªói khi ki·ªÉm tra n√∫t next sau retry: {check_e}")
                                            check_pages = False
                                            continue
                            else:
                                logger.info(f"üèÅ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Kh√¥ng c√≤n trang ti·∫øp theo")
                                check_pages = False  # ‚úÖ CH·ªà set False khi kh√¥ng c√≤n n√∫t next
                            
                            # ‚úÖ Log tr·∫°ng th√°i sau khi x·ª≠ l√Ω pagination
                            logger.info(f"üìä [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Sau khi x·ª≠ l√Ω pagination: check_pages={check_pages}, page_num={page_num}")
                        except Exception as pagination_e:
                            logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi x·ª≠ l√Ω ph√¢n trang: {pagination_e}")
                            import traceback
                            logger.error(f"Traceback: {traceback.format_exc()}")
                            # ‚úÖ Sau khi c√≥ l·ªói, ki·ªÉm tra l·∫°i xem c√≥ n√∫t next kh√¥ng
                            try:
                                await asyncio.sleep(2)
                                next_btn_retry = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                                next_btn_retry_count = await next_btn_retry.count()
                                logger.info(f"üîç [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Sau l·ªói, s·ªë l∆∞·ª£ng n√∫t next: {next_btn_retry_count}")
                                if next_btn_retry_count > 0:
                                    logger.warning(f"‚ö†Ô∏è [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] V·∫´n c√≤n n√∫t next sau l·ªói, nh∆∞ng d·ª´ng l·∫°i ƒë·ªÉ tr√°nh hang")
                                    check_pages = False
                                else:
                                    logger.info(f"üèÅ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng c√≤n n√∫t next sau l·ªói, k·∫øt th√∫c ph√¢n trang")
                                    check_pages = False
                            except Exception as retry_e:
                                # N·∫øu kh√¥ng ki·ªÉm tra ƒë∆∞·ª£c, d·ª´ng l·∫°i ƒë·ªÉ tr√°nh v√≤ng l·∫∑p v√¥ h·∫°n
                                logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ki·ªÉm tra n√∫t next sau l·ªói: {retry_e}")
                                import traceback
                                logger.error(f"Traceback: {traceback.format_exc()}")
                                check_pages = False
                        
                        # ‚úÖ Log tr∆∞·ªõc khi ti·∫øp t·ª•c v√≤ng l·∫∑p
                        if check_pages:
                            logger.info(f"üîÑ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Ti·∫øp t·ª•c v√≤ng l·∫∑p pagination, s·∫Ω x·ª≠ l√Ω trang ti·∫øp theo...")
                        else:
                            logger.info(f"üõë [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] D·ª´ng v√≤ng l·∫∑p pagination, ƒë√£ x·ª≠ l√Ω xong {page_num} trang")
                
                except Exception as e:
                    logger.error(f"‚ùå [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] L·ªói x·ª≠ l√Ω kho·∫£ng {date_range[0]} - {date_range[1]}: {e}")
                    # ‚úÖ Gi·ªØ nguy√™n percent hi·ªán t·∫°i khi c√≥ l·ªói
                    yield {
                        "type": "warning", 
                        "message": f"L·ªói x·ª≠ l√Ω kho·∫£ng {date_range}: {str(e)}",
                        "percent": int(round(min(accumulated_percent_so_far, 100))),  # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                        "accumulated_percent": int(round(min(accumulated_percent_so_far, 100))),  # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                        "accumulated_total": accumulated_total_so_far,
                        "accumulated_downloaded": accumulated_downloaded_so_far
                    }
                    # ‚úÖ V·∫´n c·ªông % c·ªßa kho·∫£ng n√†y (ƒë√£ x·ª≠ l√Ω m·ªôt ph·∫ßn ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu)
                    accumulated_percent_so_far += range_percentages[range_idx]
                    accumulated_percent_so_far = min(accumulated_percent_so_far, 100.0)  # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                    continue
                
                # ‚úÖ Log khi ho√†n th√†nh x·ª≠ l√Ω kho·∫£ng n√†y
                logger.info(f"‚úÖ [THONGBAO] [{range_idx + 1}/{len(date_ranges)}] Ho√†n th√†nh x·ª≠ l√Ω kho·∫£ng {date_range[0]} - {date_range[1]}: T·ªïng {total_count} items, Accumulated %: {accumulated_percent_so_far:.2f}%")
            
            # ‚úÖ Log t·ªïng k·∫øt sau khi x·ª≠ l√Ω t·∫•t c·∫£ c√°c kho·∫£ng
            logger.info(f"üèÅ [THONGBAO] Ho√†n th√†nh crawl t·∫•t c·∫£ {len(date_ranges)} kho·∫£ng: T·ªïng {total_count} items, Accumulated %: {accumulated_percent_so_far:.2f}%")
            
            # Parse downloaded files v√† rename
            parsed_results = []
            files_in_temp_dir = os.listdir(temp_dir) if os.path.exists(temp_dir) else []
            logger.info(f"crawl_thongbao: Found {len(files_in_temp_dir)} files in temp_dir")
            
            if files_in_temp_dir:
                for file_name in files_in_temp_dir:
                    file_path = os.path.join(temp_dir, file_name)
                    if not os.path.isfile(file_path):
                        continue
                    
                    try:
                        # Parse XML ƒë·ªÉ l·∫•y th√¥ng tin
                        with open(file_path, 'r', encoding='utf-8') as f:
                            soup = BeautifulSoup(f, 'html.parser')
                        
                        mgd = soup.find('magiaodichdtu')
                        mgd = mgd.text if mgd else ""
                        
                        ttb = soup.find('tentbao')
                        ttb = ttb.text if ttb else ""
                        
                        ma_tbao = soup.find('matbao')
                        ma_tbao = ma_tbao.text if ma_tbao else ""
                        
                        
                        if "Ti·∫øp nh·∫≠n" in ttb:
                            ttb = "Ti·∫øp nh·∫≠n"
                        if "X√°c nh·∫≠n" in ttb:
                            ttb = "X√°c nh·∫≠n"
                        if ma_tbao == "844":
                            ttb = "Kh√¥ng ch·∫•p nh·∫≠n"
                        elif ma_tbao == "451":
                            ttb = "Ch·∫•p nh·∫≠n"
                        
                        ttb_2 = "X"
                        try:
                            if ttb == "Ti·∫øp nh·∫≠n":
                                ngay_tbao_elem = soup.find('ngaytbao')
                                ttb_2 = ngay_tbao_elem.text if ngay_tbao_elem else "X"
                            else:
                                ngay_chap_nhan_elem = soup.find('ngaychapnhan')
                                if ngay_chap_nhan_elem:
                                    ttb_2 = ngay_chap_nhan_elem.text.split("T")[0] if "T" in ngay_chap_nhan_elem.text else ngay_chap_nhan_elem.text
                        except:
                            ttb_2 = "X"
                        
                        ttb_2_clean = ttb_2.replace("/", "-")
                        new_file_name = f"{mgd} - {ttb} - {ttb_2_clean}.xml"
                        new_file_name = self._remove_accents(new_file_name)
                        
                        # Rename file
                        new_file_path = os.path.join(temp_dir, new_file_name)
                        if os.path.exists(file_path):
                            try:
                                os.rename(file_path, new_file_path)
                                file_name = new_file_name
                                file_path = new_file_path
                            except Exception as rename_err:
                                logger.warning(f"Error renaming {file_name}: {rename_err}")
                        
                        parsed_results.append({
                            "ma_giao_dich": mgd,
                            "ten_thong_bao": ttb,
                            "ma_thong_bao": ma_tbao,
                            "ngay_thong_bao": ttb_2.replace("-", "/") if ttb_2 != "X" else ""  # Tr·∫£ v·ªÅ format g·ªëc
                        })
                        
                        file_size = os.path.getsize(file_path)
                        total_size += file_size
                        files_info.append({"name": file_name, "size": file_size})
                    except Exception as e:
                        logger.warning(f"Error parsing/renaming file {file_name}: {e}")
                        # N·∫øu parse l·ªói, v·∫´n th√™m v√†o files_info v·ªõi t√™n c≈©
                        try:
                            file_size = os.path.getsize(file_path)
                            total_size += file_size
                            files_info.append({"name": file_name, "size": file_size})
                        except:
                            pass
                        continue
                
                # T·∫°o download_id (UUID) ƒë·ªÉ worker c√≥ th·ªÉ download sau (gi·ªëng t·ªù khai)
                download_id = str(uuid.uuid4())
                zip_filename = f"thongbao_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                zip_file_path = os.path.join(self.ZIP_STORAGE_DIR, f"{download_id}.zip")
                
                # L∆∞u zip v√†o disk thay v√¨ ch·ªâ t·∫°o base64 (gi·ªëng t·ªù khai)
                final_files = os.listdir(temp_dir)
                logger.info(f"crawl_thongbao: Found {len(final_files)} files in temp_dir")
                logger.info(f"crawl_thongbao: Creating ZIP from {len(final_files)} files")
                
                if final_files:
                    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                        for file_name in final_files:
                            file_path = os.path.join(temp_dir, file_name)
                            if os.path.isfile(file_path):
                                zf.write(file_path, file_name)
                                logger.debug(f"Added to ZIP: {file_name}")
                    
                    # ƒê·ªçc file ƒë·ªÉ t·∫°o base64 (v·∫´n c·∫ßn cho Redis)
                    with open(zip_file_path, 'rb') as f:
                        zip_base64 = base64.b64encode(f.read()).decode('utf-8')
                    
                    logger.info(f"‚úÖ ƒê√£ t·∫°o file ZIP: {zip_filename} (download_id: {download_id})")
                    
                    # L∆∞u download_id v√†o Redis (gi·ªëng t·ªù khai)
                    try:
                        from shared.redis_client import get_redis_client
                        redis_client = get_redis_client()
                        redis_key = f"session:{session_id}:download_id"
                        redis_client.setex(redis_key, 3600, download_id.encode('utf-8'))
                    except Exception as redis_err:
                        logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u download_id v√†o Redis: {redis_err}")
                else:
                    zip_base64 = None
                    download_id = None
                    logger.warning("crawl_thongbao: No files to add to ZIP")
            else:
                zip_base64 = None
                download_id = None
                zip_filename = f"thongbao_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                logger.warning("crawl_thongbao: No files in temp_dir")
            
            actual_files_count = len(files_info)
            actual_results_count = len(parsed_results)
            
            # ‚úÖ Log tr∆∞·ªõc khi yield complete
            logger.info(f"üì¶ [THONGBAO] Chu·∫©n b·ªã yield complete: total_count={total_count}, actual_results_count={actual_results_count}, actual_files_count={actual_files_count}, zip_base64_length={len(zip_base64) if zip_base64 else 0}, download_id={download_id}")
            
            # Tr·∫£ v·ªÅ total l√† s·ªë rows ƒë√£ x·ª≠ l√Ω (s·ªë items t√¨m th·∫•y) ƒë·ªÉ hi·ªÉn th·ªã ƒë√∫ng
            # zip_base64 s·∫Ω l√† None n·∫øu kh√¥ng c√≥ files, button s·∫Ω disabled
            yield {
                "type": "complete",
                "total": total_count,  # S·ªë items ƒë√£ t√¨m th·∫•y (total_rows_processed)
                "results_count": actual_results_count,  # S·ªë items ƒë√£ parse
                "total_rows_processed": total_count,  # S·ªë rows ƒë√£ x·ª≠ l√Ω (ƒë·ªÉ debug)
                "results": parsed_results,
                "files": files_info,
                "files_count": actual_files_count,  # S·ªë file th·ª±c t·∫ø trong ZIP
                "total_size": total_size,
                "zip_base64": zip_base64,  # None n·∫øu kh√¥ng c√≥ files
                "zip_filename": zip_filename,  # ‚úÖ D√πng zip_filename ƒë√£ t·∫°o ·ªü tr√™n
                "download_id": download_id  # ‚úÖ Th√™m download_id (gi·ªëng t·ªù khai)
            }
            
        except Exception as e:
            logger.error(f"‚ùå [THONGBAO] Error in crawl_thongbao: {e}")
            import traceback
            logger.error(f"‚ùå [THONGBAO] Traceback: {traceback.format_exc()}")
            error_msg = str(e)
            
            # ‚úÖ ƒê·∫£m b·∫£o yield complete event ngay c·∫£ khi c√≥ l·ªói (v·ªõi files ƒë√£ download)
            try:
                # Parse downloaded files n·∫øu c√≥
                parsed_results = []
                files_in_temp_dir = os.listdir(temp_dir) if os.path.exists(temp_dir) else []
                files_info = []
                total_size = 0
                zip_base64 = None
                
                if files_in_temp_dir:
                    for file_name in files_in_temp_dir:
                        file_path = os.path.join(temp_dir, file_name)
                        if os.path.isfile(file_path):
                            try:
                                file_size = os.path.getsize(file_path)
                                total_size += file_size
                                files_info.append({"name": file_name, "size": file_size})
                            except:
                                pass
                    
                    if files_info:
                        # T·∫°o download_id v√† l∆∞u ZIP v√†o disk (gi·ªëng t·ªù khai)
                        try:
                            download_id = str(uuid.uuid4())
                            zip_filename = f"thongbao_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                            zip_file_path = os.path.join(self.ZIP_STORAGE_DIR, f"{download_id}.zip")
                            
                            with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                                for file_name in files_in_temp_dir:
                                    file_path = os.path.join(temp_dir, file_name)
                                    if os.path.isfile(file_path):
                                        zf.write(file_path, file_name)
                            
                            # ƒê·ªçc file ƒë·ªÉ t·∫°o base64
                            with open(zip_file_path, 'rb') as f:
                                zip_base64 = base64.b64encode(f.read()).decode('utf-8')
                            
                        except Exception as zip_e:
                            logger.error(f"‚ùå [THONGBAO] L·ªói t·∫°o ZIP: {zip_e}")
                            download_id = None
                            zip_filename = f"thongbao_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                    else:
                        download_id = None
                        zip_filename = f"thongbao_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                
                # Yield complete event v·ªõi files ƒë√£ download
                yield {
                    "type": "complete",
                    "total": total_count if 'total_count' in locals() else 0,
                    "results_count": len(parsed_results),
                    "results": parsed_results,
                    "files": files_info,
                    "files_count": len(files_info),
                    "total_size": total_size,
                    "zip_base64": zip_base64,
                    "zip_filename": zip_filename,
                    "download_id": download_id,  # ‚úÖ Th√™m download_id
                    "error": error_msg
                }
            except Exception as final_e:
                logger.error(f"‚ùå [THONGBAO] L·ªói khi yield complete event sau l·ªói: {final_e}")
            # Ki·ªÉm tra session timeout
            if "timeout" in error_msg.lower() or "phi√™n giao d·ªãch" in error_msg.lower():
                yield {"type": "error", "error": "Phi√™n giao d·ªãch h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "SESSION_EXPIRED"}
            else:
                yield {"type": "error", "error": f"L·ªói khi tra c·ª©u th√¥ng b√°o: {error_msg}", "error_code": "CRAWL_ERROR"}
        
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    
    async def _download_single_giaynoptien(self, session: SessionData, item: Dict, temp_dir: str, max_retries: int = 2) -> bool:
        """
        Download m·ªôt file gi·∫•y n·ªôp ti·ªÅn
        item: {
            "id": id_gnt,
            "row": row,
            "col_index": col_idx,
            "link_locator": links.first
        }
        """
        id_gnt = item.get("id")
        if not id_gnt:
            return False
        
        page = session.page
        frame = None
        
        # T√¨m frame ch·ª©a form gi·∫•y n·ªôp ti·ªÅn
        try:
            frames = page.frames
            for f in frames:
                if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                    frame = f
                    break
        except:
            pass
        
        if not frame:
            logger.error(f"Kh√¥ng t√¨m th·∫•y frame ƒë·ªÉ download gi·∫•y n·ªôp ti·ªÅn {id_gnt}")
            return False
        
        # L·∫•y tham s·ªë t·ª´ form reportForm
        form_params = {}
        try:
            form = frame.locator('form[name="reportForm"], form#reportForm').first
            if await form.count() > 0:
                inputs = form.locator('input[type="hidden"]')
                input_count = await inputs.count()
                for i in range(input_count):
                    try:
                        input_elem = inputs.nth(i)
                        name = await input_elem.get_attribute('name')
                        value = await input_elem.get_attribute('value')
                        if name and value:
                            form_params[name] = value
                    except:
                        continue
        except Exception as e:
            logger.warning(f"L·ªói khi l·∫•y tham s·ªë t·ª´ form cho {id_gnt}: {e}")
        
        # L·∫•y c√°c tham s·ªë c·∫ßn thi·∫øt
        dse_session_id = form_params.get('dse_sessionId', session.dse_session_id)
        dse_application_id = form_params.get('dse_applicationId', '-1')
        dse_operation_name = form_params.get('dse_operationName', 'corpQueryTaxProc')
        dse_page_id = form_params.get('dse_pageId', '35')
        dse_processor_state = form_params.get('dse_processorState', 'viewQueryPage')
        dse_processor_id = form_params.get('dse_processorId', '')
        
        # X√¢y d·ª±ng URL download (gi·ªëng khi click downloadGNT)
        download_url = (
            f"{BASE_URL}/etaxnnt/Request?"
            f"dse_sessionId={dse_session_id}&"
            f"dse_applicationId={dse_application_id}&"
            f"dse_operationName={dse_operation_name}&"
            f"dse_pageId={dse_page_id}&"
            f"dse_processorState={dse_processor_state}&"
            f"dse_processorId={dse_processor_id}&"
            f"dse_nextEventName=download&"
            f"ctuId={id_gnt}"
        )
        
        # Th·ª≠ download v·ªõi retry
        for retry in range(max_retries + 1):
            try:
                # Th·ª≠ click link tr∆∞·ªõc (nhanh h∆°n)
                link_locator = item.get("link_locator")
                if link_locator:
                    try:
                        async with page.expect_download(timeout=30000) as download_info:
                            await link_locator.click()
                        
                        download = await download_info.value
                        file_path = os.path.join(temp_dir, f"gnt_{id_gnt}.xml")
                        await download.save_as(file_path)
                        
                        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                            logger.info(f"Downloaded giay nop tien {id_gnt} via click")
                            return True
                    except:
                        pass
                
                # Fallback: d√πng URL tr·ª±c ti·∫øp
                new_page = None
                try:
                    new_page = await session.context.new_page()
                    new_page.set_default_timeout(30000)
                    
                    async with new_page.expect_download(timeout=30000) as download_info:
                        await new_page.goto(download_url, wait_until="domcontentloaded")
                    
                    download = await download_info.value
                    file_path = os.path.join(temp_dir, f"gnt_{id_gnt}.xml")
                    await download.save_as(file_path)
                    
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        logger.info(f"Downloaded giay nop tien {id_gnt} via URL")
                        return True
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout khi download giay nop tien {id_gnt}")
                except Exception as e:
                    logger.warning(f"L·ªói khi download giay nop tien {id_gnt} qua URL: {e}")
                finally:
                    if new_page:
                        try:
                            await new_page.close()
                        except:
                            pass
                
                # N·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c, th·ª≠ d√πng httpx (n·∫øu c√≥ session_id t·ª´ crawl_giay_nop_tien)
                # Note: session_id kh√¥ng c√≥ trong h√†m n√†y, b·ªè qua httpx fallback
                
            except Exception as e:
                logger.warning(f"Error downloading giaynoptien {id_gnt} (attempt {retry + 1}/{max_retries + 1}): {e}")
                if retry < max_retries:
                    await asyncio.sleep(1)
        
        return False
    
    async def crawl_giay_nop_tien(
        self,
        session_id: str,
        start_date: str,
        end_date: str,
        job_id: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        session = self.session_manager.get_session(session_id)
        if not session:
            yield {"type": "error", "error": "Session kh√¥ng t·ªìn t·∫°i ho·∫∑c ƒë√£ h·∫øt h·∫°n", "error_code": "SESSION_NOT_FOUND"}
            return
        
        if not session.is_logged_in:
            yield {"type": "error", "error": "Ch∆∞a ƒëƒÉng nh·∫≠p. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "NOT_LOGGED_IN"}
            return
        
        page = session.page
        
        # ‚úÖ FIX: T·∫°o temp directory trong source code thay v√¨ system temp (gi·ªëng t·ªù khai)
        # L·∫•y ƒë∆∞·ªùng d·∫´n project (tool-go-soft)
        current_dir = os.path.dirname(os.path.abspath(__file__))  # .../services/
        services_dir = os.path.dirname(current_dir)  # .../tool-go-soft/
        temp_base_dir = os.path.join(services_dir, "temp")  # .../tool-go-soft/temp/
        os.makedirs(temp_base_dir, exist_ok=True)
        
        # T·∫°o temp directory v·ªõi timestamp ƒë·ªÉ tr√°nh conflict
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        temp_dir = os.path.join(temp_base_dir, f"giaynoptien_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)
        
        # ‚úÖ Ch·ªâ log temp_dir khi c√≥ l·ªói (kh√¥ng log khi m·ªõi b·∫Øt ƒë·∫ßu)
        # logger.info(f"üìÅ Temp directory for debug files: {temp_dir}")
        
        # ‚úÖ FIX: Kh√¥ng t·∫°o folder screenshot ngay t·ª´ ƒë·∫ßu, ch·ªâ t·∫°o khi c√≥ l·ªói th·ª±c s·ª±
        screenshots_dir = None  # S·∫Ω ƒë∆∞·ª£c t·∫°o khi c·∫ßn screenshot
        
        ssid = session.dse_session_id
        
        try:
            yield {"type": "info", "message": "ƒêang x·ª≠ l√Ω gi·∫•y n·ªôp ti·ªÅn..."}
            
            # Navigate ƒë·∫øn trang gi·∫•y n·ªôp ti·ªÅn qua connectSSO (gi·ªëng t·ªù khai)
            success = await self._navigate_to_giaynoptien_page(page, ssid)
            
            if not success:
                # Ch·ª•p m√†n h√¨nh khi navigate th·∫•t b·∫°i
                try:
                    screenshot_path = os.path.join(screenshots_dir, "01_navigate_failed.png")
                    logger.info(f"Attempting to save screenshot to: {screenshot_path}")
                    await page.screenshot(path=screenshot_path, full_page=True)
                    if os.path.exists(screenshot_path):
                        file_size = os.path.getsize(screenshot_path)
                        logger.info(f"‚úÖ Screenshot saved: {screenshot_path} ({file_size} bytes)")
                    else:
                        logger.error(f"‚ùå Screenshot file not created: {screenshot_path}")
                except Exception as e:
                    logger.error(f"‚ùå Error saving screenshot 01_navigate_failed: {e}")
                yield {"type": "error", "error": "Kh√¥ng th·ªÉ navigate ƒë·∫øn trang tra c·ª©u gi·∫•y n·ªôp thu·∫ø. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            # T√¨m frame t·ª´ iframe SSO (gi·ªëng t·ªù khai)
            # ƒê·ª£i frame xu·∫•t hi·ªán trong page.frames (c√≥ th·ªÉ m·∫•t th·ªùi gian)
            frame = None
            max_wait = 30  # ƒê·ª£i t·ªëi ƒëa 15 gi√¢y (30 * 0.5)
            for i in range(max_wait):
                try:
                    frames = page.frames
                    for f in frames:
                        if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                            frame = f
                            logger.info(f"Found frame for giaynoptien: {frame.url[:100]}...")
                            # Ki·ªÉm tra xem frame ƒë√£ load ch∆∞a
                            try:
                                await frame.wait_for_load_state('domcontentloaded', timeout=2000)
                                break
                            except:
                                # Frame ch∆∞a load xong, ti·∫øp t·ª•c ƒë·ª£i
                                frame = None
                                pass
                    
                    if frame:
                        break
                except Exception as e:
                    logger.debug(f"Waiting for frame (attempt {i + 1}/{max_wait}): {e}")
                
                await asyncio.sleep(0.5)
            
            if not frame:
                yield {"type": "error", "error": "Kh√¥ng t√¨m th·∫•y iframe sau khi navigate. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            # ƒê·ª£i frame load v√† ki·ªÉm tra form gi·∫•y n·ªôp ti·ªÅn
            try:
                await frame.wait_for_load_state('domcontentloaded', timeout=15000)
                await asyncio.sleep(1)
                await frame.wait_for_selector('input[name="ngay_lap_tu_ngay"], #ngay_lap_tu_ngay', timeout=15000)
                logger.info("Tra cuu giay nop tien form loaded successfully")
                
                # ‚úÖ Kh√¥ng ch·ª•p screenshot khi form load th√†nh c√¥ng (ch·ªâ ch·ª•p khi c√≥ l·ªói)
            except Exception as e:
                logger.warning(f"Frame found but form not found: {e}")
                # Ch·ª•p m√†n h√¨nh khi form kh√¥ng load ƒë∆∞·ª£c
                try:
                    # ‚úÖ T·∫°o folder screenshot khi c√≥ l·ªói
                    if screenshots_dir is None:
                        screenshots_base_dir = os.path.join(services_dir, "screenshots")
                        os.makedirs(screenshots_base_dir, exist_ok=True)
                        screenshots_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        screenshots_dir = os.path.join(screenshots_base_dir, f"giaynoptien_{session_id[:8]}_{screenshots_timestamp}")
                        os.makedirs(screenshots_dir, exist_ok=True)
                    
                    screenshot_path = os.path.join(screenshots_dir, "02_form_not_found.png")
                    logger.info(f"Attempting to save screenshot to: {screenshot_path}")
                    await page.screenshot(path=screenshot_path, full_page=True)
                    if os.path.exists(screenshot_path):
                        file_size = os.path.getsize(screenshot_path)
                        logger.info(f"‚úÖ Screenshot saved: {screenshot_path} ({file_size} bytes)")
                    else:
                        logger.error(f"‚ùå Screenshot file not created: {screenshot_path}")
                except Exception as e:
                    logger.error(f"‚ùå Error saving screenshot 02_form_not_found: {e}")
                yield {"type": "error", "error": "Kh√¥ng t√¨m th·∫•y form tra c·ª©u gi·∫•y n·ªôp ti·ªÅn. Vui l√≤ng th·ª≠ l·∫°i.", "error_code": "NAVIGATION_ERROR"}
                return
            
            # Check session timeout
            if await self._check_session_timeout(page):
                yield {
                    "type": "error",
                    "error": "Phi√™n giao d·ªãch h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.",
                    "error_code": "SESSION_EXPIRED"
                }
                return
            
            # Chia kho·∫£ng th·ªùi gian
            date_ranges = self._get_date_ranges(start_date, end_date, days_interval=360)
            
            # ‚úÖ T√≠nh % cho t·ª´ng kho·∫£ng th·ªùi gian (gi·ªëng t·ªù khai)
            total_days = (datetime.strptime(end_date, "%d/%m/%Y") - datetime.strptime(start_date, "%d/%m/%Y")).days + 1
            days_per_range = 360
            range_percentages = []
            for i, dr in enumerate(self._get_date_ranges(start_date, end_date, days_interval=days_per_range)):
                start_dt = datetime.strptime(dr[0], "%d/%m/%Y")
                end_dt = datetime.strptime(dr[1], "%d/%m/%Y")
                range_days = (end_dt - start_dt).days + 1
                range_percent = (range_days / total_days) * 100 if total_days > 0 else 0
                range_percentages.append(range_percent)
            
            total_count = 0
            results = []
            files_info = []
            total_size = 0
            
            # ‚úÖ Kh·ªüi t·∫°o accumulated variables (gi·ªëng th√¥ng b√°o)
            accumulated_total_so_far = 0
            accumulated_downloaded_so_far = 0
            accumulated_percent_so_far = 0.0
            
            # ‚úÖ T·∫°o screenshot_dir m·ªôt l·∫ßn duy nh·∫•t cho to√†n b·ªô job (kh√¥ng d√πng timestamp)
            screenshot_dir = None
            def get_screenshot_dir():
                nonlocal screenshot_dir
                if screenshot_dir is None:
                    # D√πng job_id n·∫øu c√≥, n·∫øu kh√¥ng th√¨ d√πng session_id
                    folder_name = f"giaynoptien_{job_id[:8] if job_id else session_id[:8]}"
                    screenshot_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "screenshots", folder_name)
                    os.makedirs(screenshot_dir, exist_ok=True)
                return screenshot_dir
            
            yield {"type": "info", "message": f"B·∫Øt ƒë·∫ßu crawl {len(date_ranges)} kho·∫£ng th·ªùi gian..."}
            
            for range_idx, date_range in enumerate(date_ranges):
                # ‚úÖ Check cancelled tr∆∞·ªõc khi x·ª≠ l√Ω kho·∫£ng ti·∫øp theo
                if job_id and await self._check_cancelled(job_id):
                    logger.info(f"Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                    yield {"type": "error", "error": "Job ƒë√£ b·ªã h·ªßy", "error_code": "JOB_CANCELLED"}
                    return
                
                yield {
                    "type": "progress", 
                    "current": range_idx + 1, 
                    "total": len(date_ranges),
                    "message": f"ƒêang x·ª≠ l√Ω kho·∫£ng {date_range[0]} - {date_range[1]}...",
                    "accumulated_total": accumulated_total_so_far,
                    "accumulated_downloaded": accumulated_downloaded_so_far,
                    "accumulated_percent": int(round(accumulated_percent_so_far))
                }
                
                try:
                    # Nh·∫≠p ng√†y b·∫Øt ƒë·∫ßu (d√πng name attribute)
                    start_input = frame.locator('input[name="ngay_lap_tu_ngay"], input#ngay_lap_tu_ngay')
                    await start_input.fill('')
                    await start_input.fill(date_range[0])
                    
                    # Nh·∫≠p ng√†y k·∫øt th√∫c (d√πng name attribute)
                    end_input = frame.locator('input[name="ngay_lap_den_ngay"], input#ngay_lap_den_ngay')
                    await end_input.click()
                    from playwright.async_api import Keyboard
                    await end_input.press('Control+a')
                    await end_input.fill(date_range[1])
                    
                    # Click t√¨m ki·∫øm (d√πng value ho·∫∑c onclick)
                    search_btn = frame.locator('input[value="Tra c·ª©u"], input[onclick*="traCuuChungTu"]')
                    await search_btn.click()
                    
                    await asyncio.sleep(2)
                    
                    logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ click search cho kho·∫£ng: {date_range[0]} - {date_range[1]}")
                    
                    # ‚úÖ ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o request ƒë√£ ƒë∆∞·ª£c g·ª≠i
                    await asyncio.sleep(1)
                    
                    # ‚úÖ T√¨m l·∫°i frame m·ªõi sau khi click search (iframe c√≥ th·ªÉ reload khi chuy·ªÉn kho·∫£ng th·ªùi gian)
                    try:
                        frames = page.frames
                        for f in frames:
                            if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                frame = f  # C·∫≠p nh·∫≠t frame object m·ªõi
                                logger.info(f"üîÑ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ t√¨m l·∫°i frame m·ªõi sau khi click search: {frame.url[:100]}...")
                                break
                    except Exception as refind_frame_e:
                        logger.warning(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ t√¨m l·∫°i frame m·ªõi sau khi click search: {refind_frame_e}")
                    
                    # ‚úÖ ƒê·ª£i frame load xong tr∆∞·ªõc khi ƒë·ª£i table
                    try:
                        await frame.wait_for_load_state('networkidle', timeout=5000)
                        logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Frame ƒë√£ load xong (networkidle)")
                    except Exception as frame_load_e:
                        logger.debug(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ƒë·ª£i frame networkidle: {frame_load_e}")
                    
                    # ‚úÖ ƒê·ª£i table load xong ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√£ chuy·ªÉn sang kho·∫£ng m·ªõi
                    try:
                        logger.info(f"‚è≥ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] ƒêang ƒë·ª£i table load sau khi click search...")
                        table_body_check = frame.locator('table#data_content_onday tbody#allResultTableBody, #allResultTableBody').first
                        await table_body_check.wait_for(timeout=10000, state='visible')
                        logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Table ƒë√£ load xong sau khi click search")
                        
                        # ‚úÖ ƒê·ª£i th√™m m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c render xong
                        await asyncio.sleep(1.5)
                        logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ ƒë·ª£i th√™m ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë√£ render xong")
                    except Exception as wait_table_e:
                        logger.warning(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ƒë·ª£i table load sau khi click search: {wait_table_e}")
                        # Ti·∫øp t·ª•c x·ª≠ l√Ω, s·∫Ω ki·ªÉm tra "Kh√¥ng c√≥ d·ªØ li·ªáu" ·ªü b∆∞·ªõc ti·∫øp theo
                    
                    # ‚úÖ Ki·ªÉm tra "Kh√¥ng c√≥ d·ªØ li·ªáu" ngay sau khi search (tr∆∞·ªõc khi v√†o pagination)
                    try:
                        no_data_text = frame.locator('div:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), strong:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), div.align-center:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu")').first
                        if await no_data_text.count() > 0:
                            no_data_content = await no_data_text.text_content()
                            if "Kh√¥ng c√≥ d·ªØ li·ªáu" in (no_data_content or ""):
                                logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Ph√°t hi·ªán 'Kh√¥ng c√≥ d·ªØ li·ªáu' cho kho·∫£ng {date_range[0]} - {date_range[1]}")
                                yield {
                                    "type": "info", 
                                    "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}",
                                    "accumulated_percent": int(round(accumulated_percent_so_far)),
                                    "accumulated_total": accumulated_total_so_far,
                                    "accumulated_downloaded": accumulated_downloaded_so_far
                                }
                                accumulated_percent_so_far += range_percentages[range_idx] if range_idx < len(range_percentages) else 0
                                continue  # B·ªè qua kho·∫£ng n√†y, chuy·ªÉn sang kho·∫£ng ti·∫øp theo
                    except Exception as no_data_check_e:
                        logger.debug(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ki·ªÉm tra 'Kh√¥ng c√≥ d·ªØ li·ªáu': {no_data_check_e}")
                    
                    # X·ª≠ l√Ω ph√¢n trang (gi·ªëng th√¥ng b√°o)
                    check_pages = True
                    page_num = 0
                    range_total_records = None  # T·ªïng s·ªë b·∫£n ghi trong kho·∫£ng n√†y (parse t·ª´ currAcc)
                    range_downloaded_so_far = 0  # T·ªïng s·ªë file ƒë√£ download trong kho·∫£ng n√†y (t·ª´ c√°c trang tr∆∞·ªõc)
                    max_pages = 100  # ‚úÖ Gi·ªõi h·∫°n s·ªë trang t·ªëi ƒëa ƒë·ªÉ tr√°nh v√≤ng l·∫∑p v√¥ h·∫°n
                    previous_row_count = 0  # ‚úÖ L∆∞u s·ªë rows c·ªßa trang tr∆∞·ªõc ƒë·ªÉ verify table ƒë√£ chuy·ªÉn trang
                    
                    # ‚úÖ L∆∞u % t√≠ch l≈©y t·∫°i th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu kho·∫£ng n√†y (ƒë·ªÉ t√≠nh % cho kho·∫£ng n√†y ch√≠nh x√°c) (gi·ªëng th√¥ng b√°o)
                    accumulated_percent_so_far_at_range_start = accumulated_percent_so_far
                    
                    # ‚úÖ T√≠nh % cho kho·∫£ng n√†y
                    range_percent = range_percentages[range_idx] if range_idx < len(range_percentages) else 0
                    
                    while check_pages and page_num < max_pages:
                        # ‚úÖ Check cancelled tr∆∞·ªõc khi x·ª≠ l√Ω trang ti·∫øp theo
                        if job_id and await self._check_cancelled(job_id):
                            logger.info(f"[GIAYNOPTIEN] Job {job_id} ƒë√£ b·ªã cancel, d·ª´ng crawl")
                            yield {
                                "type": "error",
                                "error": "Job ƒë√£ b·ªã h·ªßy",
                                "error_code": "JOB_CANCELLED"
                            }
                            return
                        
                        page_num += 1
                        logger.info(f"üìÑ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] ƒêang x·ª≠ l√Ω trang {page_num}... (check_pages={check_pages})")
                        
                        # ‚úÖ Ki·ªÉm tra "Kh√¥ng c√≥ d·ªØ li·ªáu" tr∆∞·ªõc khi t√¨m table
                        try:
                            no_data_text = frame.locator('div:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), strong:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu")').first
                            if await no_data_text.count() > 0:
                                no_data_content = await no_data_text.text_content()
                                if "Kh√¥ng c√≥ d·ªØ li·ªáu" in (no_data_content or ""):
                                    logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Ph√°t hi·ªán 'Kh√¥ng c√≥ d·ªØ li·ªáu' cho kho·∫£ng {date_range[0]} - {date_range[1]}")
                                    yield {
                                        "type": "info", 
                                        "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}",
                                        "accumulated_percent": int(round(accumulated_percent_so_far)),
                                        "accumulated_total": accumulated_total_so_far,
                                        "accumulated_downloaded": accumulated_downloaded_so_far
                                    }
                                    accumulated_percent_so_far += range_percent
                                    break
                        except Exception as no_data_check_e:
                            logger.debug(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ki·ªÉm tra 'Kh√¥ng c√≥ d·ªØ li·ªáu': {no_data_check_e}")
                        
                        # T√¨m b·∫£ng k·∫øt qu·∫£
                        try:
                            table_body = frame.locator('table#data_content_onday tbody#allResultTableBody, #allResultTableBody').first
                            await table_body.wait_for(timeout=10000, state='visible')
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng t√¨m th·∫•y b·∫£ng k·∫øt qu·∫£ cho kho·∫£ng {date_range[0]} - {date_range[1]}: {e}")
                            
                            # ‚úÖ Ki·ªÉm tra "Kh√¥ng c√≥ d·ªØ li·ªáu" TR∆Ø·ªöC KHI screenshot
                            has_no_data = False
                            try:
                                no_data_text = frame.locator('div:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), strong:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), div.align-center:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu")').first
                                if await no_data_text.count() > 0:
                                    no_data_content = await no_data_text.text_content()
                                    if "Kh√¥ng c√≥ d·ªØ li·ªáu" in (no_data_content or ""):
                                        has_no_data = True
                                        logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Ph√°t hi·ªán 'Kh√¥ng c√≥ d·ªØ li·ªáu' (kh√¥ng c√≥ table)")
                                        yield {
                                            "type": "info", 
                                            "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}",
                                            "accumulated_percent": int(round(accumulated_percent_so_far)),
                                            "accumulated_total": accumulated_total_so_far,
                                            "accumulated_downloaded": accumulated_downloaded_so_far
                                        }
                                        accumulated_percent_so_far += range_percent
                                        break
                            except Exception as no_data_check_e2:
                                logger.debug(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ ki·ªÉm tra 'Kh√¥ng c√≥ d·ªØ li·ªáu' l·∫ßn 2: {no_data_check_e2}")
                            
                            # ‚úÖ CH·ªà screenshot khi th·ª±c s·ª± c√≥ l·ªói (kh√¥ng ph·∫£i do kh√¥ng c√≥ d·ªØ li·ªáu)
                            if not has_no_data:
                                try:
                                    screenshot_dir = get_screenshot_dir()
                                    
                                    if 'page' in locals() and page:
                                        page_screenshot = os.path.join(screenshot_dir, f"no_table_page_{range_idx + 1}_page_{page_num}.png")
                                        await page.screenshot(path=page_screenshot, full_page=True)
                                        logger.info(f"üì∏ Screenshot page saved: {page_screenshot}")
                                    
                                    if 'frame' in locals() and frame:
                                        try:
                                            frame_screenshot = os.path.join(screenshot_dir, f"no_table_frame_{range_idx + 1}_page_{page_num}.png")
                                            await frame.screenshot(path=frame_screenshot, full_page=True)
                                            logger.info(f"üì∏ Screenshot frame saved: {frame_screenshot}")
                                        except Exception as frame_screenshot_e:
                                            logger.warning(f"‚ö†Ô∏è Cannot screenshot frame: {frame_screenshot_e}")
                                        
                                        try:
                                            frame_html = await frame.content()
                                            html_file = os.path.join(screenshot_dir, f"no_table_frame_{range_idx + 1}_page_{page_num}.html")
                                            with open(html_file, 'w', encoding='utf-8') as f:
                                                f.write(frame_html)
                                            logger.info(f"üìÑ Frame HTML saved: {html_file}")
                                        except Exception as html_e:
                                            logger.warning(f"‚ö†Ô∏è Cannot save frame HTML: {html_e}")
                                    
                                    logger.info(f"üì∏ Screenshots saved to: {screenshot_dir}")
                                except Exception as screenshot_e:
                                    logger.error(f"‚ùå Error taking screenshot: {screenshot_e}")
                            else:
                                # Kh√¥ng c√≥ d·ªØ li·ªáu, kh√¥ng c·∫ßn screenshot
                                logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng c√≥ d·ªØ li·ªáu, b·ªè qua screenshot")
                            
                            # ‚úÖ N·∫øu kh√¥ng c√≥ "Kh√¥ng c√≥ d·ªØ li·ªáu" v√† kh√¥ng c√≥ table, b·ªè qua kho·∫£ng n√†y
                            logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng c√≥ table v√† kh√¥ng c√≥ 'Kh√¥ng c√≥ d·ªØ li·ªáu', b·ªè qua kho·∫£ng n√†y")
                            if total_count == 0:
                                yield {
                                    "type": "info", 
                                    "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}",
                                    "accumulated_percent": int(round(accumulated_percent_so_far)),
                                    "accumulated_total": accumulated_total_so_far,
                                    "accumulated_downloaded": accumulated_downloaded_so_far
                                }
                            accumulated_percent_so_far += range_percent
                            break
                        
                        rows = table_body.locator('tr')
                        row_count = await rows.count()
                        
                        # ‚úÖ L∆∞u row_count c·ªßa trang hi·ªán t·∫°i ƒë·ªÉ verify sau khi click next
                        if page_num == 1:
                            previous_row_count = row_count
                        
                        # ‚úÖ CH·ªà parse currAcc n·∫øu c√≥ rows (tr√°nh parse sai khi kh√¥ng c√≥ d·ªØ li·ªáu)
                        # ‚úÖ Parse t·ªïng s·ªë b·∫£n ghi t·ª´ ph·∫ßn currAcc (ch·ªâ parse ·ªü trang ƒë·∫ßu ti√™n v√† khi c√≥ rows)
                        if page_num == 1 and row_count > 0:
                            try:
                                curr_acc = frame.locator('#currAcc').first
                                if await curr_acc.count() > 0:
                                    curr_acc_text = await curr_acc.text_content()
                                    import re
                                    match = re.search(r'C√≥\s*<b>(\d+)</b>\s*b·∫£n\s*ghi|C√≥\s*(\d+)\s*b·∫£n\s*ghi', curr_acc_text)
                                    if match:
                                        range_total_records = int(match.group(1) or match.group(2))
                                        
                                        # ‚úÖ Parse s·ªë trang t·ª´ pagination info
                                        pagination_info = await self._extract_pagination_info(frame)
                                        total_pages = pagination_info.get("total_pages", 1) if pagination_info else 1
                                        
                                        logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Parse t·ªïng s·ªë b·∫£n ghi t·ª´ currAcc: {range_total_records} trong {total_pages} trang")
                                        
                                        # ‚úÖ KH√îNG c·ªông range_total_records v√†o accumulated_total ngay l·∫≠p t·ª©c
                                        # S·∫Ω c·ªông sau khi bi·∫øt s·ªë file th·ª±c s·ª± c·∫ßn download (sau khi filter duplicate)
                                        
                                        yield {
                                            "type": "info",
                                            "message": f"T√¨m th·∫•y {range_total_records} b·∫£n ghi trong {total_pages} trang. B·∫Øt ƒë·∫ßu t·∫£i...",
                                            "accumulated_total": accumulated_total_so_far,
                                            "accumulated_downloaded": accumulated_downloaded_so_far,
                                            "accumulated_percent": int(round(accumulated_percent_so_far))
                                        }
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ parse t·ªïng s·ªë b·∫£n ghi t·ª´ currAcc: {e}")
                        
                        # ‚úÖ Ki·ªÉm tra n·∫øu kh√¥ng c√≥ rows (table r·ªóng)
                        if row_count == 0:
                            logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Table r·ªóng (0 rows)")
                            if page_num == 1 and total_count == 0:
                                yield {
                                    "type": "info", 
                                    "message": f"Kh√¥ng c√≥ d·ªØ li·ªáu trong kho·∫£ng {date_range[0]} - {date_range[1]}",
                                    "accumulated_percent": int(round(accumulated_percent_so_far)),
                                    "accumulated_total": accumulated_total_so_far,
                                    "accumulated_downloaded": accumulated_downloaded_so_far
                                }
                                accumulated_percent_so_far += range_percent
                                break
                            else:
                                # Kh√¥ng c√≥ rows tr√™n trang n√†y, d·ª´ng pagination
                                check_pages = False
                                break
                        
                        logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: T√¨m th·∫•y {row_count} rows, Range %: {range_percent:.2f}%, Accumulated %: {accumulated_percent_so_far:.2f}%")
                        
                        yield {
                            "type": "progress", 
                            "current": total_count, 
                            "message": f"ƒêang x·ª≠ l√Ω {row_count} gi·∫•y n·ªôp ti·ªÅn (trang hi·ªán t·∫°i)...",
                            "percent": int(round(min(accumulated_percent_so_far, 100))),
                            "accumulated_percent": int(round(min(accumulated_percent_so_far, 100))),
                            "accumulated_total": accumulated_total_so_far,
                            "accumulated_downloaded": accumulated_downloaded_so_far
                        }
                        
                        download_queue = []
                        page_valid_count = 0
                        
                        for i in range(row_count):
                            try:
                                row = rows.nth(i)
                                cols = row.locator('td')
                                col_count = await cols.count()
                                
                                if col_count < 5:
                                    continue
                                
                                # L·∫•y id_gnt t·ª´ link chiTietCT(id) trong c·ªôt 5 (index 4)
                                # Ho·∫∑c t·ª´ link downloadGNT(id) trong c·ªôt 19 (index 18)
                                id_gnt = None
                                
                                # Th·ª≠ l·∫•y t·ª´ c·ªôt 5 (chiTietCT)
                                try:
                                    if col_count > 4:
                                        col5_links = cols.nth(4).locator('a[href*="chiTietCT"]')
                                        if await col5_links.count() > 0:
                                            href = await col5_links.first.get_attribute('href')
                                            if href and 'chiTietCT(' in href:
                                                match = re.search(r'chiTietCT\((\d+)\)', href)
                                                if match:
                                                    id_gnt = match.group(1)
                                except:
                                    pass
                                
                                # N·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c t·ª´ c·ªôt 5, th·ª≠ l·∫•y t·ª´ c·ªôt 19 (downloadGNT)
                                if not id_gnt:
                                    try:
                                        if col_count > 18:
                                            col19_links = cols.nth(18).locator('a[href*="downloadGNT"]')
                                            if await col19_links.count() > 0:
                                                href = await col19_links.first.get_attribute('href')
                                                if href and 'downloadGNT(' in href:
                                                    match = re.search(r'downloadGNT\((\d+)\)', href)
                                                    if match:
                                                        id_gnt = match.group(1)
                                    except:
                                        pass
                                
                                # Fallback: L·∫•y t·ª´ c·ªôt 2
                                if not id_gnt:
                                    try:
                                        id_gnt = await cols.nth(2).text_content()
                                        id_gnt = id_gnt.strip() if id_gnt else ""
                                        if not id_gnt or len(id_gnt) < 4:
                                            id_gnt = None
                                    except:
                                        pass
                                
                                if not id_gnt:
                                    continue
                                
                                # Ch·ªâ ƒë·∫øm khi item h·ª£p l·ªá (gi·ªëng th√¥ng b√°o)
                                page_valid_count += 1
                                total_count += 1
                                
                                # T√¨m link download t·ª´ c√°c c·ªôt 17-20 (c·ªôt 19 l√† c·ªôt # c√≥ link downloadGNT)
                                download_link_found = None
                                download_col_index = None
                                
                                for col_idx in [17, 18, 19, 20]:
                                    if col_count > col_idx and not download_link_found:
                                        try:
                                            links = cols.nth(col_idx).locator('a[href*="downloadGNT"], a[onclick*="downloadGNT"]')
                                            link_count = await links.count()
                                            if link_count > 0:
                                                download_link_found = links.first
                                                download_col_index = col_idx
                                                logger.info(f"Found download link for giaynoptien {id_gnt} in column {col_idx}")
                                                break
                                        except Exception as e:
                                            logger.debug(f"Error checking column {col_idx} for download link: {e}")
                                            pass
                                
                                if download_link_found:
                                                download_queue.append({
                                                    "id": id_gnt,
                                        "download_link": download_link_found,
                                        "cols": cols,
                                        "col_index": download_col_index
                                    })
                            
                            except Exception as e:
                                logger.error(f"Error processing row: {e}")
                                continue
                        
                        logger.info(f"üìã [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: C√≥ {page_valid_count} items h·ª£p l·ªá, {len(download_queue)} items c√≥ link download")
                        
                        # Download t·ª´ng file v√† yield progress (gi·ªëng th√¥ng b√°o)
                        if download_queue:
                            queue_total = len(download_queue)
                            
                            # ‚úÖ T√≠nh % cho m·ªói file download (gi·ªëng th√¥ng b√°o)
                            if range_total_records:
                                # T√≠nh % d·ª±a tr√™n t·ªïng s·ªë b·∫£n ghi trong kho·∫£ng (d√πng cho t·∫•t c·∫£ c√°c trang)
                                percent_per_file = range_percent / range_total_records
                                logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: D√πng range_total_records={range_total_records} ƒë·ªÉ t√≠nh % per file: {percent_per_file:.4f}%")
                            elif queue_total > 0:
                                # N·∫øu kh√¥ng c√≥ range_total_records, t√≠nh % d·ª±a tr√™n s·ªë file tr√™n trang hi·ªán t·∫°i
                                percent_per_file = range_percent / queue_total
                            else:
                                percent_per_file = 0.0
                            
                            # ‚úÖ C·∫≠p nh·∫≠t accumulated_total khi bi·∫øt s·ªë file c·∫ßn download (gi·ªëng th√¥ng b√°o)
                            if range_total_records and page_num == 1:
                                # Ch·ªâ c·∫≠p nh·∫≠t accumulated_total ·ªü trang ƒë·∫ßu ti√™n v·ªõi t·ªïng s·ªë b·∫£n ghi
                                accumulated_total_so_far += range_total_records
                                logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] C·∫≠p nh·∫≠t accumulated_total v·ªõi range_total_records={range_total_records}, accumulated_total_so_far={accumulated_total_so_far}")
                            elif not range_total_records:
                                # N·∫øu kh√¥ng c√≥ range_total_records, c·ªông s·ªë file tr√™n trang hi·ªán t·∫°i
                                accumulated_total_so_far += queue_total
                            
                            # Hi·ªÉn th·ªã t·ªïng s·ªë file s·∫Ω t·∫£i (d√πng range_total_records n·∫øu c√≥, n·∫øu kh√¥ng d√πng queue_total)
                            display_total = range_total_records if range_total_records else queue_total
                            
                            logger.info(f"‚¨áÔ∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: B·∫Øt ƒë·∫ßu download {queue_total} files (t·ªïng kho·∫£ng: {display_total}), Range %: {range_percent:.2f}%, Percent per file: {percent_per_file:.4f}%, Accumulated total: {accumulated_total_so_far}, Accumulated %: {accumulated_percent_so_far:.2f}%")
                            
                            # ‚úÖ CH·ªà publish download_start khi b·∫Øt ƒë·∫ßu kho·∫£ng m·ªõi (trang 1), kh√¥ng publish khi chuy·ªÉn trang
                            if page_num == 1:
                                yield {
                                    "type": "download_start",
                                    "total_to_download": display_total,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file s·∫Ω t·∫£i trong kho·∫£ng
                                    "current_page_download": queue_total,  # S·ªë file tr√™n trang hi·ªán t·∫°i
                                    "date_range": f"{date_range[0]} - {date_range[1]}",
                                    "range_index": range_idx + 1,
                                    "total_ranges": len(date_ranges),
                                    "accumulated_total": accumulated_total_so_far,
                                    "accumulated_downloaded": accumulated_downloaded_so_far,
                                    "range_percent": range_percent,  # % c·ªßa kho·∫£ng n√†y
                                    "accumulated_percent": int(round(min(accumulated_percent_so_far, 100))),  # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                                    "message": f"B·∫Øt ƒë·∫ßu t·∫£i {display_total} gi·∫•y n·ªôp ti·ªÅn trong kho·∫£ng {date_range[0]} - {date_range[1]}..."
                                }
                            
                            downloaded = 0
                            
                            for item_idx, item in enumerate(download_queue, 1):
                                try:
                                    logger.info(f"üì• [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang download file {item_idx}/{queue_total}: {item.get('id', 'N/A')}...")
                                    success = await self._download_single_giaynoptien(session, item, temp_dir)
                                    if success:
                                        downloaded += 1
                                        accumulated_downloaded_so_far += 1
                                        range_downloaded_so_far += 1  # ‚úÖ C·ªông d·ªìn s·ªë file ƒë√£ download trong kho·∫£ng n√†y
                                        logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ download th√†nh c√¥ng file {item_idx}/{queue_total}: {item.get('id', 'N/A')}")
                                    else:
                                        logger.warning(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Download th·∫•t b·∫°i file {item_idx}/{queue_total}: {item.get('id', 'N/A')}")
                                except Exception as download_e:
                                    logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi download file {item_idx}/{queue_total} ({item.get('id', 'N/A')}): {download_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    # Ti·∫øp t·ª•c download file ti·∫øp theo
                                    continue
                                
                                # ‚úÖ T√≠nh % t√≠ch l≈©y: % t·ª´ c√°c kho·∫£ng tr∆∞·ªõc + % c·ªßa c√°c file ƒë√£ download trong kho·∫£ng n√†y
                                # QUAN TR·ªåNG: D√πng accumulated_percent_so_far_at_range_start (kh√¥ng ph·∫£i accumulated_percent_so_far)
                                # ƒë·ªÉ tr√°nh c·ªông d·ªìn sai khi ƒë√£ c·∫≠p nh·∫≠t accumulated_percent_so_far trong v√≤ng l·∫∑p
                                if range_total_records:
                                    # T√≠nh % d·ª±a tr√™n t·ªïng s·ªë b·∫£n ghi trong kho·∫£ng
                                    # % c·ªßa kho·∫£ng n√†y = (s·ªë file ƒë√£ download / t·ªïng s·ªë file trong kho·∫£ng) * % c·ªßa kho·∫£ng
                                    range_accumulated_percent = (range_downloaded_so_far / range_total_records) * range_percent
                                    # C·ªông v·ªõi % t√≠ch l≈©y t·ª´ c√°c kho·∫£ng tr∆∞·ªõc (t·∫°i th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu kho·∫£ng n√†y)
                                    current_accumulated_percent = accumulated_percent_so_far_at_range_start + range_accumulated_percent
                                else:
                                    # T√≠nh % d·ª±a tr√™n s·ªë file tr√™n trang hi·ªán t·∫°i
                                    current_accumulated_percent = accumulated_percent_so_far_at_range_start + (downloaded * percent_per_file)
                                
                                # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                                current_accumulated_percent = min(current_accumulated_percent, 100.0)
                                
                                # ‚úÖ C·∫¨P NH·∫¨T accumulated_percent_so_far li√™n t·ª•c trong qu√° tr√¨nh download
                                accumulated_percent_so_far = current_accumulated_percent
                                
                                current_accumulated_percent = min(current_accumulated_percent, 100.0)
                                accumulated_percent_so_far = current_accumulated_percent
                                
                                display_downloaded = range_downloaded_so_far if range_total_records else downloaded
                                # Hi·ªÉn th·ªã t·ªïng s·ªë file ƒë√£ download trong kho·∫£ng (d√πng range_total_records n·∫øu c√≥)
                                display_total = range_total_records if range_total_records else queue_total
                                display_downloaded = range_downloaded_so_far if range_total_records else downloaded
                                
                                if item_idx % 5 == 0 or item_idx == queue_total:  # Log m·ªói 5 file ho·∫∑c file cu·ªëi
                                    logger.info(f"‚¨áÔ∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ download {display_downloaded}/{display_total} files (trang: {downloaded}/{queue_total}), Current accumulated %: {accumulated_percent_so_far:.2f}%")
                                
                                # ‚úÖ Yield progress event v·ªõi exception handling (gi·ªëng th√¥ng b√°o)
                                try:
                                    yield {
                                        "type": "download_progress",
                                        "downloaded": display_downloaded,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file ƒë√£ download trong kho·∫£ng
                                        "total": display_total,  # ‚úÖ Hi·ªÉn th·ªã t·ªïng s·ªë file s·∫Ω t·∫£i trong kho·∫£ng
                                        "current_page_downloaded": downloaded,  # S·ªë file ƒë√£ download tr√™n trang hi·ªán t·∫°i
                                        "current_page_total": queue_total,  # S·ªë file tr√™n trang hi·ªán t·∫°i
                                        "percent": round(display_downloaded / display_total * 100, 1) if display_total > 0 else 0,
                                        "current_item": item.get("id", ""),
                                        "accumulated_total": accumulated_total_so_far,
                                        "accumulated_downloaded": accumulated_downloaded_so_far,
                                        "accumulated_percent": int(round(accumulated_percent_so_far)),  # ‚úÖ D√πng accumulated_percent_so_far ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t
                                        "message": f"ƒê√£ t·∫£i {display_downloaded}/{display_total} ({round(display_downloaded / display_total * 100, 1) if display_total > 0 else 0}%)"
                                    }
                                except Exception as yield_e:
                                    logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi yield progress event: {yield_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    # Ti·∫øp t·ª•c download file ti·∫øp theo, kh√¥ng d·ª´ng v√¨ l·ªói yield
                                    pass
                            
                            # ‚úÖ C·∫≠p nh·∫≠t previous_row_count sau khi x·ª≠ l√Ω xong trang n√†y
                            previous_row_count = row_count
                            
                            # ‚úÖ C·∫≠p nh·∫≠t accumulated_percent_so_far sau khi download xong kho·∫£ng n√†y (ch·ªâ ·ªü trang cu·ªëi c√πng) (gi·ªëng th√¥ng b√°o)
                            # Ch·ªâ c·∫≠p nh·∫≠t khi kh√¥ng c√≤n trang ti·∫øp theo v√† ƒë√£ download h·∫øt t·∫•t c·∫£ file trong kho·∫£ng
                            if not check_pages:  # N·∫øu kh√¥ng c√≤n trang ti·∫øp theo
                                # ƒê·∫£m b·∫£o accumulated_percent_so_far ƒë·∫°t ƒë√∫ng % c·ªßa kho·∫£ng n√†y
                                # N·∫øu c√≥ range_total_records, ƒë√£ t√≠nh % d·ª±a tr√™n s·ªë file download, kh√¥ng c·∫ßn c·ªông th√™m
                                # N·∫øu kh√¥ng c√≥ range_total_records, c·ªông % c·ªßa kho·∫£ng n√†y
                                if not range_total_records:
                                    accumulated_percent_so_far += range_percent
                                # ‚úÖ ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 100%
                                accumulated_percent_so_far = min(accumulated_percent_so_far, 100.0)
                            
                            # Hi·ªÉn th·ªã t·ªïng s·ªë file ƒë√£ download trong kho·∫£ng (d√πng range_total_records n·∫øu c√≥)
                            display_total = range_total_records if range_total_records else queue_total
                            display_downloaded = range_downloaded_so_far if range_total_records else downloaded
                            
                            logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Ho√†n th√†nh download {display_downloaded}/{display_total} files (trang: {downloaded}/{queue_total}), Accumulated %: {accumulated_percent_so_far:.2f}%")
                            
                            yield {
                                "type": "download_complete",
                                "downloaded": display_downloaded,
                                "total": display_total,
                                "current_page_downloaded": downloaded,
                                "current_page_total": queue_total,
                                "accumulated_total": accumulated_total_so_far,
                                "accumulated_downloaded": accumulated_downloaded_so_far,
                                "accumulated_percent": int(round(accumulated_percent_so_far))
                            }
                        
                        # Ch·ªâ c·ªông s·ªë items h·ª£p l·ªá v√†o total_count
                        total_count += page_valid_count
                        
                        # Check pagination - next page (gi·ªëng th√¥ng b√°o)
                        try:
                            logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang ki·ªÉm tra n√∫t next...")
                            next_btn = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                            next_btn_count = await next_btn.count()
                            logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: S·ªë l∆∞·ª£ng n√∫t next: {next_btn_count}")
                            
                            if next_btn_count > 0:
                                logger.info(f"‚û°Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: C√≥ trang ti·∫øp theo, ƒëang chuy·ªÉn trang...")
                                
                                try:
                                    logger.info(f"üñ±Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang click n√∫t next...")
                                    await asyncio.wait_for(next_btn.click(), timeout=10.0)
                                    logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ click n√∫t next th√†nh c√¥ng")
                                except asyncio.TimeoutError:
                                    logger.error(f"‚è±Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Timeout khi click n√∫t next (10s)")
                                    
                                    # ‚úÖ Screenshot khi timeout click next
                                    try:
                                        screenshot_dir = get_screenshot_dir()
                                        
                                        if 'page' in locals() and page:
                                            page_screenshot = os.path.join(screenshot_dir, f"timeout_click_next_page_{range_idx + 1}_page_{page_num}.png")
                                            await page.screenshot(path=page_screenshot, full_page=True)
                                            logger.info(f"üì∏ Screenshot page saved: {page_screenshot}")
                                        
                                        if 'frame' in locals() and frame:
                                            try:
                                                frame_screenshot = os.path.join(screenshot_dir, f"timeout_click_next_frame_{range_idx + 1}_page_{page_num}.png")
                                                await frame.screenshot(path=frame_screenshot, full_page=True)
                                                logger.info(f"üì∏ Screenshot frame saved: {frame_screenshot}")
                                            except Exception as frame_screenshot_e:
                                                logger.warning(f"‚ö†Ô∏è Cannot screenshot frame: {frame_screenshot_e}")
                                        
                                        logger.info(f"üì∏ Screenshots saved to: {screenshot_dir}")
                                    except Exception as screenshot_e:
                                        logger.error(f"‚ùå Error taking screenshot: {screenshot_e}")
                                    
                                    check_pages = False
                                    continue
                                except Exception as click_e:
                                    logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi click n√∫t next: {click_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    
                                    # ‚úÖ Screenshot khi l·ªói click next
                                    try:
                                        screenshot_dir = get_screenshot_dir()
                                        
                                        if 'page' in locals() and page:
                                            page_screenshot = os.path.join(screenshot_dir, f"error_click_next_page_{range_idx + 1}_page_{page_num}.png")
                                            await page.screenshot(path=page_screenshot, full_page=True)
                                            logger.info(f"üì∏ Screenshot page saved: {page_screenshot}")
                                        
                                        if 'frame' in locals() and frame:
                                            try:
                                                frame_screenshot = os.path.join(screenshot_dir, f"error_click_next_frame_{range_idx + 1}_page_{page_num}.png")
                                                await frame.screenshot(path=frame_screenshot, full_page=True)
                                                logger.info(f"üì∏ Screenshot frame saved: {frame_screenshot}")
                                            except Exception as frame_screenshot_e:
                                                logger.warning(f"‚ö†Ô∏è Cannot screenshot frame: {frame_screenshot_e}")
                                        
                                        logger.info(f"üì∏ Screenshots saved to: {screenshot_dir}")
                                    except Exception as screenshot_e:
                                        logger.error(f"‚ùå Error taking screenshot: {screenshot_e}")
                                    
                                    check_pages = False
                                    continue
                                
                                logger.info(f"‚è≥ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê·ª£i 2 gi√¢y sau khi click...")
                                await asyncio.sleep(2)
                                logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ ƒë·ª£i xong 2 gi√¢y, b·∫Øt ƒë·∫ßu ƒë·ª£i table load...")
                                
                                # ‚úÖ Ki·ªÉm tra l·∫°i xem c√≥ trang ti·∫øp theo kh√¥ng (sau khi click)
                                try:
                                    logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang ƒë·ª£i table load cho trang {page_num + 1}...")
                                    # ‚úÖ Ki·ªÉm tra frame c√≤n t·ªìn t·∫°i kh√¥ng
                                    try:
                                        frame_url = frame.url
                                        logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Frame URL: {frame_url[:100]}...")
                                    except Exception as frame_check_e:
                                        logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Frame kh√¥ng c√≤n t·ªìn t·∫°i sau khi click: {frame_check_e}")
                                        import traceback
                                        logger.error(f"Traceback: {traceback.format_exc()}")
                                        check_pages = False
                                        continue
                                    
                                    # ƒê·ª£i table load ƒë·ªÉ ƒë·∫£m b·∫£o trang ƒë√£ chuy·ªÉn (tƒÉng timeout l√™n 15 gi√¢y)
                                    logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒêang t√¨m table locator...")
                                    try:
                                        table_body_check = frame.locator('table#data_content_onday tbody#allResultTableBody, #allResultTableBody').first
                                        logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: ƒê√£ t√¨m th·∫•y table locator, ƒëang ƒë·ª£i table visible...")
                                    except Exception as locator_e:
                                        logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] L·ªói khi t√¨m table locator: {locator_e}")
                                        import traceback
                                        logger.error(f"Traceback: {traceback.format_exc()}")
                                        check_pages = False
                                        continue
                                    
                                    try:
                                        await asyncio.wait_for(
                                            table_body_check.wait_for(timeout=15000, state='visible'),
                                            timeout=20.0  # T·ªïng timeout 20 gi√¢y
                                        )
                                        await asyncio.sleep(1)
                                        
                                        try:
                                            rows_check = table_body_check.locator('tr')
                                            row_count_check = await rows_check.count()
                                            
                                            if row_count_check == previous_row_count and previous_row_count > 0:
                                                await asyncio.sleep(2)
                                                row_count_check = await rows_check.count()
                                                
                                                if row_count_check == previous_row_count:
                                                    check_pages = False
                                                    continue
                                        except Exception as verify_e:
                                            pass
                                        
                                        # ‚úÖ Ti·∫øp t·ª•c v√≤ng l·∫∑p (check_pages v·∫´n True) - gi·ªëng th√¥ng b√°o
                                    except Exception as wait_table_e:
                                        raise
                                except asyncio.TimeoutError:
                                    await asyncio.sleep(3)
                                    try:
                                        table_body_check_retry = frame.locator('table#data_content_onday tbody#allResultTableBody, #allResultTableBody').first
                                        await asyncio.wait_for(
                                            table_body_check_retry.wait_for(timeout=15000, state='visible'),
                                            timeout=20.0
                                        )
                                        
                                        try:
                                            frames = page.frames
                                            for f in frames:
                                                if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                                    frame = f
                                                    break
                                        except Exception as refind_frame_e:
                                            pass
                                    except Exception as retry_e:
                                        try:
                                            no_data_text = frame.locator('div:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), strong:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu"), div.align-center:has-text("Kh√¥ng c√≥ d·ªØ li·ªáu")').first
                                            if await no_data_text.count() > 0:
                                                no_data_content = await no_data_text.text_content()
                                                if "Kh√¥ng c√≥ d·ªØ li·ªáu" in (no_data_content or ""):
                                                    check_pages = False
                                                    continue
                                        except Exception as no_data_check_e3:
                                            pass
                                        
                                        await asyncio.sleep(2)
                                        try:
                                            next_btn_check = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                                            next_btn_check_count = await next_btn_check.count()
                                            if next_btn_check_count == 0:
                                                check_pages = False
                                                continue
                                        except Exception as check_next_e:
                                            check_pages = False
                                            continue
                                        
                                        # Screenshot khi retry th·∫•t b·∫°i
                                        try:
                                            screenshot_dir = get_screenshot_dir()
                                            
                                            if 'page' in locals() and page:
                                                page_screenshot = os.path.join(screenshot_dir, f"table_not_load_page_{range_idx + 1}_page_{page_num}.png")
                                                await page.screenshot(path=page_screenshot, full_page=True)
                                                logger.info(f"üì∏ Screenshot page saved: {page_screenshot}")
                                            
                                            if 'frame' in locals() and frame:
                                                try:
                                                    frame_screenshot = os.path.join(screenshot_dir, f"table_not_load_frame_{range_idx + 1}_page_{page_num}.png")
                                                    await frame.screenshot(path=frame_screenshot, full_page=True)
                                                    logger.info(f"üì∏ Screenshot frame saved: {frame_screenshot}")
                                                except Exception as frame_screenshot_e:
                                                    logger.warning(f"‚ö†Ô∏è Cannot screenshot frame: {frame_screenshot_e}")
                                                
                                                try:
                                                    frame_html = await frame.content()
                                                    html_file = os.path.join(screenshot_dir, f"table_not_load_frame_{range_idx + 1}_page_{page_num}.html")
                                                    with open(html_file, 'w', encoding='utf-8') as f:
                                                        f.write(frame_html)
                                                    logger.info(f"üìÑ Frame HTML saved: {html_file}")
                                                except Exception as html_e:
                                                    logger.warning(f"‚ö†Ô∏è Cannot save frame HTML: {html_e}")
                                            
                                            logger.info(f"üì∏ Screenshots saved to: {screenshot_dir}")
                                        except Exception as screenshot_e:
                                            logger.error(f"‚ùå Error taking screenshot: {screenshot_e}")
                                        
                                        check_pages = False
                                        continue
                                    
                                except Exception as wait_e:
                                    logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num + 1} ch∆∞a load xong sau khi click next: {wait_e}")
                                    import traceback
                                    logger.error(f"Traceback: {traceback.format_exc()}")
                                    
                                    # ‚úÖ Screenshot khi table kh√¥ng load sau khi click next
                                    try:
                                        screenshot_dir = get_screenshot_dir()
                                        
                                        if 'page' in locals() and page:
                                            page_screenshot = os.path.join(screenshot_dir, f"table_not_load_page_{range_idx + 1}_page_{page_num}.png")
                                            await page.screenshot(path=page_screenshot, full_page=True)
                                            logger.info(f"üì∏ Screenshot page saved: {page_screenshot}")
                                        
                                        if 'frame' in locals() and frame:
                                            try:
                                                frame_screenshot = os.path.join(screenshot_dir, f"table_not_load_frame_{range_idx + 1}_page_{page_num}.png")
                                                await frame.screenshot(path=frame_screenshot, full_page=True)
                                                logger.info(f"üì∏ Screenshot frame saved: {frame_screenshot}")
                                            except Exception as frame_screenshot_e:
                                                logger.warning(f"‚ö†Ô∏è Cannot screenshot frame: {frame_screenshot_e}")
                                            
                                            try:
                                                frame_html = await frame.content()
                                                html_file = os.path.join(screenshot_dir, f"table_not_load_frame_{range_idx + 1}_page_{page_num}.html")
                                                with open(html_file, 'w', encoding='utf-8') as f:
                                                    f.write(frame_html)
                                                logger.info(f"üìÑ Frame HTML saved: {html_file}")
                                            except Exception as html_e:
                                                logger.warning(f"‚ö†Ô∏è Cannot save frame HTML: {html_e}")
                                        
                                        logger.info(f"üì∏ Screenshots saved to: {screenshot_dir}")
                                    except Exception as screenshot_e:
                                        logger.error(f"‚ùå Error taking screenshot: {screenshot_e}")
                                    await asyncio.sleep(3)
                                    try:
                                        table_body_check_retry = frame.locator('table#data_content_onday tbody#allResultTableBody, #allResultTableBody').first
                                        await asyncio.wait_for(
                                            table_body_check_retry.wait_for(timeout=15000, state='visible'),
                                            timeout=20.0
                                        )
                                        logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Retry th√†nh c√¥ng, trang {page_num + 1} ƒë√£ load xong")
                                        
                                        # ‚úÖ T√¨m l·∫°i frame m·ªõi sau khi retry (iframe c√≥ th·ªÉ reload)
                                        try:
                                            frames = page.frames
                                            for f in frames:
                                                if 'thuedientu.gdt.gov.vn' in f.url and 'etaxnnt' in f.url:
                                                    frame = f  # C·∫≠p nh·∫≠t frame object m·ªõi
                                                    logger.info(f"üîÑ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] ƒê√£ t√¨m l·∫°i frame m·ªõi sau retry: {frame.url[:100]}...")
                                                    break
                                        except Exception as refind_frame_e:
                                            logger.warning(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Kh√¥ng th·ªÉ t√¨m l·∫°i frame m·ªõi sau retry: {refind_frame_e}")
                                    except Exception as retry_e:
                                        logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Retry v·∫´n th·∫•t b·∫°i: {retry_e}")
                                        await asyncio.sleep(2)
                                        try:
                                            next_btn_check = frame.locator('img[src="/etaxnnt/static/images/pagination_right.gif"]')
                                            next_btn_check_count = await next_btn_check.count()
                                            logger.info(f"üîç [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Sau retry, s·ªë l∆∞·ª£ng n√∫t next: {next_btn_check_count}")
                                            if next_btn_check_count == 0:
                                                logger.info(f"üèÅ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Sau khi click, kh√¥ng c√≤n n√∫t next, k·∫øt th√∫c ph√¢n trang")
                                                check_pages = False
                                            else:
                                                logger.warning(f"‚ö†Ô∏è [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] V·∫´n c√≤n n√∫t next nh∆∞ng table kh√¥ng load, k·∫øt th√∫c ph√¢n trang ƒë·ªÉ tr√°nh hang")
                                                check_pages = False
                                        except Exception as check_e:
                                            logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] L·ªói khi ki·ªÉm tra n√∫t next sau retry: {check_e}")
                                            check_pages = False
                            else:
                                logger.info(f"üèÅ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: Kh√¥ng c√≤n trang ti·∫øp theo")
                                check_pages = False  # ‚úÖ CH·ªà set False khi kh√¥ng c√≤n n√∫t next
                            
                            # ‚úÖ Log tr·∫°ng th√°i sau khi x·ª≠ l√Ω pagination
                            logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Sau khi x·ª≠ l√Ω pagination: check_pages={check_pages}, page_num={page_num}")
                        except Exception as pagination_e:
                            logger.error(f"‚ùå [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Trang {page_num}: L·ªói khi x·ª≠ l√Ω ph√¢n trang: {pagination_e}")
                            import traceback
                            logger.error(f"Traceback: {traceback.format_exc()}")
                            check_pages = False
                        
                        # ‚úÖ Log tr∆∞·ªõc khi ti·∫øp t·ª•c v√≤ng l·∫∑p (gi·ªëng th√¥ng b√°o)
                        if check_pages:
                            logger.info(f"üîÑ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Ti·∫øp t·ª•c v√≤ng l·∫∑p pagination, s·∫Ω x·ª≠ l√Ω trang ti·∫øp theo...")
                        else:
                            logger.info(f"üõë [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] D·ª´ng v√≤ng l·∫∑p pagination, ƒë√£ x·ª≠ l√Ω xong {page_num} trang")
                    
                    # ‚úÖ ƒêi·ªÅu ch·ªânh accumulated_total_so_far sau khi download xong kho·∫£ng n√†y
                    # N·∫øu ƒë√£ c·ªông range_total_records ·ªü trang ƒë·∫ßu, nh∆∞ng s·ªë file th·ª±c s·ª± download √≠t h∆°n (do duplicate),
                    # th√¨ ƒëi·ªÅu ch·ªânh l·∫°i accumulated_total_so_far
                    if range_total_records:
                        # ƒê√£ c·ªông range_total_records v√†o accumulated_total_so_far ·ªü trang ƒë·∫ßu
                        # Nh∆∞ng s·ªë file th·ª±c s·ª± download l√† range_downloaded_so_far
                        # ƒêi·ªÅu ch·ªânh: accumulated_total_so_far = accumulated_total_so_far - range_total_records + range_downloaded_so_far
                        actual_files_downloaded = range_downloaded_so_far
                        if actual_files_downloaded < range_total_records:
                            # C√≥ duplicate files, ƒëi·ªÅu ch·ªânh accumulated_total_so_far
                            adjustment = range_total_records - actual_files_downloaded
                            accumulated_total_so_far -= adjustment
                            logger.info(f"üìä [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] ƒêi·ªÅu ch·ªânh accumulated_total: -{adjustment} (duplicate files), actual={actual_files_downloaded}, expected={range_total_records}, accumulated_total_so_far={accumulated_total_so_far}")
                    
                    logger.info(f"‚úÖ [GIAYNOPTIEN] [{range_idx + 1}/{len(date_ranges)}] Ho√†n th√†nh x·ª≠ l√Ω kho·∫£ng {date_range[0]} - {date_range[1]}: T·ªïng {range_downloaded_so_far if range_total_records else total_count} items, Accumulated %: {accumulated_percent_so_far:.2f}%")
                
                except Exception as e:
                    logger.error(f"Error processing date range {date_range}: {e}")
                    import traceback
                    logger.error(f"Traceback: {traceback.format_exc()}")
                    
                    # ‚úÖ Screenshot khi c√≥ l·ªói (l∆∞u v√†o D:\tool-gotax\tool-gotax\tool-go-soft\screenshots)
                    try:
                        # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng: tool-go-soft/screenshots/giaynoptien_...
                        screenshot_dir = get_screenshot_dir()
                        logger.info(f"üì∏ Screenshot directory: {screenshot_dir}")
                        
                        # Screenshot page (n·∫øu c√≥)
                        if 'page' in locals() and page:
                            try:
                                page_screenshot = os.path.join(screenshot_dir, f"01_error_page_range_{range_idx + 1}.png")
                                await page.screenshot(path=page_screenshot, full_page=True)
                                logger.info(f"üì∏ Screenshot page saved: {page_screenshot}")
                            except Exception as page_e:
                                logger.warning(f"‚ö†Ô∏è Cannot screenshot page: {page_e}")
                        
                        # Screenshot frame (n·∫øu c√≥)
                        if 'frame' in locals() and frame:
                            try:
                                frame_screenshot = os.path.join(screenshot_dir, f"02_error_frame_range_{range_idx + 1}.png")
                                await frame.screenshot(path=frame_screenshot, full_page=True)
                                logger.info(f"üì∏ Screenshot frame saved: {frame_screenshot}")
                            except Exception as frame_e:
                                logger.warning(f"‚ö†Ô∏è Cannot screenshot frame: {frame_e}")
                            
                            # L·∫•y HTML c·ªßa frame ƒë·ªÉ debug
                            try:
                                frame_html = await frame.content()
                                html_file = os.path.join(screenshot_dir, f"03_error_frame_range_{range_idx + 1}.html")
                                with open(html_file, 'w', encoding='utf-8') as f:
                                    f.write(frame_html)
                                logger.info(f"üìÑ Frame HTML saved: {html_file}")
                            except Exception as html_e:
                                logger.warning(f"‚ö†Ô∏è Cannot save frame HTML: {html_e}")
                        
                        # L·∫•y HTML c·ªßa page ƒë·ªÉ debug (n·∫øu c√≥)
                        if 'page' in locals() and page:
                            try:
                                page_html = await page.content()
                                html_file = os.path.join(screenshot_dir, f"04_error_page_range_{range_idx + 1}.html")
                                with open(html_file, 'w', encoding='utf-8') as f:
                                    f.write(page_html)
                                logger.info(f"üìÑ Page HTML saved: {html_file}")
                            except Exception as html_e:
                                logger.warning(f"‚ö†Ô∏è Cannot save page HTML: {html_e}")
                        
                        logger.info(f"üì∏ Screenshots saved to: {screenshot_dir}")
                    except Exception as screenshot_e:
                        logger.error(f"‚ùå Error taking screenshot: {screenshot_e}")
                        import traceback
                        logger.error(f"Screenshot error traceback: {traceback.format_exc()}")
                    
                    yield {
                        "type": "warning", 
                        "message": f"L·ªói x·ª≠ l√Ω kho·∫£ng {date_range}: {str(e)}",
                        "accumulated_total": accumulated_total_so_far,
                        "accumulated_downloaded": accumulated_downloaded_so_far,
                        "accumulated_percent": int(round(accumulated_percent_so_far))
                    }
                    continue
            
            # Parse downloaded files v√† rename
            parsed_results = []
            files_in_temp_dir = os.listdir(temp_dir) if os.path.exists(temp_dir) else []
            logger.info(f"crawl_giay_nop_tien: Found {len(files_in_temp_dir)} files in temp_dir")
            
            # ‚úÖ Kh√¥ng log screenshots khi kh√¥ng c√≥ l·ªói (ch·ªâ log khi c√≥ l·ªói th·ª±c s·ª±)
            
            if files_in_temp_dir:
                nnn = 0
                
                for file_name in files_in_temp_dir:
                    file_path = os.path.join(temp_dir, file_name)
                    if not os.path.isfile(file_path):
                        continue
                    
                    try:
                        # Parse XML ƒë·ªÉ l·∫•y th√¥ng tin
                        with open(file_path, 'r', encoding='utf-8') as f:
                            soup = BeautifulSoup(f, 'html.parser')
                        
                        ma_ndkt = soup.find('ma_ndkt')
                        ma_ndkt = ma_ndkt.text if ma_ndkt else ""
                        
                        ngay_lap = soup.find('ngay_lap')
                        ngay_lap = ngay_lap.text if ngay_lap else ""
                        ngay_lap = ngay_lap.replace("/", "-")
                        
                        ma_chuong = soup.find('ma_chuong')
                        ma_chuong = ma_chuong.text if ma_chuong else ""
                        
                        ky_thue = soup.find('ky_thue')
                        ky_thue = ky_thue.text if ky_thue else ""
                        ky_thue = ky_thue.replace("/", "-")
                        
                        # Rename file theo format
                        nnn += 1
                        new_file_name = f"{ma_ndkt} - {ma_chuong} - Kynopthue - {ky_thue} - Ngaynopthue - {ngay_lap} [{nnn}].xml"
                        new_file_name = self._remove_accents(new_file_name)
                        
                        # Rename file
                        new_file_path = os.path.join(temp_dir, new_file_name)
                        if os.path.exists(file_path):
                            try:
                                os.rename(file_path, new_file_path)
                                file_name = new_file_name
                                file_path = new_file_path
                            except Exception as rename_err:
                                logger.warning(f"Error renaming {file_name}: {rename_err}")
                        
                        parsed_results.append({
                            "ma_noi_dung_kinh_te": ma_ndkt,
                            "ngay_lap": ngay_lap.replace("-", "/") if ngay_lap else "",  # Tr·∫£ v·ªÅ format g·ªëc
                            "ma_chuong": ma_chuong,
                            "ky_thue": ky_thue.replace("-", "/") if ky_thue else ""  # Tr·∫£ v·ªÅ format g·ªëc
                        })
                        
                        file_size = os.path.getsize(file_path)
                        total_size += file_size
                        files_info.append({"name": file_name, "size": file_size})
                    except Exception as e:
                        logger.warning(f"Error parsing/renaming file {file_name}: {e}")
                        # N·∫øu parse l·ªói, v·∫´n th√™m v√†o files_info v·ªõi t√™n c≈©
                        try:
                            file_size = os.path.getsize(file_path)
                            total_size += file_size
                            files_info.append({"name": file_name, "size": file_size})
                        except:
                            pass
                        continue
                
                download_id = str(uuid.uuid4())
                zip_filename = f"giaynoptien_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                zip_file_path = os.path.join(self.ZIP_STORAGE_DIR, f"{download_id}.zip")
                
                final_files = os.listdir(temp_dir)
                logger.info(f"crawl_giay_nop_tien: Found {len(final_files)} files in temp_dir")
                logger.info(f"crawl_giay_nop_tien: Creating ZIP from {len(final_files)} files")
                
                if final_files:
                    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                        for file_name in final_files:
                            file_path = os.path.join(temp_dir, file_name)
                            if os.path.isfile(file_path):
                                zf.write(file_path, file_name)
                                logger.debug(f"Added to ZIP: {file_name}")
                    
                    with open(zip_file_path, 'rb') as f:
                        zip_base64 = base64.b64encode(f.read()).decode('utf-8')
                    
                    logger.info(f"‚úÖ ƒê√£ t·∫°o file ZIP: {zip_filename} (download_id: {download_id})")
                    
                    try:
                        from shared.redis_client import get_redis_client
                        redis_client = get_redis_client()
                        redis_key = f"session:{session_id}:download_id"
                        redis_client.setex(redis_key, 3600, download_id.encode('utf-8'))
                    except Exception as redis_err:
                        logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u download_id v√†o Redis: {redis_err}")
                else:
                    zip_base64 = None
                    download_id = None
                    logger.warning("crawl_giay_nop_tien: No files to add to ZIP")
            else:
                zip_base64 = None
                download_id = None
                zip_filename = f"giaynoptien_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
                logger.warning("crawl_giay_nop_tien: No files in temp_dir")
            
            actual_files_count = len(files_info)
            actual_results_count = len(parsed_results)
            
            yield {
                "type": "complete",
                "total": actual_files_count,  # S·ªë file th·ª±c t·∫ø trong ZIP
                "results_count": actual_results_count,  # S·ªë items ƒë√£ parse
                "total_rows_processed": total_count,  # S·ªë rows ƒë√£ x·ª≠ l√Ω (ƒë·ªÉ debug)
                "results": parsed_results,
                "files": files_info,
                "files_count": actual_files_count,
                "total_size": total_size,
                "zip_base64": zip_base64,
                "zip_filename": zip_filename,
                "download_id": download_id
            }
            
        except Exception as e:
            logger.error(f"Error in crawl_giay_nop_tien: {e}")
            error_msg = str(e)
            # Ki·ªÉm tra session timeout
            if "timeout" in error_msg.lower() or "phi√™n giao d·ªãch" in error_msg.lower():
                yield {"type": "error", "error": "Phi√™n giao d·ªãch h·∫øt h·∫°n. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "SESSION_EXPIRED"}
            else:
                yield {"type": "error", "error": f"L·ªói khi tra c·ª©u gi·∫•y n·ªôp ti·ªÅn: {error_msg}", "error_code": "CRAWL_ERROR"}
        
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    _gnt_download_counter = 0
    
    async def _download_single_giaynoptien(self, session: SessionData, item: Dict, temp_dir: str, max_retries: int = 2) -> bool:
        page = session.page
        id_gnt = item["id"]
        
        for retry in range(max_retries + 1):
            try:
                # N·∫øu ƒë√£ c√≥ link_locator, d√πng tr·ª±c ti·∫øp
                if "link_locator" in item:
                    download_link = item["link_locator"]
                else:
                    # Fallback: t√¨m l·∫°i link t·ª´ row v√† col_index
                    row = item.get("row")
                    col_idx = item.get("col_index")
                    if row and col_idx is not None:
                        cols = row.locator('td')
                        links = cols.nth(col_idx).locator('a[href*="downloadGNT"]')
                        link_count = await links.count()
                        
                        # N·∫øu c√≥ 2 links th√¨ click link th·ª© 2, n·∫øu kh√¥ng th√¨ click link ƒë·∫ßu
                        if link_count >= 2:
                            download_link = links.nth(1)
                        elif link_count >= 1:
                            download_link = links.first
                        else:
                            logger.warning(f"No download link found for {id_gnt}")
                            return False
                    else:
                        logger.warning(f"Missing link_locator or row/col_index for {id_gnt}")
                        return False
                
                # Download file
                async with page.expect_download(timeout=30000) as download_info:
                    await download_link.click()
                
                download = await download_info.value
                
                # L∆∞u file v·ªõi t√™n t·∫°m unique
                TaxCrawlerService._gnt_download_counter += 1
                temp_name = f"chungtu_{id_gnt}_{TaxCrawlerService._gnt_download_counter}.xml"
                save_path = os.path.join(temp_dir, temp_name)
                await download.save_as(save_path)
                
                # Verify file exists and has content
                await asyncio.sleep(0.3)
                if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                    logger.info(f"Downloaded giaynoptien {id_gnt} -> {temp_name}")
                    return True
                else:
                    raise Exception("File empty or not saved")
                    
            except Exception as e:
                logger.warning(f"Error downloading giaynoptien {id_gnt} (attempt {retry + 1}/{max_retries + 1}): {e}")
                if retry < max_retries:
                    await asyncio.sleep(1)  # Wait before retry
        
        return False
    
    async def convert_xml_to_xlsx(self, xml_files_base64: str) -> Dict[str, Any]:
        temp_dir = tempfile.mkdtemp()
        
        try:
            # Gi·∫£i n√©n ZIP
            zip_bytes = base64.b64decode(xml_files_base64)
            zip_buffer = BytesIO(zip_bytes)
            
            with zipfile.ZipFile(zip_buffer, 'r') as zf:
                zf.extractall(temp_dir)
            
            # T·∫°o workbook
            workbook = Workbook()
            worksheet = workbook.active
            
            # Headers
            headers = [
                'T√™n', 'K·ª≥ t√≠nh thu·∫ø Th√°ng/Qu√Ω', 'L·∫ßn', 'NƒÉm',
                'VAT ƒë·∫ßu k·ª≥', 'Gi√° tr·ªã HH mua v√†o', 'VAT mua v√†o',
                'VAT ƒë∆∞·ª£c kh·∫•u tr·ª´ k·ª≥ n√†y', 'Gi√° tr·ªã HH b√°n ra', 'VAT b√°n ra',
                'ƒêi·ªÅu ch·ªânh tƒÉng', 'ƒêi·ªÅu ch·ªânh gi·∫£m', 'Thu·∫ø v√£ng lai ngo·∫°i t·ªânh',
                'VAT c√≤n ph·∫£i n·ªôp', 'VAT c√≤n ƒë∆∞·ª£c kh·∫•u tr·ª´ chuy·ªÉn k·ª≥ sau'
            ]
            worksheet.append(headers)
            
            # Parse each XML file
            for filename in os.listdir(temp_dir):
                if not filename.endswith('.xml'):
                    continue
                
                file_path = os.path.join(temp_dir, filename)
                
                try:
                    tree = ET.parse(file_path)
                    root = tree.getroot()
                    
                    # Get namespace
                    namespace = {'ns0': root.tag.split('}')[0][1:]} if '}' in root.tag else {}
                    
                    # Extract data
                    def get_element_text(tag):
                        if namespace:
                            elem = root.find(f'.//ns0:{tag}', namespace)
                        else:
                            elem = root.find(f'.//{tag}')
                        return elem.text if elem is not None else ''
                    
                    ky_kkhai = get_element_text('kyKKhai')
                    ky = ky_kkhai.split("/")[0] if "/" in ky_kkhai else ''
                    nam = ky_kkhai.split("/")[1] if "/" in ky_kkhai else ''
                    
                    try:
                        so_lan = filename.split("-")[2] + " " + filename.split("-")[3]
                    except:
                        so_lan = ""
                    
                    row = [
                        filename,
                        ky,
                        so_lan,
                        nam,
                        get_element_text('ct22'),
                        get_element_text('ct23'),
                        get_element_text('ct24'),
                        get_element_text('ct25'),
                        get_element_text('ct34'),
                        get_element_text('ct35'),
                        get_element_text('ct38'),
                        get_element_text('ct37'),
                        get_element_text('ct39'),
                        get_element_text('ct40'),
                        get_element_text('ct43'),
                    ]
                    
                    worksheet.append(row)
                    
                except Exception as e:
                    logger.error(f"Error parsing XML {filename}: {e}")
                    continue
            
            # Format worksheet
            header_font = Font(bold=True)
            thin_border = Border(
                left=Side(style='thin'),
                right=Side(style='thin'),
                top=Side(style='thin'),
                bottom=Side(style='thin')
            )
            
            for col in range(1, worksheet.max_column + 1):
                cell = worksheet.cell(row=1, column=col)
                cell.font = header_font
                cell.border = thin_border
            
            for row in range(2, worksheet.max_row + 1):
                for col in range(1, worksheet.max_column + 1):
                    cell = worksheet.cell(row=row, column=col)
                    cell.border = thin_border
                    
                    if col >= 5:
                        if cell.value:
                            try:
                                cell.value = float(cell.value)
                                cell.number_format = FORMAT_NUMBER_COMMA_SEPARATED1
                            except ValueError:
                                pass
            
            for col in range(1, worksheet.max_column + 1):
                header_text = worksheet.cell(row=1, column=col).value
                worksheet.column_dimensions[get_column_letter(col)].width = len(str(header_text)) + 5
            
            xlsx_buffer = BytesIO()
            workbook.save(xlsx_buffer)
            xlsx_base64 = base64.b64encode(xlsx_buffer.getvalue()).decode('utf-8')
            
            return {
                "success": True,
                "xlsx_base64": xlsx_base64,
                "row_count": worksheet.max_row - 1
            }
            
        except Exception as e:
            logger.error(f"Error in convert_xml_to_xlsx: {e}")
            return {"success": False, "error": str(e)}
        
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    async def get_tokhai_types(self, session_id: str) -> Dict[str, Any]:
        session = self.session_manager.get_session(session_id)
        if not session:
            return {"success": False, "error": "Session not found"}
        
        if not session.is_logged_in:
            return {"success": False, "error": "Not logged in"}
        
        page = session.page
        
        try:
            # Navigate ƒë·∫øn trang tra c·ª©u t·ªù khai b·∫±ng JavaScript (nhanh h∆°n)
            success = await self._navigate_to_tokhai_page(page, session.dse_session_id)
            
            if not success:
                return {"success": False, "error": "Kh√¥ng th·ªÉ navigate ƒë·∫øn trang tra c·ª©u. Vui l√≤ng th·ª≠ l·∫°i."}
            
            frame = page.frame('mainframe')
            if not frame:
                return {"success": False, "error": "Kh√¥ng t√¨m th·∫•y mainframe"}
            
            # T√¨m dropdown id="maTKhai"
            select = frame.locator('#maTKhai')
            await select.wait_for(timeout=10000)
            
            options = await select.locator('option').all()
            tokhai_types = []
            
            # Th√™m option "T·∫•t c·∫£" v√†o ƒë·∫ßu danh s√°ch
            tokhai_types.append({
                "value": "00",
                "label": "--T·∫•t c·∫£--"
            })
            
            for option in options:
                value = await option.get_attribute('value')
                text = await option.text_content()
                # B·ªè qua header groups (value="--") v√† "T·∫•t c·∫£" (value="00") v√¨ ƒë√£ th√™m ·ªü tr√™n
                if value and value not in ['--', '00'] and text:
                    tokhai_types.append({
                        "value": value,
                        "label": text.strip()
                    })
            
            return {
                "success": True,
                "tokhai_types": tokhai_types
            }
            
        except Exception as e:
            logger.error(f"Error getting tokhai types: {e}")
            return {"success": False, "error": str(e)}
    
    async def crawl_batch(
        self,
        session_id: str,
        start_date: str,
        end_date: str,
        crawl_types: List[str],
        tokhai_type: str = "00"
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Crawl nhi·ªÅu lo·∫°i d·ªØ li·ªáu ƒë·ªìng th·ªùi (t·ªù khai, th√¥ng b√°o, gi·∫•y n·ªôp ti·ªÅn)
        
        Args:
            session_id: Session ID ƒë√£ ƒëƒÉng nh·∫≠p
            start_date: Ng√†y b·∫Øt ƒë·∫ßu (dd/mm/yyyy)
            end_date: Ng√†y k·∫øt th√∫c (dd/mm/yyyy)
            crawl_types: Danh s√°ch lo·∫°i c·∫ßn crawl ["tokhai", "thongbao", "giaynoptien"]
            tokhai_type: Lo·∫°i t·ªù khai (ch·ªâ √°p d·ª•ng n·∫øu crawl tokhai)
        
        Yields:
            Dict v·ªõi progress v√† k·∫øt qu·∫£ t·ª´ng lo·∫°i
        """
        session = self.session_manager.get_session(session_id)
        if not session:
            yield {"type": "error", "error": "Session kh√¥ng t·ªìn t·∫°i ho·∫∑c ƒë√£ h·∫øt h·∫°n", "error_code": "SESSION_NOT_FOUND"}
            return
        
        if not session.is_logged_in:
            yield {"type": "error", "error": "Ch∆∞a ƒëƒÉng nh·∫≠p. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i.", "error_code": "NOT_LOGGED_IN"}
            return
        
        # Validate crawl_types
        valid_types = ["tokhai", "thongbao", "giaynoptien"]
        crawl_types = [t for t in crawl_types if t in valid_types]
        
        if not crawl_types:
            yield {"type": "error", "error": "Kh√¥ng c√≥ lo·∫°i crawl h·ª£p l·ªá. Ch·ªçn t·ª´: tokhai, thongbao, giaynoptien", "error_code": "INVALID_CRAWL_TYPES"}
            return
        
        total_types = len(crawl_types)
        yield {
            "type": "batch_start",
            "message": f"B·∫Øt ƒë·∫ßu crawl {total_types} lo·∫°i d·ªØ li·ªáu: {', '.join(crawl_types)}",
            "crawl_types": crawl_types,
            "total_types": total_types
        }
        
        # K·∫øt qu·∫£ t·ªïng h·ª£p
        batch_results = {
            "tokhai": None,
            "thongbao": None,
            "giaynoptien": None
        }
        
        # X·ª≠ l√Ω t·ª´ng lo·∫°i tu·∫ßn t·ª± (v√¨ c√πng d√πng 1 session/page)
        for idx, crawl_type in enumerate(crawl_types):
            yield {
                "type": "batch_progress",
                "current_type": crawl_type,
                "type_index": idx + 1,
                "total_types": total_types,
                "message": f"ƒêang crawl {crawl_type} ({idx + 1}/{total_types})..."
            }
            
            try:
                if crawl_type == "tokhai":
                    # Crawl t·ªù khai - th·ª© t·ª±: session_id, tokhai_type, start_date, end_date
                    async for result in self.crawl_tokhai(session_id, tokhai_type, start_date, end_date):
                        # Forward progress events v·ªõi prefix
                        if result.get("type") == "complete":
                            batch_results["tokhai"] = result
                            yield {
                                "type": "type_complete",
                                "crawl_type": "tokhai",
                                "result": result
                            }
                        elif result.get("type") == "zip_data":
                            # L∆∞u zip_data v√†o batch_results
                            if batch_results.get("tokhai"):
                                batch_results["tokhai"]["zip_base64"] = result.get("zip_base64")
                            # Forward event
                            yield {
                                **result,
                                "crawl_type": "tokhai"
                            }
                        elif result.get("type") == "error":
                            yield {
                                "type": "type_error",
                                "crawl_type": "tokhai",
                                "error": result.get("error")
                            }
                        else:
                            # Forward info/progress events
                            yield {
                                **result,
                                "crawl_type": "tokhai"
                            }
                
                elif crawl_type == "thongbao":
                    # Crawl th√¥ng b√°o
                    async for result in self.crawl_thongbao(session_id, start_date, end_date):
                        if result.get("type") == "complete":
                            batch_results["thongbao"] = result
                            # N·∫øu c√≥ zip_base64 trong complete event, gi·ªØ l·∫°i
                            yield {
                                "type": "type_complete",
                                "crawl_type": "thongbao",
                                "result": result
                            }
                        elif result.get("type") == "zip_data":
                            if batch_results.get("thongbao"):
                                batch_results["thongbao"]["zip_base64"] = result.get("zip_base64")
                            yield {
                                **result,
                                "crawl_type": "thongbao"
                            }
                        elif result.get("type") == "error":
                            yield {
                                "type": "type_error",
                                "crawl_type": "thongbao",
                                "error": result.get("error")
                            }
                        else:
                            yield {
                                **result,
                                "crawl_type": "thongbao"
                            }
                
                elif crawl_type == "giaynoptien":
                    # Crawl gi·∫•y n·ªôp ti·ªÅn
                    async for result in self.crawl_giay_nop_tien(session_id, start_date, end_date):
                        if result.get("type") == "complete":
                            batch_results["giaynoptien"] = result
                            yield {
                                "type": "type_complete",
                                "crawl_type": "giaynoptien",
                                "result": result
                            }
                        elif result.get("type") == "zip_data":
                            if batch_results.get("giaynoptien"):
                                batch_results["giaynoptien"]["zip_base64"] = result.get("zip_base64")
                            yield {
                                **result,
                                "crawl_type": "giaynoptien"
                            }
                        elif result.get("type") == "error":
                            yield {
                                "type": "type_error",
                                "crawl_type": "giaynoptien",
                                "error": result.get("error")
                            }
                        else:
                            yield {
                                **result,
                                "crawl_type": "giaynoptien"
                            }
                
            except Exception as e:
                logger.error(f"Error crawling {crawl_type}: {e}")
                yield {
                    "type": "type_error",
                    "crawl_type": crawl_type,
                    "error": str(e)
                }
        
        # T·ªïng h·ª£p k·∫øt qu·∫£ cu·ªëi c√πng
        # Merge t·∫•t c·∫£ ZIP files th√†nh 1 ZIP duy nh·∫•t
        merged_zip_buffer = BytesIO()
        total_files = 0
        total_size = 0
        all_results = []
        
        with zipfile.ZipFile(merged_zip_buffer, 'w', zipfile.ZIP_DEFLATED) as merged_zip:
            for crawl_type, result in batch_results.items():
                if result and result.get("zip_base64"):
                    try:
                        # Decode ZIP c·ªßa t·ª´ng lo·∫°i
                        type_zip_bytes = base64.b64decode(result["zip_base64"])
                        type_zip_buffer = BytesIO(type_zip_bytes)
                        
                        with zipfile.ZipFile(type_zip_buffer, 'r') as type_zip:
                            for file_info in type_zip.filelist:
                                # Th√™m prefix folder theo lo·∫°i
                                new_name = f"{crawl_type}/{file_info.filename}"
                                file_data = type_zip.read(file_info.filename)
                                merged_zip.writestr(new_name, file_data)
                                total_files += 1
                                total_size += len(file_data)
                        
                        # Collect results
                        if result.get("results"):
                            for r in result["results"]:
                                r["crawl_type"] = crawl_type
                                all_results.append(r)
                    except Exception as e:
                        logger.warning(f"Error merging ZIP for {crawl_type}: {e}")
        
        # Encode merged ZIP
        merged_zip_base64 = base64.b64encode(merged_zip_buffer.getvalue()).decode('utf-8') if total_files > 0 else None
        zip_filename = f"batch_crawl_{start_date.replace('/', '')}_{end_date.replace('/', '')}.zip"
        
        yield {
            "type": "batch_complete",
            "message": f"Ho√†n th√†nh crawl {total_types} lo·∫°i d·ªØ li·ªáu",
            "total_files": total_files,
            "total_size": total_size,
            "results": all_results,
            "batch_results": {
                crawl_type: {
                    "total": result.get("total", 0) if result else 0,
                    "files_count": result.get("files_count", 0) if result else 0,
                    "total_size": result.get("total_size", 0) if result else 0,
                    "zip_base64": result.get("zip_base64") if result else None,
                    "zip_filename": result.get("zip_filename") if result else None,
                    "results": result.get("results", []) if result else []
                }
                for crawl_type, result in batch_results.items()
                if crawl_type in crawl_types
            },
            "zip_base64": merged_zip_base64,
            "zip_filename": zip_filename
        }
    
    async def _extract_pagination_info(self, frame) -> Optional[Dict[str, int]]:
        """
        Extract pagination info t·ª´ gi·∫•y n·ªôp ti·ªÅn page.
        Format: "Trang 1/<b>2</b>. C√≥ <b>11</b> b·∫£n ghi."
        Returns: {"current_page": 1, "total_pages": 2, "total_records": 11} ho·∫∑c None
        """
        try:
            # T√¨m pagination div: id="currAcc" v·ªõi class "table_headerto"
            pagination_div = frame.locator('#currAcc.table_headerto, #currAcc, .table_headerto')
            if await pagination_div.count() == 0:
                return None
            
            pagination_text = await pagination_div.text_content()
            if not pagination_text:
                return None
            
            # Parse: "Trang 1/<b>2</b>. C√≥ <b>11</b> b·∫£n ghi."
            # Ho·∫∑c: "Trang 1/2. C√≥ 11 b·∫£n ghi."
            # T√¨m "Trang X/Y" ho·∫∑c "Trang X/<b>Y</b>"
            page_match = re.search(r'Trang\s+(\d+)\s*/\s*(?:<b>)?(\d+)(?:</b>)?', pagination_text)
            if not page_match:
                return None
            
            current_page = int(page_match.group(1))
            total_pages = int(page_match.group(2))
            
            # T√¨m "C√≥ X b·∫£n ghi" ho·∫∑c "C√≥ <b>X</b> b·∫£n ghi"
            records_match = re.search(r'C√≥\s+(?:<b>)?(\d+)(?:</b>)?\s+b·∫£n ghi', pagination_text)
            total_records = int(records_match.group(1)) if records_match else 0
            
            return {
                "current_page": current_page,
                "total_pages": total_pages,
                "total_records": total_records
            }
        except Exception as e:
            logger.warning(f"Error extracting pagination info: {e}")
            return None
    
    async def _navigate_to_page(self, frame, page_num: int) -> bool:
        """
        Navigate ƒë·∫øn trang page_num c·ªßa gi·∫•y n·ªôp ti·ªÅn.
        C√≥ th·ªÉ d√πng link ho·∫∑c JavaScript gotoPage().
        """
        try:
            # Th·ª≠ click v√†o link s·ªë trang tr∆∞·ªõc (n·∫øu c√≥)
            # Link format: <a href="...&pn=2">2</a>
            page_link = frame.locator(f'a[href*="pn={page_num}"]:has-text("{page_num}")')
            if await page_link.count() > 0:
                await page_link.first.click()
                await asyncio.sleep(1)
                
                # Verify navigation: check xem c√≥ ƒë√∫ng trang kh√¥ng
                pagination_info = await self._extract_pagination_info(frame)
                if pagination_info and pagination_info["current_page"] == page_num:
                    logger.info(f"‚úÖ Navigated to page {page_num} via link")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è Navigation verification failed: expected page {page_num}, got {pagination_info.get('current_page') if pagination_info else 'unknown'}")
            
            # N·∫øu link kh√¥ng work, th·ª≠ d√πng JavaScript gotoPage()
            try:
                # T√¨m input field: id="gotoPageNO_objectList"
                goto_input = frame.locator('#gotoPageNO_objectList')
                if await goto_input.count() > 0:
                    # Fill page number
                    await goto_input.fill(str(page_num))
                    await asyncio.sleep(0.3)
                    
                    # Click n√∫t "go" (img v·ªõi src="/etaxnnt/static/images/pagination_go.gif")
                    go_btn = frame.locator('a[href*="gotoPage"] img[src*="pagination_go"], a:has(img[src*="pagination_go"])')
                    if await go_btn.count() > 0:
                        await go_btn.first.click()
                        await asyncio.sleep(1)
                        
                        # Verify navigation
                        pagination_info = await self._extract_pagination_info(frame)
                        if pagination_info and pagination_info["current_page"] == page_num:
                            logger.info(f"‚úÖ Navigated to page {page_num} via JavaScript gotoPage")
                            return True
            except Exception as js_e:
                logger.debug(f"JavaScript gotoPage failed: {js_e}")
            
            # N·∫øu c·∫£ 2 c√°ch ƒë·ªÅu kh√¥ng work, th·ª≠ click v√†o n√∫t "next" (pagination_right.gif) nhi·ªÅu l·∫ßn
            # Nh∆∞ng c√°ch n√†y kh√¥ng ch√≠nh x√°c, ch·ªâ d√πng khi kh√¥ng c√≥ c√°ch n√†o kh√°c
            current_page = 1
            pagination_info = await self._extract_pagination_info(frame)
            if pagination_info:
                current_page = pagination_info["current_page"]
            
            if current_page < page_num:
                # Click n√∫t "next" (pagination_right.gif) cho ƒë·∫øn khi ƒë·∫øn ƒë√∫ng trang
                next_btn = frame.locator('a[href*="pn="] img[src*="pagination_right"], a:has(img[src*="pagination_right"])')
                clicks_needed = page_num - current_page
                for _ in range(min(clicks_needed, 10)):  # Gi·ªõi h·∫°n t·ªëi ƒëa 10 l·∫ßn click
                    if await next_btn.count() > 0:
                        await next_btn.first.click()
                        await asyncio.sleep(1)
                        
                        # Check xem ƒë√£ ƒë·∫øn ƒë√∫ng trang ch∆∞a
                        pagination_info = await self._extract_pagination_info(frame)
                        if pagination_info and pagination_info["current_page"] == page_num:
                            logger.info(f"‚úÖ Navigated to page {page_num} via next button")
                            return True
                        elif pagination_info and pagination_info["current_page"] > page_num:
                            # ƒê√£ v∆∞·ª£t qu√° trang c·∫ßn ƒë·∫øn
                            break
                    else:
                        break
                
                # Verify sau khi click
                pagination_info = await self._extract_pagination_info(frame)
                if pagination_info and pagination_info["current_page"] == page_num:
                    return True
            
            logger.warning(f"‚ö†Ô∏è Cannot navigate to page {page_num}")
            return False
            
        except Exception as e:
            logger.error(f"Error navigating to page {page_num}: {e}")
            return False
    
    async def _download_single_giaynoptien(self, session: SessionData, item: Dict, temp_dir: str, max_retries: int = 2) -> bool:
        """
        Download 1 file gi·∫•y n·ªôp ti·ªÅn v·ªõi retry logic (gi·ªëng th√¥ng b√°o)
        
        Args:
            session: SessionData object
            item: Dict ch·ª©a th√¥ng tin file c·∫ßn download (id, download_link, cols, col_index)
            temp_dir: Th∆∞ m·ª•c t·∫°m ƒë·ªÉ l∆∞u file
            max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
        
        Returns:
            True n·∫øu download th√†nh c√¥ng
        """
        page = session.page
        id_gnt = item["id"]
        file_name = item.get("file_name", f"chungtu_{id_gnt}")
        
        for retry in range(max_retries + 1):
            try:
                # ∆Øu ti√™n d√πng download_link ƒë√£ t√¨m s·∫µn
                download_link = item.get("download_link")
                
                if not download_link:
                    # Fallback: t√¨m l·∫°i t·ª´ cols
                    cols = item.get("cols")
                    col_idx = item.get("col_index", 18)
                    if cols:
                        download_link = cols.nth(col_idx).locator('a[href*="downloadGNT"], a[onclick*="downloadGNT"]')
                
                if download_link and await download_link.count() > 0:
                    async with page.expect_download(timeout=30000) as download_info:
                        await download_link.first.click()
                    
                    download = await download_info.value
                    save_path = os.path.join(temp_dir, file_name + ".xml" if not file_name.endswith(".xml") else file_name)
                    await download.save_as(save_path)
                    
                    # Verify file exists and has content
                    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
                        logger.info(f"Downloaded giaynoptien {id_gnt} -> {file_name}")
                        return True
                    else:
                        raise Exception("File empty or not saved")
                else:
                    logger.warning(f"No download link for giaynoptien {id_gnt}")
                    return False
                    
            except Exception as e:
                logger.warning(f"Error downloading giaynoptien {id_gnt} (attempt {retry + 1}/{max_retries + 1}): {e}")
                if retry < max_retries:
                    await asyncio.sleep(1)  # Wait before retry
        
        return False


# Singleton instance - s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o v·ªõi session_manager
_tax_crawler_instance = None

def get_tax_crawler() -> TaxCrawlerService:
    global _tax_crawler_instance
    if _tax_crawler_instance is None:
        from .session_manager import session_manager
        _tax_crawler_instance = TaxCrawlerService(session_manager)
    return _tax_crawler_instance

# Backwards compatibility
tax_crawler = None  # Will be lazy-initialized
