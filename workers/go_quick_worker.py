import sys
import os
import json
import asyncio
import logging
import base64
import threading

# Get the project root directory (tool-gotax)
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Import shared modules
from shared.redis_client import get_redis_client, publish_progress, is_job_cancelled

# Import tool-go-quick modules
tool_go_quick_path = os.path.join(project_root, 'tool-go-quick')
sys.path.insert(0, tool_go_quick_path)

# Import model cache v√† extractor
from api.routes import get_model_cache, get_cccd_extractor_streaming

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Redis queues
QUEUE_GO_QUICK = 'go-quick:jobs'

async def process_go_quick_job(job_data):
    job_id = job_data.get('job_id')
    action = job_data.get('action', 'process-pdf')
    params = job_data.get('params', {})
    
    logger.info(f"[Job {job_id}] ‚ö° B·∫Øt ƒë·∫ßu x·ª≠ l√Ω job, action={action}")
    
    redis_client = get_redis_client()
    
    try:
        if is_job_cancelled(job_id):
            logger.info(f"[Job {job_id}] Job ƒë√£ b·ªã cancel tr∆∞·ªõc khi x·ª≠ l√Ω")
            redis_client.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
            publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
            return
        
        # Update status: processing
        redis_client.set(f"job:{job_id}:status", "processing".encode('utf-8'))
        
        # Extract params
        file_path = params.get('file_path')
        file_type = params.get('file_type', 'pdf')  # pdf, excel, zip, images
        
        if not file_path:
            error_msg = "Thi·∫øu th√¥ng tin: file_path"
            logger.error(f"[Job {job_id}] {error_msg}")
            redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
            publish_progress(job_id, 0, error_msg)
            return
        
        # X√°c ƒë·ªãnh func_type d·ª±a tr√™n action
        total_cccd = 0  # S·∫Ω ƒë∆∞·ª£c set sau khi c√≥ k·∫øt qu·∫£
        if action == 'process-pdf':
            func_type = 2  # PDF
            logger.info(f"[Job {job_id}] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF")
            publish_progress(job_id, 0, "B·∫Øt ƒë·∫ßu x·ª≠ l√Ω PDF...")
        elif action == 'process-excel':
            func_type = 3  # Excel
            logger.info(f"[Job {job_id}] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Excel")
            publish_progress(job_id, 0, "B·∫Øt ƒë·∫ßu x·ª≠ l√Ω Excel...")
        elif action == 'process-cccd':
            func_type = 1  # CCCD/ZIP
            logger.info(f"[Job {job_id}] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω CCCD")
            publish_progress(job_id, 0, "B·∫Øt ƒë·∫ßu x·ª≠ l√Ω CCCD...")
        else:
            error_msg = f"Action kh√¥ng h·ª£p l·ªá: {action}"
            logger.error(f"[Job {job_id}] {error_msg}")
            redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
            publish_progress(job_id, 0, error_msg)
            return
        
        # Read file content
        if not os.path.exists(file_path):
            error_msg = f"File kh√¥ng t·ªìn t·∫°i: {file_path}"
            logger.error(f"[Job {job_id}] {error_msg}")
            redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
            publish_progress(job_id, 0, error_msg)
            return
        
        # Get file name from path
        file_name = os.path.basename(file_path)
        
        # Read file as bytes
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        logger.info(f"[Job {job_id}] ƒê·ªçc file: {file_name} ({len(file_content)} bytes)")
        
        # Load model cache (n·∫øu ch∆∞a load)
        logger.info(f"[Job {job_id}] ƒêang load model cache...")
        model_cache = get_model_cache()
        logger.info(f"[Job {job_id}] Models ƒë√£ s·∫µn s√†ng")
        
        # T·∫°o CCCDExtractor instance
        logger.info(f"[Job {job_id}] ƒêang t·∫°o CCCDExtractor instance...")
        CCCDExtractorClass = get_cccd_extractor_streaming()
        extractor = CCCDExtractorClass(cached_models=model_cache)
        logger.info(f"[Job {job_id}] CCCDExtractor instance ƒë√£ ƒë∆∞·ª£c t·∫°o")
        
        # T·∫°o task
        task = {
            "func_type": func_type,
            "inp_path": file_content,  # Pass bytes directly
            "job_id": job_id  # Pass job_id ƒë·ªÉ c√≥ th·ªÉ publish progress trong DetectWorker
        }
        
        logger.info(f"[Job {job_id}] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v·ªõi func_type={func_type}")
        
        # Check cancellation tr∆∞·ªõc khi x·ª≠ l√Ω
        if is_job_cancelled(job_id):
            logger.info(f"[Job {job_id}] Job ƒë√£ b·ªã cancel tr∆∞·ªõc khi x·ª≠ l√Ω")
            redis_client.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
            publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
            return
        
        # G·ªçi tr·ª±c ti·∫øp handle_task (kh√¥ng qua HTTP)
        # Ch·∫°y trong thread pool ƒë·ªÉ kh√¥ng block event loop
        logger.info(f"[Job {job_id}] ƒêang g·ªçi extractor.handle_task() trong thread pool...")
        try:
            # Ch·∫°y handle_task trong thread pool ƒë·ªÉ kh√¥ng block event loop
            # ƒêi·ªÅu n√†y cho ph√©p nhi·ªÅu jobs ch·∫°y song song th·ª±c s·ª±
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, extractor.handle_task, task)
            logger.info(f"[Job {job_id}] extractor.handle_task() ƒë√£ ho√†n th√†nh")
        except Exception as e:
            # N·∫øu exception l√† do cancellation, handle ri√™ng
            if "ƒë√£ b·ªã h·ªßy" in str(e) or "Job ƒë√£ b·ªã h·ªßy" in str(e):
                logger.info(f"[Job {job_id}] Job ƒë√£ b·ªã h·ªßy trong qu√° tr√¨nh x·ª≠ l√Ω: {e}")
                redis_client.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
                return
            # N·∫øu l√† exception kh√°c, re-raise ƒë·ªÉ ƒë∆∞·ª£c handle ·ªü ngo√†i
            raise
        
        # Check cancellation sau khi x·ª≠ l√Ω
        if is_job_cancelled(job_id):
            logger.info(f"[Job {job_id}] Job ƒë√£ b·ªã cancel sau khi x·ª≠ l√Ω")
            redis_client.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
            publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
            return
        
        logger.info(f"[Job {job_id}] ƒê√£ x·ª≠ l√Ω xong")
        logger.info(f"[Job {job_id}] Response keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
        
        # L·∫•y total_cccd t·ª´ k·∫øt qu·∫£ (func_type 1, 2 ho·∫∑c 3)
        if isinstance(result, dict) and result.get("status") == "success":
            # L·∫•y total_cccd t·ª´ result (c√≥ th·ªÉ t·ª´ total_rows ho·∫∑c total_cccd)
            if "total_cccd" in result:
                total_cccd = result.get("total_cccd", 0)
            elif "total_rows" in result:
                # Excel: total_rows = s·ªë CCCD
                total_cccd = result.get("total_rows", 0)
            elif "total_images" in result:
                # PDF: total_images // 2 = s·ªë CCCD
                total_cccd = result.get("total_images", 0) // 2
            else:
                total_cccd = 0
            
            if total_cccd > 0:
                # L∆∞u total_cccd v√†o Redis ƒë·ªÉ frontend c√≥ th·ªÉ hi·ªÉn th·ªã
                redis_client.set(f"job:{job_id}:total_cccd", str(total_cccd))
                # Publish progress v·ªõi format 0/total_cccd v√† 0% - G·ª¨I total_cccd trong message
                publish_progress(job_id, 0, f"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω... (0/{total_cccd} CCCD - 0%)", total_cccd=total_cccd, processed_cccd=0)
                logger.info(f"[Job {job_id}] ‚úÖ T·ªïng s·ªë CCCD: {total_cccd}")
            else:
                logger.warning(f"[Job {job_id}] ‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c total_cccd t·ª´ result")
        
        # N·∫øu c√≥ zip_base64 (PDF/Excel), decode v√† g·ªçi l·∫°i v·ªõi func_type=1
        if isinstance(result, dict) and result.get("status") == "success" and result.get("zip_base64"):
            zip_base64 = result.get("zip_base64")
            zip_bytes = base64.b64decode(zip_base64)
            
            logger.info(f"[Job {job_id}] ƒê√£ convert xong, b·∫Øt ƒë·∫ßu OCR v·ªõi {len(zip_bytes)} bytes")
            # Publish progress v·ªõi format 0/total_cccd n·∫øu ƒë√£ c√≥ total_cccd
            if total_cccd > 0:
                publish_progress(job_id, 20, f"ƒêang x·ª≠ l√Ω OCR... (0/{total_cccd} CCCD - 20%)", total_cccd=total_cccd, processed_cccd=0)
            else:
                publish_progress(job_id, 20, "ƒêang x·ª≠ l√Ω OCR...")
            
            # T·∫°o task2 ƒë·ªÉ x·ª≠ l√Ω OCR
            task2 = {
                "func_type": 1,  # Chuy·ªÉn sang x·ª≠ l√Ω CCCD
                "inp_path": zip_bytes,
                "job_id": job_id,
                "total_cccd": total_cccd  # Pass total_cccd ƒë·ªÉ DetectWorker c√≥ th·ªÉ publish progress
            }
            
            # G·ªçi l·∫°i handle_task v·ªõi func_type=1
            logger.info(f"[Job {job_id}] ƒêang g·ªçi extractor.handle_task() (OCR) trong thread pool...")
            try:
                # Ch·∫°y trong thread pool ƒë·ªÉ kh√¥ng block event loop
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, extractor.handle_task, task2)
                logger.info(f"[Job {job_id}] extractor.handle_task() (OCR) ƒë√£ ho√†n th√†nh")
            except Exception as e:
                # N·∫øu exception l√† do cancellation, handle ri√™ng
                if "ƒë√£ b·ªã h·ªßy" in str(e) or "Job ƒë√£ b·ªã h·ªßy" in str(e):
                    logger.info(f"[Job {job_id}] Job ƒë√£ b·ªã h·ªßy trong qu√° tr√¨nh OCR: {e}")
                    redis_client.set(f"job:{job_id}:status", "cancelled".encode('utf-8'))
                    publish_progress(job_id, 0, "Job ƒë√£ b·ªã h·ªßy")
                    return
                # N·∫øu l√† exception kh√°c, re-raise ƒë·ªÉ ƒë∆∞·ª£c handle ·ªü ngo√†i
                raise
            
            # L·∫•y total_cccd t·ª´ k·∫øt qu·∫£ func_type=1 (n·∫øu ch∆∞a c√≥)
            if isinstance(result, dict) and result.get("status") == "success" and total_cccd == 0:
                total_cccd = result.get("total_cccd", 0)
                if total_cccd > 0:
                    redis_client.set(f"job:{job_id}:total_cccd", str(total_cccd))
                    # Publish l·∫°i progress v·ªõi total_cccd
                    publish_progress(job_id, 20, f"ƒêang x·ª≠ l√Ω OCR... (0/{total_cccd} CCCD - 20%)", total_cccd=total_cccd, processed_cccd=0)
                    logger.info(f"[Job {job_id}] ‚úÖ T·ªïng s·ªë CCCD (t·ª´ OCR): {total_cccd}")
                else:
                    logger.warning(f"[Job {job_id}] ‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c total_cccd t·ª´ OCR result")
        
        # Save result to Redis
        result_data = {
            'status': 'success',
            'data': result
        }
        
        redis_client.set(f"job:{job_id}:result", json.dumps(result_data, ensure_ascii=False).encode('utf-8'))
        redis_client.set(f"job:{job_id}:status", "completed".encode('utf-8'))
        
        # Publish final progress
        customer_count = 0
        if isinstance(result, dict) and 'customer' in result:
            customer_count = len(result.get('customer', []))
        
        # L·∫•y total_cccd t·ª´ result n·∫øu ch∆∞a c√≥ (func_type=1 tr·ª±c ti·∫øp)
        if total_cccd == 0 and isinstance(result, dict):
            total_cccd = result.get("total_cccd", 0)
            if total_cccd == 0:
                # Fallback: d√πng customer_count n·∫øu kh√¥ng c√≥ total_cccd
                total_cccd = customer_count
            if total_cccd > 0:
                redis_client.set(f"job:{job_id}:total_cccd", str(total_cccd))
                logger.info(f"[Job {job_id}] ‚úÖ L·∫•y total_cccd t·ª´ result cu·ªëi: {total_cccd}")
        
        # Publish progress v·ªõi format cu·ªëi c√πng
        if total_cccd > 0:
            publish_progress(job_id, 100, f"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {customer_count}/{total_cccd} CCCD (100%)", total_cccd=total_cccd, processed_cccd=customer_count)
            logger.info(f"[Job {job_id}] ‚úÖ Job ho√†n th√†nh: {customer_count}/{total_cccd} CCCD")
        else:
            publish_progress(job_id, 100, f"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {customer_count} CCCD", processed_cccd=customer_count)
            logger.warning(f"[Job {job_id}] ‚ö†Ô∏è Job ho√†n th√†nh: {customer_count} CCCD (kh√¥ng c√≥ total_cccd)")
        
    except Exception as e:
        error_msg = f"L·ªói x·ª≠ l√Ω job: {str(e)}"
        logger.error(f"[Job {job_id}] {error_msg}", exc_info=True)
        redis_client.set(f"job:{job_id}:status", "failed".encode('utf-8'))
        redis_client.set(f"job:{job_id}:error", error_msg.encode('utf-8'))
        publish_progress(job_id, 0, error_msg)
    finally:
        # Cleanup: Delete temp file
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"[Job {job_id}] ƒê√£ x√≥a temp file: {file_path}")
        except Exception as e:
            logger.warning(f"[Job {job_id}] Kh√¥ng th·ªÉ x√≥a temp file: {e}")

async def process_job_wrapper(job_data):
    """Wrapper ƒë·ªÉ x·ª≠ l√Ω job trong background task"""
    job_id = job_data.get('job_id', 'unknown')
    logger.info(f"[Job {job_id}] üîÑ process_job_wrapper ƒë∆∞·ª£c g·ªçi")
    try:
        await process_go_quick_job(job_data)
        logger.info(f"[Job {job_id}] ‚úÖ process_job_wrapper ho√†n th√†nh")
    except Exception as e:
        logger.error(f"‚ùå Error processing job {job_id}: {e}", exc_info=True)

async def main():
    """Main worker loop - x·ª≠ l√Ω nhi·ªÅu jobs parallel"""
    redis_client = get_redis_client()
    logger.info(f"üöÄ Go-Quick Worker started, listening on queue: {QUEUE_GO_QUICK}")
    logger.info(f"üìä Worker s·∫Ω x·ª≠ l√Ω nhi·ªÅu jobs parallel (kh√¥ng block)")
    
    # Set ƒë·ªÉ track c√°c tasks ƒëang ch·∫°y
    running_tasks = set()
    max_concurrent_jobs = 10  # S·ªë l∆∞·ª£ng jobs t·ªëi ƒëa ch·∫°y c√πng l√∫c
    
    while True:
        try:
            # Ch·ªâ l·∫•y job m·ªõi n·∫øu ch∆∞a ƒë·∫°t max concurrent
            if len(running_tasks) < max_concurrent_jobs:
                # Blocking pop from queue (wait up to 1 second)
                # Ch·∫°y blpop trong thread pool ƒë·ªÉ kh√¥ng block event loop
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(None, lambda: redis_client.blpop([QUEUE_GO_QUICK], timeout=1))
                
                if result:
                    queue_name, job_data_json = result
                    job_data = json.loads(job_data_json.decode('utf-8'))
                    
                    job_id = job_data.get('job_id')
                    logger.info(f"üì• Received job: {job_id} (Running: {len(running_tasks)}/{max_concurrent_jobs})")
                    
                    # T·∫°o task ƒë·ªÉ x·ª≠ l√Ω job trong background
                    logger.info(f"[Job {job_id}] üîÑ T·∫°o asyncio task ƒë·ªÉ x·ª≠ l√Ω...")
                    task = asyncio.create_task(process_job_wrapper(job_data))
                    running_tasks.add(task)
                    logger.info(f"[Job {job_id}] ‚úÖ Task ƒë√£ ƒë∆∞·ª£c t·∫°o v√† th√™m v√†o running_tasks (Total running: {len(running_tasks)})")
                    
                    # X√≥a task kh·ªèi set khi ho√†n th√†nh
                    def remove_task(task):
                        running_tasks.discard(task)
                        logger.debug(f"[Job {job_id}] üóëÔ∏è Task ƒë√£ ho√†n th√†nh, ƒë√£ x√≥a kh·ªèi running_tasks")
                    
                    task.add_done_callback(remove_task)
            else:
                # ƒê√£ ƒë·∫°t max concurrent, ƒë·ª£i m·ªôt ch√∫t
                await asyncio.sleep(0.1)
            
            # Cleanup completed tasks
            running_tasks = {t for t in running_tasks if not t.done()}
                
        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è Worker d·ª´ng b·ªüi ng∆∞·ªùi d√πng, ƒë·ª£i c√°c jobs ƒëang ch·∫°y ho√†n th√†nh...")
            # ƒê·ª£i t·∫•t c·∫£ tasks ho√†n th√†nh
            if running_tasks:
                await asyncio.gather(*running_tasks, return_exceptions=True)
            break
        except Exception as e:
            logger.error(f"‚ùå Error in worker loop: {e}", exc_info=True)
            await asyncio.sleep(5)  # Wait before retry

if __name__ == '__main__':
    asyncio.run(main())
